{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modification of the original code\n",
    "I just took the code of the winner of the RSNA 2022 cervical spine fracture detection competition.  \n",
    "Link: https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-train-stage1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import ast\n",
    "import cv2\n",
    "import imageio\n",
    "import time\n",
    "import timm\n",
    "import pickle\n",
    "import random\n",
    "import pydicom\n",
    "import dicomsdl\n",
    "import argparse\n",
    "import warnings\n",
    "import numpy as np\n",
    "# import cupy as cp\n",
    "import pandas as pd\n",
    "import glob\n",
    "import nibabel as nib\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import segmentation_models_pytorch as smp\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "import gzip\n",
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.cuda.amp as amp\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 20, 8\n",
    "device = torch.device('cuda')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"]=\"1\"\n",
    "sys.path.append('./lib_models')\n",
    "import timm_new\n",
    "\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RESOL = [160, 160, 160]\n",
    "CROP_RESOL = 128\n",
    "\n",
    "\n",
    "BASE_PATH = './kaggle/input/rsna-2023-abdominal-trauma-detection'\n",
    "MASK_SAVE_PATH = f'{BASE_PATH}/mask_preprocessed'\n",
    "MASK_VALID_PATH = f'{BASE_PATH}/mask_validation'\n",
    "TRAIN_PATH = f'{BASE_PATH}/train_images'\n",
    "\n",
    "BATCH_MASK_PRED = 8\n",
    "N_PROCESS_CROP = 24\n",
    "PREPROC_NORM_OR_STD = True # True: normalization, False: standardization\n",
    "\n",
    "# seg_weight_name = '231001_timm3d_res10tc_CV0.938.pt'\n",
    "seg_weight_name = 'timm3d_res18d_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_bs4_lr3e4_20x50ep_fold0_best.pth'\n",
    "load_kernel = None\n",
    "load_last = True\n",
    "n_blocks = 4\n",
    "backbone = 'timm/resnet10t.c3_in1k'\n",
    "drop_rate = 0.\n",
    "drop_path_rate = 0.\n",
    "loss_weights = [1, 1]\n",
    "p_mixup = 0.1\n",
    "\n",
    "data_dir = './kaggle/input/rsna-2022-cervical-spine-fracture-detection'\n",
    "use_amp = True\n",
    "\n",
    "\n",
    "num_workers = 24\n",
    "out_dim = 5\n",
    "\n",
    "model_dir = f'{BASE_PATH}/seg_models_backup'\n",
    "seg_inference_dir = f'{BASE_PATH}/seg_infer_results'\n",
    "cropped_img_dir   = f'{BASE_PATH}/3d_preprocessed_crop_ratio'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(MASK_VALID_PATH, exist_ok=True)\n",
    "os.makedirs(seg_inference_dir, exist_ok = True)\n",
    "os.makedirs(cropped_img_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path to save cropped image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(f'{BASE_PATH}/train_meta.csv')\n",
    "mask_paths = []\n",
    "cropped_paths = []\n",
    "for i in range(0, len(df_train)):\n",
    "    row = df_train.iloc[i]\n",
    "    file_name = row['path'].split('/')[-1]\n",
    "    mask_paths.append(f'{seg_inference_dir}/{file_name}')\n",
    "    cropped_paths.append(f'{cropped_img_dir}/{file_name}')\n",
    "df_train['cropped_path'] = cropped_paths\n",
    "df_train['mask_path']    = mask_paths\n",
    "df_train.tail()\n",
    "\n",
    "df_train.to_csv(f'{BASE_PATH}/train_meta.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>series</th>\n",
       "      <th>bowel_healthy</th>\n",
       "      <th>bowel_injury</th>\n",
       "      <th>extravasation_healthy</th>\n",
       "      <th>extravasation_injury</th>\n",
       "      <th>kidney_healthy</th>\n",
       "      <th>kidney_low</th>\n",
       "      <th>kidney_high</th>\n",
       "      <th>liver_healthy</th>\n",
       "      <th>liver_low</th>\n",
       "      <th>liver_high</th>\n",
       "      <th>spleen_healthy</th>\n",
       "      <th>spleen_low</th>\n",
       "      <th>spleen_high</th>\n",
       "      <th>any_injury</th>\n",
       "      <th>fold</th>\n",
       "      <th>path</th>\n",
       "      <th>cropped_path</th>\n",
       "      <th>mask_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32072</td>\n",
       "      <td>35846</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37338</td>\n",
       "      <td>6641</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37338</td>\n",
       "      <td>3542</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22372</td>\n",
       "      <td>29262</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22580</td>\n",
       "      <td>9795</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4706</th>\n",
       "      <td>35765</td>\n",
       "      <td>31821</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4707</th>\n",
       "      <td>32568</td>\n",
       "      <td>39807</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4708</th>\n",
       "      <td>32568</td>\n",
       "      <td>26178</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4709</th>\n",
       "      <td>18025</td>\n",
       "      <td>11166</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4710</th>\n",
       "      <td>31087</td>\n",
       "      <td>39366</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "      <td>./kaggle/input/rsna-2023-abdominal-trauma-dete...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4711 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      patient_id  series  bowel_healthy  bowel_injury  extravasation_healthy  \\\n",
       "0          32072   35846              1             0                      1   \n",
       "1          37338    6641              1             0                      1   \n",
       "2          37338    3542              1             0                      1   \n",
       "3          22372   29262              1             0                      1   \n",
       "4          22580    9795              1             0                      1   \n",
       "...          ...     ...            ...           ...                    ...   \n",
       "4706       35765   31821              1             0                      1   \n",
       "4707       32568   39807              1             0                      1   \n",
       "4708       32568   26178              1             0                      1   \n",
       "4709       18025   11166              1             0                      1   \n",
       "4710       31087   39366              1             0                      1   \n",
       "\n",
       "      extravasation_injury  kidney_healthy  kidney_low  kidney_high  \\\n",
       "0                        0               1           0            0   \n",
       "1                        0               1           0            0   \n",
       "2                        0               1           0            0   \n",
       "3                        0               1           0            0   \n",
       "4                        0               1           0            0   \n",
       "...                    ...             ...         ...          ...   \n",
       "4706                     0               1           0            0   \n",
       "4707                     0               1           0            0   \n",
       "4708                     0               1           0            0   \n",
       "4709                     0               1           0            0   \n",
       "4710                     0               1           0            0   \n",
       "\n",
       "      liver_healthy  liver_low  liver_high  spleen_healthy  spleen_low  \\\n",
       "0                 1          0           0               1           0   \n",
       "1                 1          0           0               1           0   \n",
       "2                 1          0           0               1           0   \n",
       "3                 0          1           0               1           0   \n",
       "4                 1          0           0               1           0   \n",
       "...             ...        ...         ...             ...         ...   \n",
       "4706              1          0           0               1           0   \n",
       "4707              0          1           0               1           0   \n",
       "4708              0          1           0               1           0   \n",
       "4709              1          0           0               1           0   \n",
       "4710              1          0           0               1           0   \n",
       "\n",
       "      spleen_high  any_injury  fold  \\\n",
       "0               0           0     4   \n",
       "1               0           0     3   \n",
       "2               0           0     3   \n",
       "3               0           1     3   \n",
       "4               0           0     3   \n",
       "...           ...         ...   ...   \n",
       "4706            0           0     2   \n",
       "4707            0           1     2   \n",
       "4708            0           1     2   \n",
       "4709            0           0     3   \n",
       "4710            0           0     0   \n",
       "\n",
       "                                                   path  \\\n",
       "0     ./kaggle/input/rsna-2023-abdominal-trauma-dete...   \n",
       "1     ./kaggle/input/rsna-2023-abdominal-trauma-dete...   \n",
       "2     ./kaggle/input/rsna-2023-abdominal-trauma-dete...   \n",
       "3     ./kaggle/input/rsna-2023-abdominal-trauma-dete...   \n",
       "4     ./kaggle/input/rsna-2023-abdominal-trauma-dete...   \n",
       "...                                                 ...   \n",
       "4706  ./kaggle/input/rsna-2023-abdominal-trauma-dete...   \n",
       "4707  ./kaggle/input/rsna-2023-abdominal-trauma-dete...   \n",
       "4708  ./kaggle/input/rsna-2023-abdominal-trauma-dete...   \n",
       "4709  ./kaggle/input/rsna-2023-abdominal-trauma-dete...   \n",
       "4710  ./kaggle/input/rsna-2023-abdominal-trauma-dete...   \n",
       "\n",
       "                                           cropped_path  \\\n",
       "0     ./kaggle/input/rsna-2023-abdominal-trauma-dete...   \n",
       "1     ./kaggle/input/rsna-2023-abdominal-trauma-dete...   \n",
       "2     ./kaggle/input/rsna-2023-abdominal-trauma-dete...   \n",
       "3     ./kaggle/input/rsna-2023-abdominal-trauma-dete...   \n",
       "4     ./kaggle/input/rsna-2023-abdominal-trauma-dete...   \n",
       "...                                                 ...   \n",
       "4706  ./kaggle/input/rsna-2023-abdominal-trauma-dete...   \n",
       "4707  ./kaggle/input/rsna-2023-abdominal-trauma-dete...   \n",
       "4708  ./kaggle/input/rsna-2023-abdominal-trauma-dete...   \n",
       "4709  ./kaggle/input/rsna-2023-abdominal-trauma-dete...   \n",
       "4710  ./kaggle/input/rsna-2023-abdominal-trauma-dete...   \n",
       "\n",
       "                                              mask_path  \n",
       "0     ./kaggle/input/rsna-2023-abdominal-trauma-dete...  \n",
       "1     ./kaggle/input/rsna-2023-abdominal-trauma-dete...  \n",
       "2     ./kaggle/input/rsna-2023-abdominal-trauma-dete...  \n",
       "3     ./kaggle/input/rsna-2023-abdominal-trauma-dete...  \n",
       "4     ./kaggle/input/rsna-2023-abdominal-trauma-dete...  \n",
       "...                                                 ...  \n",
       "4706  ./kaggle/input/rsna-2023-abdominal-trauma-dete...  \n",
       "4707  ./kaggle/input/rsna-2023-abdominal-trauma-dete...  \n",
       "4708  ./kaggle/input/rsna-2023-abdominal-trauma-dete...  \n",
       "4709  ./kaggle/input/rsna-2023-abdominal-trauma-dete...  \n",
       "4710  ./kaggle/input/rsna-2023-abdominal-trauma-dete...  \n",
       "\n",
       "[4711 rows x 20 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(name, data):\n",
    "    with gzip.open(name, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def decompress(name):\n",
    "    with gzip.open(name, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "def compress_fast(name, data):\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def decompress_fast(name):\n",
    "    with open(name, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "# class SEGDataset(Dataset):\n",
    "#     def __init__(self, df, mode):\n",
    "\n",
    "#         self.df = df.reset_index()\n",
    "#         self.mode = mode\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.df.shape[0]\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         row = self.df.iloc[index]\n",
    "        \n",
    "#         image = decompress(row['path']).unsqueeze(0)\n",
    "#         #image = torch.from_numpy(image).to(torch.float32)\n",
    "#         save_path = row['mask_path']\n",
    "\n",
    "#         return image, save_path\n",
    "\n",
    "\n",
    "def load_dicom(path):\n",
    "    dicom = pydicom.read_file(path)\n",
    "    data = dicom.pixel_array\n",
    "    data = cv2.resize(data, (RESOL[0], RESOL[1]), interpolation = cv2.INTER_LINEAR)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def load_dicom_line_par(path):\n",
    "    print(path)\n",
    "    # t_paths = sorted(glob(os.path.join(path)),\n",
    "    #    key=lambda x: int(x.split('/')[-1].split(\".\")[0]))\n",
    "    \n",
    "    t_paths = sorted(glob.glob(os.path.join(path)),\n",
    "        key=lambda x: int(os.path.splitext(os.path.basename(x))[0].split('_')[0]))\n",
    "    \n",
    "\n",
    "    n_scans = len(t_paths)\n",
    "    indices = np.quantile(list(range(n_scans)), np.linspace(0., 1., RESOL[2])).round().astype(int)\n",
    "    t_paths = [t_paths[i] for i in indices]\n",
    "\n",
    "    images = []\n",
    "    for filename in t_paths:\n",
    "        images.append(load_dicom(filename))\n",
    "    images = np.stack(images, -1)\n",
    "    \n",
    "    images = images - np.min(images)\n",
    "    images = images / (np.max(images) + 1e-4)\n",
    "    images = (images * 255).astype(np.uint8)\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "def load_sample(row, has_mask=True):\n",
    "\n",
    "    # image = load_dicom_line_par(row.image_folder)\n",
    "    image = load_dicom_line_par(row.cropped_path)    \n",
    "    if image.ndim < 4:\n",
    "        image = np.expand_dims(image, 0).repeat(3, 0)  # to 3ch\n",
    "\n",
    "    if has_mask:\n",
    "        # mask_org = nib.load(row.mask_file).get_fdata()\n",
    "        mask_org = nib.load(row.mask_path).get_fdata()\n",
    "    \n",
    "        mask_org = mask_org.transpose(1, 0, 2)[::-1, :, ::-1]  # (d, w, h)\n",
    "        shape = mask_org.shape\n",
    "        mask = np.zeros((5, shape[0], shape[1], shape[2]))\n",
    "        for cid in range(5):\n",
    "            mask[cid] = (mask_org == (cid+1))\n",
    "        mask = mask.astype(np.uint8) * 255\n",
    "        mask = R(mask).numpy()\n",
    "        \n",
    "        return image, mask\n",
    "    else:\n",
    "        return image\n",
    "\n",
    "\n",
    "# by junseong \n",
    "class SEGDataset(Dataset):\n",
    "    def __init__(self, df, mode, transform):\n",
    "\n",
    "        self.df = df.reset_index()\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.masks = []\n",
    "        for i in tqdm(range(0, len(df))):\n",
    "            image, mask = load_sample(df.iloc[i], has_mask=True)\n",
    "            self.images.append(image)\n",
    "            self.masks.append(mask)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index].copy()\n",
    "        mask  = self.masks[index].copy()\n",
    "        res = self.transform({'image':image, 'mask':mask})\n",
    "        image = res['image'] / 255.\n",
    "        mask = res['mask']\n",
    "        mask = (mask > 127).astype(np.float32)\n",
    "        image, mask = torch.tensor(image).float(), torch.tensor(mask).float()\n",
    "\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TimmSegModel(nn.Module):\n",
    "#     def __init__(self, backbone, segtype='unet', pretrained=False):\n",
    "#         super(TimmSegModel, self).__init__()\n",
    "\n",
    "#         self.encoder = timm_new.create_model(\n",
    "#             backbone,\n",
    "#             in_chans=1,\n",
    "#             features_only=True,\n",
    "#             pretrained=pretrained\n",
    "#         )\n",
    "#         g = self.encoder(torch.rand(1, 1, 64, 64))\n",
    "#         encoder_channels = [1] + [_.shape[1] for _ in g]\n",
    "#         decoder_channels = [256, 128, 64, 32, 16]\n",
    "#         if segtype == 'unet':\n",
    "#             self.decoder = smp.unet.decoder.UnetDecoder(\n",
    "#                 encoder_channels=encoder_channels[:n_blocks+1],\n",
    "#                 decoder_channels=decoder_channels[:n_blocks],\n",
    "#                 n_blocks=n_blocks,\n",
    "#             )\n",
    "\n",
    "#         self.segmentation_head = nn.Conv2d(decoder_channels[n_blocks-1], out_dim, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "#     def forward(self,x):\n",
    "#         global_features = [0] + self.encoder(x)[:n_blocks]\n",
    "#         seg_features = self.decoder(*global_features)\n",
    "#         seg_features = self.segmentation_head(seg_features)\n",
    "#         return seg_features\n",
    "\n",
    "class TimmSegModel(nn.Module):\n",
    "    def __init__(self, backbone, segtype='unet', pretrained=False):\n",
    "        super(TimmSegModel, self).__init__()\n",
    "\n",
    "        self.encoder = timm_new.create_model(\n",
    "            backbone,\n",
    "            in_chans=3,\n",
    "            features_only=True,\n",
    "            drop_rate=drop_rate,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "        g = self.encoder(torch.rand(1, 3, 64, 64))\n",
    "        encoder_channels = [1] + [_.shape[1] for _ in g]\n",
    "        decoder_channels = [256, 128, 64, 32, 16]\n",
    "        if segtype == 'unet':\n",
    "            self.decoder = smp.unet.decoder.UnetDecoder(\n",
    "                encoder_channels=encoder_channels[:n_blocks+1],\n",
    "                decoder_channels=decoder_channels[:n_blocks],\n",
    "                n_blocks=n_blocks,\n",
    "            )\n",
    "\n",
    "        self.segmentation_head = nn.Conv2d(decoder_channels[n_blocks-1], out_dim, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        global_features = [0] + self.encoder(x)[:n_blocks]\n",
    "        seg_features = self.decoder(*global_features)\n",
    "        seg_features = self.segmentation_head(seg_features)\n",
    "        return seg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 128, 128, 128])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from timm.models.layers.conv2d_same import Conv2dSame\n",
    "# from conv3d_same import Conv3dSame\n",
    "\n",
    "\n",
    "# def convert_3d(module):\n",
    "\n",
    "#     module_output = module\n",
    "#     if isinstance(module, torch.nn.BatchNorm2d):\n",
    "#         module_output = torch.nn.BatchNorm3d(\n",
    "#             module.num_features,\n",
    "#             module.eps,\n",
    "#             module.momentum,\n",
    "#             module.affine,\n",
    "#             module.track_running_stats,\n",
    "#         )\n",
    "#         if module.affine:\n",
    "#             with torch.no_grad():\n",
    "#                 module_output.weight = module.weight\n",
    "#                 module_output.bias = module.bias\n",
    "#         module_output.running_mean = module.running_mean\n",
    "#         module_output.running_var = module.running_var\n",
    "#         module_output.num_batches_tracked = module.num_batches_tracked\n",
    "#         if hasattr(module, \"qconfig\"):\n",
    "#             module_output.qconfig = module.qconfig\n",
    "            \n",
    "#     elif isinstance(module, Conv2dSame):\n",
    "#         module_output = Conv3dSame(\n",
    "#             in_channels=module.in_channels,\n",
    "#             out_channels=module.out_channels,\n",
    "#             kernel_size=module.kernel_size[0],\n",
    "#             stride=module.stride[0],\n",
    "#             padding=module.padding[0],\n",
    "#             dilation=module.dilation[0],\n",
    "#             groups=module.groups,\n",
    "#             bias=module.bias is not None,\n",
    "#         )\n",
    "#         module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "#     elif isinstance(module, torch.nn.Conv2d):\n",
    "#         module_output = torch.nn.Conv3d(\n",
    "#             in_channels=module.in_channels,\n",
    "#             out_channels=module.out_channels,\n",
    "#             kernel_size=module.kernel_size[0],\n",
    "#             stride=module.stride[0],\n",
    "#             padding=module.padding[0],\n",
    "#             dilation=module.dilation[0],\n",
    "#             groups=module.groups,\n",
    "#             bias=module.bias is not None,\n",
    "#             padding_mode=module.padding_mode\n",
    "#         )\n",
    "#         module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "#     elif isinstance(module, torch.nn.MaxPool2d):\n",
    "#         module_output = torch.nn.MaxPool3d(\n",
    "#             kernel_size=module.kernel_size,\n",
    "#             stride=module.stride,\n",
    "#             padding=module.padding,\n",
    "#             dilation=module.dilation,\n",
    "#             ceil_mode=module.ceil_mode,\n",
    "#         )\n",
    "#     elif isinstance(module, torch.nn.AvgPool2d):\n",
    "#         module_output = torch.nn.AvgPool3d(\n",
    "#             kernel_size=module.kernel_size,\n",
    "#             stride=module.stride,\n",
    "#             padding=module.padding,\n",
    "#             ceil_mode=module.ceil_mode,\n",
    "#         )\n",
    "\n",
    "#     for name, child in module.named_children():\n",
    "#         module_output.add_module(\n",
    "#             name, convert_3d(child)\n",
    "#         )\n",
    "#     del module\n",
    "\n",
    "#     return module_output\n",
    "\n",
    "# m = TimmSegModel(backbone)\n",
    "# m = convert_3d(m)\n",
    "# # m(torch.rand(1, 1, 128,128,128)).shape\n",
    "# m(torch.rand(1, 1, 128,128,128)).shape\n",
    "\n",
    "from timm.models.layers.conv2d_same import Conv2dSame\n",
    "from conv3d_same import Conv3dSame\n",
    "\n",
    "\n",
    "def convert_3d(module):\n",
    "\n",
    "    module_output = module\n",
    "    if isinstance(module, torch.nn.BatchNorm2d):\n",
    "        module_output = torch.nn.BatchNorm3d(\n",
    "            module.num_features,\n",
    "            module.eps,\n",
    "            module.momentum,\n",
    "            module.affine,\n",
    "            module.track_running_stats,\n",
    "        )\n",
    "        if module.affine:\n",
    "            with torch.no_grad():\n",
    "                module_output.weight = module.weight\n",
    "                module_output.bias = module.bias\n",
    "        module_output.running_mean = module.running_mean\n",
    "        module_output.running_var = module.running_var\n",
    "        module_output.num_batches_tracked = module.num_batches_tracked\n",
    "        if hasattr(module, \"qconfig\"):\n",
    "            module_output.qconfig = module.qconfig\n",
    "            \n",
    "    elif isinstance(module, Conv2dSame):\n",
    "        module_output = Conv3dSame(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.Conv2d):\n",
    "        module_output = torch.nn.Conv3d(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "            padding_mode=module.padding_mode\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.MaxPool2d):\n",
    "        module_output = torch.nn.MaxPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            dilation=module.dilation,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "    elif isinstance(module, torch.nn.AvgPool2d):\n",
    "        module_output = torch.nn.AvgPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "\n",
    "    for name, child in module.named_children():\n",
    "        module_output.add_module(\n",
    "            name, convert_3d(child)\n",
    "        )\n",
    "    del module\n",
    "\n",
    "    return module_output\n",
    "\n",
    "\n",
    "m = TimmSegModel(backbone)\n",
    "m = convert_3d(m)\n",
    "m(torch.rand(1, 3, 128,128,128)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_func(model, loader_valid):\n",
    "    model.eval()\n",
    "    valid_loss = []\n",
    "    outputs = []\n",
    "    th = 0.1\n",
    "    batch_metrics = [[]]\n",
    "    bar = tqdm(loader_valid)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with amp.autocast():\n",
    "            for images, save_paths in bar:\n",
    "                images = images.cuda()\n",
    "                logits = model(images)\n",
    "                preds = (logits.sigmoid() > th).float().detach().cpu().numpy()\n",
    "                y_preds = (preds+0.1).astype(np.uint8)\n",
    "                def save_mask(ind, preds = y_preds): \n",
    "                    compress(save_paths[ind], preds[ind])\n",
    "                \n",
    "                Parallel(n_jobs = len(y_preds))(delayed(save_mask)(i) for i in range(len(y_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms_train = transforms.Compose([\n",
    "#     transforms.RandFlipd(keys=[\"image\", \"mask\"], prob=0.5, spatial_axis=1),\n",
    "#     transforms.RandFlipd(keys=[\"image\", \"mask\"], prob=0.5, spatial_axis=2),\n",
    "#     transforms.RandAffined(keys=[\"image\", \"mask\"], translate_range=[int(x*y) for x, y in zip(image_sizes, [0.3, 0.3, 0.3])], padding_mode='zeros', prob=0.7),\n",
    "#     transforms.RandGridDistortiond(keys=(\"image\", \"mask\"), prob=0.5, distort_limit=(-0.01, 0.01), mode=\"nearest\"),    \n",
    "# ])\n",
    "\n",
    "transforms_valid = transforms.Compose([\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold):\n",
    "    model_file = os.path.join(model_dir, f'{seg_weight_name}')\n",
    "    # dataset_train = SEGDataset(df_train, 'valid')\n",
    "    dataset_train = SEGDataset(df_train, 'valid', transform=transforms_valid)\n",
    "    loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_MASK_PRED, shuffle=False, num_workers=BATCH_MASK_PRED)\n",
    "\n",
    "    model = TimmSegModel(backbone, pretrained=True)\n",
    "    model = convert_3d(model)\n",
    "\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    model = model.to(device)\n",
    "\n",
    "    print(len(dataset_train))\n",
    "    \n",
    "    infer_func(model, loader_train)\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                       | 0/4711 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./kaggle/input/rsna-2023-abdominal-trauma-detection/3d_preprocessed_crop_ratio/32072_35846.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "cannot do a non-empty take from an empty axes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[62], line 4\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(fold)\u001b[0m\n\u001b[1;32m      2\u001b[0m model_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseg_weight_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# dataset_train = SEGDataset(df_train, 'valid')\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m dataset_train \u001b[38;5;241m=\u001b[39m \u001b[43mSEGDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms_valid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m loader_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(dataset_train, batch_size\u001b[38;5;241m=\u001b[39mBATCH_MASK_PRED, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39mBATCH_MASK_PRED)\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m TimmSegModel(backbone, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[57], line 105\u001b[0m, in \u001b[0;36mSEGDataset.__init__\u001b[0;34m(self, df, mode, transform)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df))):\n\u001b[0;32m--> 105\u001b[0m     image, mask \u001b[38;5;241m=\u001b[39m \u001b[43mload_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages\u001b[38;5;241m.\u001b[39mappend(image)\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasks\u001b[38;5;241m.\u001b[39mappend(mask)\n",
      "Cell \u001b[0;32mIn[57], line 74\u001b[0m, in \u001b[0;36mload_sample\u001b[0;34m(row, has_mask)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_sample\u001b[39m(row, has_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     72\u001b[0m \n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# image = load_dicom_line_par(row.image_folder)\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mload_dicom_line_par\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcropped_path\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m     76\u001b[0m         image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(image, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# to 3ch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[57], line 56\u001b[0m, in \u001b[0;36mload_dicom_line_par\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     51\u001b[0m t_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path)),\n\u001b[1;32m     52\u001b[0m     key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(x))[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     55\u001b[0m n_scans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(t_paths)\n\u001b[0;32m---> 56\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_scans\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinspace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRESOL\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mround()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     57\u001b[0m t_paths \u001b[38;5;241m=\u001b[39m [t_paths[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices]\n\u001b[1;32m     59\u001b[0m images \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mquantile\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/numpy/lib/function_base.py:4461\u001b[0m, in \u001b[0;36mquantile\u001b[0;34m(a, q, axis, out, overwrite_input, method, keepdims, interpolation)\u001b[0m\n\u001b[1;32m   4459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _quantile_is_valid(q):\n\u001b[1;32m   4460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuantiles must be in the range [0, 1]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 4461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_quantile_unchecked\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4462\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/numpy/lib/function_base.py:4473\u001b[0m, in \u001b[0;36m_quantile_unchecked\u001b[0;34m(a, q, axis, out, overwrite_input, method, keepdims)\u001b[0m\n\u001b[1;32m   4465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_quantile_unchecked\u001b[39m(a,\n\u001b[1;32m   4466\u001b[0m                         q,\n\u001b[1;32m   4467\u001b[0m                         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4470\u001b[0m                         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   4471\u001b[0m                         keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   4472\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Assumes that q is in [0, 1], and is an ndarray\"\"\"\u001b[39;00m\n\u001b[0;32m-> 4473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ureduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4474\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_quantile_ureduce_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4475\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4476\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4477\u001b[0m \u001b[43m                    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4478\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4479\u001b[0m \u001b[43m                    \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4480\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/numpy/lib/function_base.py:3752\u001b[0m, in \u001b[0;36m_ureduce\u001b[0;34m(a, func, keepdims, **kwargs)\u001b[0m\n\u001b[1;32m   3749\u001b[0m             index_out \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, ) \u001b[38;5;241m*\u001b[39m nd\n\u001b[1;32m   3750\u001b[0m             kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m out[(\u001b[38;5;28mEllipsis\u001b[39m, ) \u001b[38;5;241m+\u001b[39m index_out]\n\u001b[0;32m-> 3752\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/numpy/lib/function_base.py:4639\u001b[0m, in \u001b[0;36m_quantile_ureduce_func\u001b[0;34m(a, q, axis, out, overwrite_input, method)\u001b[0m\n\u001b[1;32m   4637\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4638\u001b[0m         arr \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m-> 4639\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43m_quantile\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4640\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mquantiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4641\u001b[0m \u001b[43m                   \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4642\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4643\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4644\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/numpy/lib/function_base.py:4745\u001b[0m, in \u001b[0;36m_quantile\u001b[0;34m(arr, quantiles, axis, method, out)\u001b[0m\n\u001b[1;32m   4737\u001b[0m arr\u001b[38;5;241m.\u001b[39mpartition(\n\u001b[1;32m   4738\u001b[0m     np\u001b[38;5;241m.\u001b[39munique(np\u001b[38;5;241m.\u001b[39mconcatenate(([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m   4739\u001b[0m                               previous_indexes\u001b[38;5;241m.\u001b[39mravel(),\n\u001b[1;32m   4740\u001b[0m                               next_indexes\u001b[38;5;241m.\u001b[39mravel(),\n\u001b[1;32m   4741\u001b[0m                               ))),\n\u001b[1;32m   4742\u001b[0m     axis\u001b[38;5;241m=\u001b[39mDATA_AXIS)\n\u001b[1;32m   4743\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(arr\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minexact):\n\u001b[1;32m   4744\u001b[0m     slices_having_nans \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39misnan(\n\u001b[0;32m-> 4745\u001b[0m         \u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATA_AXIS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4746\u001b[0m     )\n\u001b[1;32m   4747\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4748\u001b[0m     slices_having_nans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mtake\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/numpy/core/fromnumeric.py:190\u001b[0m, in \u001b[0;36mtake\u001b[0;34m(a, indices, axis, out, mode)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_take_dispatcher)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtake\u001b[39m(a, indices, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     95\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    Take elements from an array along an axis.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03m           [5, 7]])\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtake\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mIndexError\u001b[0m: cannot do a non-empty take from an empty axes."
     ]
    }
   ],
   "source": [
    "run(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocss to get crop regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns GPU array\n",
    "def standardize_pixel_array(pixel_array, dcm_rows):\n",
    "    \"\"\"\n",
    "    Source : https://www.kaggle.com/competitions/rsna-2023-abdominal-trauma-detection/discussion/427217\n",
    "    \"\"\"\n",
    "    # Correct DICOM pixel_array if PixelRepresentation == 1.\n",
    "    for z in range(0, len(pixel_array)):\n",
    "        if int(dcm_rows[z]['PixelRepresentation']) == 1:\n",
    "            bit_shift = dcm_rows[z]['BitsAllocated'] - dcm_rows[z]['BitsStored']\n",
    "            dtype = pixel_array[z].dtype \n",
    "            pixel_array[z] = (pixel_array[z] << bit_shift).astype(dtype) >>  bit_shift\n",
    "\n",
    "    pixel_array = torch.from_numpy(pixel_array.astype(np.float16)).to(DEVICE).to(torch.float16)    \n",
    "\n",
    "    for z in range(0, len(pixel_array)):\n",
    "        intercept = float(dcm_rows[z]['RescaleIntercept'])\n",
    "        slope = float(dcm_rows[z]['RescaleSlope'])\n",
    "        center = int(dcm_rows[z]['WindowCenter'])\n",
    "        width = int(dcm_rows[z]['WindowWidth'])\n",
    "        low = center - width / 2\n",
    "        high = center + width / 2    \n",
    "        \n",
    "        pixel_array[z] = (pixel_array[z] * slope) + intercept\n",
    "        pixel_array[z] = torch.clip(pixel_array[z], low, high)\n",
    "        \n",
    "    gc.collect()    \n",
    "    return pixel_array\n",
    "\n",
    "#to make longest axis to z\n",
    "def get_new_axis_order(max_ind, n_dims=3):\n",
    "    origin_order = np.arange(0, 3)\n",
    "    new_order = origin_order.copy()\n",
    "    new_order[0] = max_ind\n",
    "    new_order[max_ind]=0\n",
    "    return new_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The order of the crop region data format\n",
    "#Z start/end, Y start/end, X start/end for each mask channels + total region for the extravasation prediction\n",
    "def calc_crop_region(mask):\n",
    "    crop_range = np.zeros((6, 6))\n",
    "    crop_range[:,::2]=10000\n",
    "    mask_z = np.max(mask, axis = (2, 3)).astype(bool)\n",
    "    mask_y = np.max(mask, axis = (1, 3)).astype(bool)\n",
    "    mask_x = np.max(mask, axis = (1, 2)).astype(bool)\n",
    "    \n",
    "    template_zrange = np.arange(0, RESOL[0])\n",
    "    template_yrange = np.arange(0, RESOL[1])\n",
    "    template_xrange = np.arange(0, RESOL[2])\n",
    "        \n",
    "    for mi in range(0, 5):\n",
    "        zrange = template_zrange[mask_z[mi]]\n",
    "        yrange = template_yrange[mask_y[mi]]\n",
    "        xrange = template_xrange[mask_x[mi]]\n",
    "        # For incomplete organ\n",
    "        if(len(zrange)==0):\n",
    "            zrange = template_zrange.copy()\n",
    "            yrange = template_yrange.copy()\n",
    "            xrange = template_xrange.copy()\n",
    "\n",
    "        crop_range[mi] = np.min(zrange), np.max(zrange)+1, np.min(yrange), np.max(yrange)+1, np.min(xrange), np.max(xrange)+1\n",
    "\n",
    "    crop_range[5] = np.min(crop_range[:5, 0]), np.max(crop_range[:5, 1]), np.min(crop_range[:5, 2]), \\\n",
    "                    np.max(crop_range[:5, 3]), np.min(crop_range[:5,4]), np.max(crop_range[:5, 5])\n",
    "    \n",
    "    crop_range[:,:2]/=len(mask_z[0])\n",
    "    crop_range[:,2:4]/=len(mask_y[0])\n",
    "    crop_range[:,4:6]/=len(mask_x[0])\n",
    "\n",
    "    # Then make extravasation (# 5 mask) to reference one and convert other mask's crop respective to it\n",
    "    # --> To minimize the loading size due to speed issue.\n",
    "    zmin, rel_zrange = crop_range[5,0], crop_range[5,1]-crop_range[5,0]\n",
    "    ymin, rel_yrange = crop_range[5,2], crop_range[5,3]-crop_range[5,2]\n",
    "    xmin, rel_xrange = crop_range[5,4], crop_range[5,5]-crop_range[5,4]\n",
    "\n",
    "    crop_range[:5,:2] = (crop_range[:5,:2]-zmin)/rel_zrange\n",
    "    crop_range[:5,2:4] = (crop_range[:5,2:4]-ymin)/rel_yrange\n",
    "    crop_range[:5,4:6] = (crop_range[:5,4:6]-xmin)/rel_xrange\n",
    "\n",
    "    return crop_range\n",
    "\n",
    "def crop_resize_avg_and_std_3d(data, region, resize_shape, is_norm = PREPROC_NORM_OR_STD):\n",
    "    shapes = data.shape\n",
    "    region[:2]*=shapes[0]\n",
    "    region[2:4]*=shapes[1]\n",
    "    region[4:6]*=shapes[2]\n",
    "    region = region.astype(int)\n",
    "\n",
    "    cropped = torch.clone(data[region[0]:region[1], region[2]:region[3], region[4]:region[5]])    \n",
    "    \n",
    "    #resize xy\n",
    "    cropped = transforms.Resize((int(resize_shape[1]), int(resize_shape[2])), antialias = True)(cropped)\n",
    "    \n",
    "    #zyx to xzy\n",
    "    cropped = torch.permute(cropped, (2, 0, 1))\n",
    "    #Resize yz\n",
    "    cropped = transforms.Resize((int(resize_shape[0]), int(resize_shape[1])), antialias = True)(cropped)\n",
    "    #xzy to zyx\n",
    "    cropped = torch.permute(cropped, (1, 2, 0))\n",
    "\n",
    "    if is_norm:\n",
    "        bottom = torch.min(cropped)\n",
    "        cropped -= bottom\n",
    "        top    = torch.max(cropped)\n",
    "        cropped/=top\n",
    "        del top, bottom\n",
    "    else:\n",
    "        avg = torch.mean(cropped, (0, 1, 2))\n",
    "        std = torch.std(cropped, (0, 1, 2))\n",
    "        cropped = (cropped-avg)/std\n",
    "        del avg, std\n",
    "        \n",
    "    del shapes, region\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return cropped\n",
    "\n",
    "# Read each slice and stack them to make 3d data\n",
    "def process_3d_crop(save_path, mask_path, resize_shapes, data_path = TRAIN_PATH):\n",
    "    tmp = save_path.split('/')[-1][:-4]\n",
    "    tmp = tmp.split('_')\n",
    "    patient, study = int(tmp[0]), int(tmp[1])\n",
    "    \n",
    "    mask = decompress(mask_path)\n",
    "    crop_regions = calc_crop_region(mask)\n",
    "    absolute_crop = crop_regions[5].copy() # To load minimum pixels...\n",
    "\n",
    "    del mask\n",
    "    gc.collect()\n",
    "    crop_regions[5] = 0, 1, 0, 1, 0, 1\n",
    "\n",
    "    imgs = {}    \n",
    "    for f in sorted(glob.glob(data_path + f'/{patient}/{study}/*.dcm')):  \n",
    "        pos_z = -int((f.split('/')[-1])[:-4])\n",
    "        imgs[pos_z] = f\n",
    "\n",
    "    imgs_3d = []\n",
    "    n_imgs = len(imgs)    \n",
    "    z_crop_range= (absolute_crop[0:2]*n_imgs).astype(int)\n",
    "    \n",
    "    dcm_rows = []\n",
    "    for i, k in enumerate(sorted(imgs.keys())):\n",
    "        if(i >= z_crop_range[0] and i < z_crop_range[1]):\n",
    "            IS_XY_CROP = False\n",
    "            f = imgs[k]\n",
    "            #Exception for the corrupted dicom file\n",
    "            if (f=='/kaggle/input/rsna-2023-abdominal-trauma-detection/test_images/3124/5842/514.dcm'):\n",
    "                continue\n",
    "            opened_dicom = dicomsdl.open(f)\n",
    "            img = opened_dicom.pixelData(storedvalue=True)\n",
    "            params = opened_dicom.getPixelDataInfo()\n",
    "\n",
    "            if not IS_XY_CROP:\n",
    "                img_shape = np.shape(img)\n",
    "                xy_crop_range = absolute_crop[2:].copy()   \n",
    "                xy_crop_range[0:2]*=img_shape[0]\n",
    "                xy_crop_range[2:4]*=img_shape[1]            \n",
    "                xy_crop_range = xy_crop_range.astype(int)                \n",
    "                IS_XY_CROP = True\n",
    "                \n",
    "            img = img[xy_crop_range[0]:xy_crop_range[1], xy_crop_range[2]:xy_crop_range[3]]             \n",
    "\n",
    "            #dcm_row = pd.DataFrame.from_dict(params)                   \n",
    "            dcm_rows.append(params)                  \n",
    "            imgs_3d.append(img[None])\n",
    "\n",
    "    del opened_dicom\n",
    "    gc.collect()\n",
    "                \n",
    "    imgs_3d = np.vstack(imgs_3d)\n",
    "\n",
    "    imgs_3d  = standardize_pixel_array(imgs_3d, dcm_rows)\n",
    "\n",
    "    min_imgs = torch.min(imgs_3d)\n",
    "    max_imgs = torch.max(imgs_3d)\n",
    "        \n",
    "    imgs_3d = ((imgs_3d - min_imgs) / (max_imgs - min_imgs + 1e-6))\n",
    "\n",
    "    if str(dcm_rows[0]['PhotometricInterpretation']) == \"MONOCHROME1\":\n",
    "        imgs_3d = 1.0 - imgs_3d\n",
    "\n",
    "    #Loaded original imgs_3d    \n",
    "    origin_shape = imgs_3d.shape\n",
    "    for i in range(0, 6):    \n",
    "        #To deal with almost not detected slices\n",
    "        try:   \n",
    "            # To deal with possible noises\n",
    "            if(((crop_regions[i,1]-crop_regions[i,0]) < 10/origin_shape[0]) or \n",
    "                ((crop_regions[i,3]-crop_regions[i,2]) < 10/origin_shape[1]) or \n",
    "                ((crop_regions[i,5]-crop_regions[i,4]) < 10/origin_shape[2])):\n",
    "                dummy_failure_function()\n",
    "            \n",
    "            processed_img_3d = (crop_resize_avg_and_std_3d(imgs_3d, crop_regions[i], resize_shapes[i])).to(torch.float16).to('cpu')\n",
    "            compress_fast(f'{save_path}_{i}', processed_img_3d)      \n",
    "\n",
    "            del processed_img_3d\n",
    "            gc.collect()\n",
    "        except:\n",
    "            processed_img_3d = (crop_resize_avg_and_std_3d(imgs_3d, np.array([0, 1, 0, 1, 0, 1]), resize_shapes[i])).to(torch.float16).to('cpu')\n",
    "            compress_fast(f'{save_path}_{i}', processed_img_3d)\n",
    "            del processed_img_3d\n",
    "            gc.collect()  \n",
    "\n",
    "    del imgs, img, imgs_3d\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "def calc_size_3d_crop(save_path, mask_path, data_path = TRAIN_PATH):\n",
    "    tmp = save_path.split('/')[-1][:-4]\n",
    "    tmp = tmp.split('_')\n",
    "    patient, study = int(tmp[0]), int(tmp[1])\n",
    "    \n",
    "    mask = decompress(mask_path)\n",
    "    crop_regions = calc_crop_region(mask)\n",
    "    absolute_crop = crop_regions[5].copy() # To load minimum pixels...\n",
    "\n",
    "    crop_regions[5] = 0, 1, 0, 1, 0, 1\n",
    "\n",
    "    imgs = {}    \n",
    "    \n",
    "    for f in sorted(glob.glob(data_path + f'/{patient}/{study}/*.dcm')):      \n",
    "        try:            \n",
    "            img = dicomsdl.open(f).pixelData(storedvalue=True)\n",
    "            img_shape = np.shape(img)\n",
    "            xy_crop_range = absolute_crop[2:].copy()   \n",
    "            xy_crop_range[0:2]*=img_shape[0]\n",
    "            xy_crop_range[2:4]*=img_shape[1]            \n",
    "            xy_crop_range = xy_crop_range.astype(int)\n",
    "            img = img.astype(float)\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    base_crop_shape = np.zeros(3, float)\n",
    "    base_crop_shape[:] = absolute_crop[1::2] - absolute_crop[0::2]\n",
    "    base_crop_shape[0]*= len(glob.glob(data_path + f'/{patient}/{study}/*.dcm'))\n",
    "    base_crop_shape[1:3] = xy_crop_range[1::2] - xy_crop_range[0::2]\n",
    "    \n",
    "    all_crop_shape = crop_regions[:,1::2] - crop_regions[:,0::2]\n",
    "    for i in range(0, 6):\n",
    "        all_crop_shape[i] *= base_crop_shape\n",
    "    del img\n",
    "    gc.collect()\n",
    "    return all_crop_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate average crop size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataset\n",
    "rng_samples = np.linspace(0, len(df_train), N_PROCESS_CROP+1, dtype = int)\n",
    "def process_3d_wrapper(process_ind, rng_samples = rng_samples, train_meta_df = df_train):\n",
    "    partial_crop_shapes = []\n",
    "    for i in tqdm(range(rng_samples[process_ind], rng_samples[process_ind+1])):\n",
    "        partial_crop_shapes.append(calc_size_3d_crop(df_train.iloc[i]['cropped_path'], df_train.iloc[i]['mask_path'])[None])\n",
    "    partial_crop_shapes = np.vstack(partial_crop_shapes)\n",
    "    return partial_crop_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 197/197 [00:20<00:00,  9.49it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:21<00:00,  9.02it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:24<00:00,  7.91it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:25<00:00,  7.57it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 197/197 [00:26<00:00,  7.37it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:27<00:00,  7.13it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:27<00:00,  7.12it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 197/197 [00:27<00:00,  7.13it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:27<00:00,  7.01it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:28<00:00,  6.98it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:28<00:00,  6.91it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 197/197 [00:28<00:00,  6.99it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:28<00:00,  6.94it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:28<00:00,  6.91it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:28<00:00,  6.90it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:28<00:00,  6.91it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 197/197 [00:28<00:00,  6.80it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:29<00:00,  6.74it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 197/197 [00:29<00:00,  6.64it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:29<00:00,  6.59it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:30<00:00,  6.53it/s]\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 194/197 [00:29<00:00, 15.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 570 ms, sys: 337 ms, total: 907 ms\n",
      "Wall time: 30.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:30<00:00,  6.51it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [00:30<00:00,  6.52it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 197/197 [00:30<00:00,  6.54it/s]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_crop_shapes = Parallel(n_jobs = N_PROCESS_CROP)(delayed(process_3d_wrapper)(i) for i in range(N_PROCESS_CROP))\n",
    "all_crop_shapes = np.vstack(all_crop_shapes)\n",
    "avg_all_crop_shapes = np.average(all_crop_shapes, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cropwise resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[131, 107, 148],\n",
       "       [107, 129, 150],\n",
       "       [105, 138, 144],\n",
       "       [ 77, 150, 180],\n",
       "       [ 85, 155, 158],\n",
       "       [123, 109, 155]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(0, len(avg_all_crop_shapes)):\n",
    "    cur_shape = avg_all_crop_shapes[i]\n",
    "    rescale_factor =  (CROP_RESOL**3/(cur_shape[0]*cur_shape[1]*cur_shape[2]))**(1/3)\n",
    "    avg_all_crop_shapes[i,:]*=rescale_factor\n",
    "\n",
    "avg_all_crop_shapes+=0.1\n",
    "avg_all_crop_shapes = avg_all_crop_shapes.astype(int)\n",
    "avg_all_crop_shapes\n",
    "\n",
    "# for i in range(0, len(avg_all_crop_shapes)):\n",
    "#     cur_shape = avg_all_crop_shapes[i]\n",
    "#     rescale_factor =  (CROP_RESOL**3/(cur_shape[0]*cur_shape[1]*cur_shape[2]))**(1/3)\n",
    "#     if(rescale_factor < 1):\n",
    "#         avg_all_crop_shapes[i,:]*=rescale_factor\n",
    "\n",
    "# avg_all_crop_shapes+=0.1\n",
    "# avg_all_crop_shapes = avg_all_crop_shapes.astype(int)\n",
    "# avg_all_crop_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataset\n",
    "rng_samples = np.linspace(0, len(df_train), N_PROCESS_CROP+1, dtype = int)\n",
    "def process_3d_wrapper(process_ind, rng_samples = rng_samples, train_meta_df = df_train, resize_shapes =  avg_all_crop_shapes):\n",
    "    for i in tqdm(range(rng_samples[process_ind], rng_samples[process_ind+1])):\n",
    "        if not os.path.isfile(f\"{df_train.iloc[i]['cropped_path']}_0\"):\n",
    "            process_3d_crop(train_meta_df.iloc[i]['cropped_path'], train_meta_df.iloc[i]['mask_path'], resize_shapes)     \n",
    "        \n",
    "        #For error correction\n",
    "        else:            \n",
    "            try:\n",
    "                data = decompress(f\"df_train.iloc[i]['cropped_path']_0\")\n",
    "            except: \n",
    "                process_3d_crop(train_meta_df.iloc[i]['cropped_path'], train_meta_df.iloc[i]['mask_path'], resize_shapes)  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [09:43<00:00,  2.98s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [09:48<00:00,  3.00s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [09:48<00:00,  3.00s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 197/197 [09:57<00:00,  3.03s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 197/197 [09:59<00:00,  3.04s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [10:01<00:00,  3.07s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 197/197 [10:02<00:00,  3.06s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [10:04<00:00,  3.08s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [10:06<00:00,  3.10s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 197/197 [10:11<00:00,  3.10s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [10:14<00:00,  3.13s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [10:14<00:00,  3.14s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [10:14<00:00,  3.14s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [10:15<00:00,  3.14s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [10:16<00:00,  3.15s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [10:16<00:00,  3.15s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [10:17<00:00,  3.15s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 197/197 [10:17<00:00,  3.13s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [10:17<00:00,  3.15s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [10:17<00:00,  3.15s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 197/197 [10:17<00:00,  3.14s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [10:18<00:00,  3.15s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 197/197 [10:18<00:00,  3.14s/it]\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 195/196 [10:18<00:01,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.79 s, sys: 816 ms, total: 4.6 s\n",
      "Wall time: 10min 20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [10:19<00:00,  3.16s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "Parallel(n_jobs = N_PROCESS_CROP)(delayed(process_3d_wrapper)(i) for i in range(N_PROCESS_CROP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 8\n",
    "cropped = decompress_fast(f\"{df_train.iloc[ind]['cropped_path']}_1\")\n",
    "mask    = decompress(df_train.iloc[ind]['mask_path'])\n",
    "img     = decompress(df_train.iloc[ind]['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardization not performed\n"
     ]
    }
   ],
   "source": [
    "if (np.abs(np.average(cropped.numpy().astype(np.float64))))>0.01:\n",
    "    print('standardization not performed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
