{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a02daf71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T13:29:38.125595Z",
     "iopub.status.busy": "2023-10-14T13:29:38.124549Z",
     "iopub.status.idle": "2023-10-14T13:32:17.576798Z",
     "shell.execute_reply": "2023-10-14T13:32:17.575377Z"
    },
    "papermill": {
     "duration": 159.464559,
     "end_time": "2023-10-14T13:32:17.579478",
     "exception": false,
     "start_time": "2023-10-14T13:29:38.114919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --no-deps -qq /kaggle/input/dicomsdl--0-109-2/dicomsdl-0.109.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n",
    "!pip install --no-deps -qq /kaggle/input/segmentation-models-pytorch-021/wheels/munch-2.5.0-py2.py3-none-any.whl\n",
    "!pip install --no-deps -qq /kaggle/input/segmentation-models-pytorch-021/wheels/pretrainedmodels-0.7.4-py3-none-any.whl\n",
    "!pip install --no-deps -qq /kaggle/input/segmentation-models-pytorch-021/wheels/timm-0.4.12-py3-none-any.whl\n",
    "!pip install --no-deps -qq /kaggle/input/segmentation-models-pytorch-021/wheels/efficientnet_pytorch-0.6.3-py3-none-any.whl\n",
    "!pip install --no-deps -qq /kaggle/input/segmentation-models-pytorch-021/wheels/segmentation_models_pytorch-0.2.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a733609",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-10-14T13:32:17.598458Z",
     "iopub.status.busy": "2023-10-14T13:32:17.597674Z",
     "iopub.status.idle": "2023-10-14T13:32:27.469037Z",
     "shell.execute_reply": "2023-10-14T13:32:27.468061Z"
    },
    "papermill": {
     "duration": 9.883758,
     "end_time": "2023-10-14T13:32:27.471726",
     "exception": false,
     "start_time": "2023-10-14T13:32:17.587968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import gc\n",
    "import sys\n",
    "import pydicom\n",
    "import dicomsdl\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torch.cuda.amp as amp\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "sys.path.append('/kaggle/input/resnet-3d-rsna-atd')\n",
    "sys.path.append('/kaggle/input/covn3d-same')\n",
    "sys.path.append('/kaggle/input/rsna-atd-lib')\n",
    "from resnet3d import generate_model\n",
    "import timm\n",
    "import timm_new\n",
    "\n",
    "#For dataloader using cuda\n",
    "torch.multiprocessing.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258127c7",
   "metadata": {
    "papermill": {
     "duration": 0.006733,
     "end_time": "2023-10-14T13:32:27.485741",
     "exception": false,
     "start_time": "2023-10-14T13:32:27.479008",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68447123",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T13:32:27.501104Z",
     "iopub.status.busy": "2023-10-14T13:32:27.500792Z",
     "iopub.status.idle": "2023-10-14T13:32:27.585143Z",
     "shell.execute_reply": "2023-10-14T13:32:27.584169Z"
    },
    "papermill": {
     "duration": 0.09479,
     "end_time": "2023-10-14T13:32:27.587420",
     "exception": false,
     "start_time": "2023-10-14T13:32:27.492630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESOL = 160\n",
    "BATCH_SIZE = 4\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEBUG = False\n",
    "N_DEBUG_SAMPLES = 20\n",
    "\n",
    "UP_RESOL = 128\n",
    "N_CHANNELS = 6\n",
    "PREPROC_NORM_OR_STD = False # True: normalization, False: standardization\n",
    "\n",
    "BASE_PATH = '/kaggle/input/rsna-2023-abdominal-trauma-detection'\n",
    "SAVE_PATH = '/tmp'\n",
    "MASK_SAVE_PATH = f'{SAVE_PATH}/mask_preprocessed'\n",
    "MASK_VALID_PATH = f'{SAVE_PATH}/mask_validation'\n",
    "\n",
    "DATA_PATH = f'{BASE_PATH}/test_images'\n",
    "if DEBUG:\n",
    "    DATA_PATH = f'{BASE_PATH}/train_images'\n",
    "N_PREPROCESS_CHUNKS = 4\n",
    "N_PROCESS_CROP = 4\n",
    "\n",
    "#kernel_type = 'timm3d_res50d_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_bs4_lr3e4_20x50ep'\n",
    "#kernel_type = 'timm3d_res50d_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_bs4_lr3e4_20x50ep_fold0_best_0.9.pth'\n",
    "seg_model_path = '/kaggle/input/rsna-atd-weights/231001_timm3d_res10tc_CV0.938.pt'\n",
    "load_kernel = None\n",
    "load_last = True\n",
    "n_blocks = 4\n",
    "backbone = 'timm/resnet10t.c3_in1k'\n",
    "\n",
    "backbone_classifier = 'timm/resnetrs50.tf_in1k'\n",
    "# weights_classifier = '/kaggle/input/1007-resnetrs50-cv0460/timm_resnetrs50.tf_in1k_lr0.0002_epochs_500_resol128_batch12-cv0460.pt'\n",
    "weights_classifier = '/kaggle/input/1007-resnetrs50-cv0460/timm_resnetrs50.tf_in1k_lr0.0002_epochs_500_resol128_batch12-cv0454.pt'\n",
    "weights_classifier1_2 = '/kaggle/input/1010-resnet50-cv04827/timm_resnetrs50.tf_in1k_lr0.0002_epochs_500_resol128_batch12_fold1-cv04827.pt'\n",
    "\n",
    "backbone_classifier2 = 'timm/resnet10t.c3_in1k'\n",
    "weights_classifier2 = '/kaggle/input/rsna-atd-weights/231002_timm_resnet10t.c3_in1k_lr0.005_epochs_200_resol128_batch24.pt2'\n",
    "\n",
    "backbone_classifiers3 = 'timm/resnet10t.c3_in1k'\n",
    "weights_classifiers3 = '/kaggle/input/rsna-atd-weights/timm_resnet10t_LSTM_CV0.44.pt'\n",
    "\n",
    "\n",
    "data_dir = '../input/rsna-2022-cervical-spine-fracture-detection'\n",
    "use_amp = True\n",
    "num_workers = 0\n",
    "out_dim = 5\n",
    "n_blocks = 4\n",
    "drop_rate = 0.2\n",
    "drop_path_rate = 0.2\n",
    "p_mixup = 0.0\n",
    "\n",
    "chan_keys = ['bowel', 'left_kidney', 'right_kidney', 'liver', 'spleen', 'total']\n",
    "chan_dict = {}\n",
    "for i in range(0, 6):\n",
    "    chan_dict[i] = chan_keys[i]\n",
    "\n",
    "log_dir = f'{SAVE_PATH}/seg_models_backup'\n",
    "model_dir = f'{SAVE_PATH}/seg_models_backup'\n",
    "seg_inference_dir = f'{SAVE_PATH}/seg_infer_results'\n",
    "origin_img_dir = f'{SAVE_PATH}/3d_preprocessed'\n",
    "cropped_img_dir   = f'{SAVE_PATH}/3d_preprocessed_crop'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(MASK_VALID_PATH, exist_ok=True)\n",
    "os.makedirs(seg_inference_dir, exist_ok = True)\n",
    "os.makedirs(cropped_img_dir, exist_ok = True)\n",
    "os.makedirs(origin_img_dir, exist_ok = True)\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f60fca69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T13:32:27.604122Z",
     "iopub.status.busy": "2023-10-14T13:32:27.603753Z",
     "iopub.status.idle": "2023-10-14T13:32:27.608490Z",
     "shell.execute_reply": "2023-10-14T13:32:27.607321Z"
    },
    "papermill": {
     "duration": 0.015618,
     "end_time": "2023-10-14T13:32:27.610517",
     "exception": false,
     "start_time": "2023-10-14T13:32:27.594899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transforms_valid = transforms.Compose([\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f972a164",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T13:32:27.627550Z",
     "iopub.status.busy": "2023-10-14T13:32:27.627217Z",
     "iopub.status.idle": "2023-10-14T13:32:27.684670Z",
     "shell.execute_reply": "2023-10-14T13:32:27.683308Z"
    },
    "papermill": {
     "duration": 0.068776,
     "end_time": "2023-10-14T13:32:27.687112",
     "exception": false,
     "start_time": "2023-10-14T13:32:27.618336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'{BASE_PATH}/train.csv')\n",
    "train_meta = pd.read_csv(f'{BASE_PATH}/train_series_meta.csv')\n",
    "train_df = train_df.sort_values(by=['patient_id'])\n",
    "\n",
    "n_chunk = 8\n",
    "patients = os.listdir(DATA_PATH)\n",
    "n_patients = len(patients)\n",
    "rng_patients = np.linspace(0, n_patients+1, n_chunk+1, dtype = int)\n",
    "patients_cts = glob.glob(f'{DATA_PATH}/*/*')\n",
    "n_cts = len(patients_cts)\n",
    "patients_cts_arr = np.zeros((n_cts, 2), int)\n",
    "data_paths=[]\n",
    "for i in range(0, n_cts):\n",
    "    patient, ct = patients_cts[i].split('/')[-2:]\n",
    "    patients_cts_arr[i] = patient, ct\n",
    "    data_paths.append(f'{SAVE_PATH}/3d_preprocessed/{patients_cts_arr[i,0]}_{patients_cts_arr[i,1]}.pkl')\n",
    "TRAIN_IMG_PATH = BASE_PATH + '/processed' \n",
    "\n",
    "#Generate tables for training\n",
    "df_data = pd.DataFrame(patients_cts_arr, columns = ['patient_id', 'series'])\n",
    "\n",
    "#5-fold splitting\n",
    "train_df['fold'] = 0\n",
    "labels = train_df[['bowel_healthy','bowel_injury',\n",
    "                    'extravasation_healthy','extravasation_injury',\n",
    "                    'kidney_healthy','kidney_low','kidney_high',\n",
    "                    'liver_healthy','liver_low','liver_high',\n",
    "                    'spleen_healthy','spleen_low','spleen_high',\n",
    "                    'any_injury']].to_numpy()\n",
    "\n",
    "\n",
    "#df_data = df_data.join(train_df.set_index('patient_id'), on='patient_id')\n",
    "df_data['path']=data_paths\n",
    "\n",
    "#For mask paths\n",
    "mask_paths = []\n",
    "cropped_paths = []\n",
    "for i in range(0, len(df_data)):\n",
    "    row = df_data.iloc[i]\n",
    "    file_name = row['path'].split('/')[-1]\n",
    "    mask_paths.append(f'{seg_inference_dir}/{file_name}')\n",
    "    cropped_paths.append(f'{cropped_img_dir}/{file_name}')\n",
    "df_data['mask_path'] = mask_paths\n",
    "df_data['cropped_path'] = cropped_paths\n",
    "\n",
    "df_data.to_csv(f'{SAVE_PATH}/data_meta.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f91ec7c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T13:32:27.705311Z",
     "iopub.status.busy": "2023-10-14T13:32:27.703707Z",
     "iopub.status.idle": "2023-10-14T13:32:27.717846Z",
     "shell.execute_reply": "2023-10-14T13:32:27.716710Z"
    },
    "papermill": {
     "duration": 0.024825,
     "end_time": "2023-10-14T13:32:27.719963",
     "exception": false,
     "start_time": "2023-10-14T13:32:27.695138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>series</th>\n",
       "      <th>path</th>\n",
       "      <th>mask_path</th>\n",
       "      <th>cropped_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63706</td>\n",
       "      <td>39279</td>\n",
       "      <td>/tmp/3d_preprocessed/63706_39279.pkl</td>\n",
       "      <td>/tmp/seg_infer_results/63706_39279.pkl</td>\n",
       "      <td>/tmp/3d_preprocessed_crop/63706_39279.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50046</td>\n",
       "      <td>24574</td>\n",
       "      <td>/tmp/3d_preprocessed/50046_24574.pkl</td>\n",
       "      <td>/tmp/seg_infer_results/50046_24574.pkl</td>\n",
       "      <td>/tmp/3d_preprocessed_crop/50046_24574.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48843</td>\n",
       "      <td>62825</td>\n",
       "      <td>/tmp/3d_preprocessed/48843_62825.pkl</td>\n",
       "      <td>/tmp/seg_infer_results/48843_62825.pkl</td>\n",
       "      <td>/tmp/3d_preprocessed_crop/48843_62825.pkl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id  series                                  path  \\\n",
       "0       63706   39279  /tmp/3d_preprocessed/63706_39279.pkl   \n",
       "1       50046   24574  /tmp/3d_preprocessed/50046_24574.pkl   \n",
       "2       48843   62825  /tmp/3d_preprocessed/48843_62825.pkl   \n",
       "\n",
       "                                mask_path  \\\n",
       "0  /tmp/seg_infer_results/63706_39279.pkl   \n",
       "1  /tmp/seg_infer_results/50046_24574.pkl   \n",
       "2  /tmp/seg_infer_results/48843_62825.pkl   \n",
       "\n",
       "                                cropped_path  \n",
       "0  /tmp/3d_preprocessed_crop/63706_39279.pkl  \n",
       "1  /tmp/3d_preprocessed_crop/50046_24574.pkl  \n",
       "2  /tmp/3d_preprocessed_crop/48843_62825.pkl  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7638b0c",
   "metadata": {
    "papermill": {
     "duration": 0.007362,
     "end_time": "2023-10-14T13:32:27.735949",
     "exception": false,
     "start_time": "2023-10-14T13:32:27.728587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocess images to 3D data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cdfd82",
   "metadata": {
    "papermill": {
     "duration": 0.006968,
     "end_time": "2023-10-14T13:32:27.750252",
     "exception": false,
     "start_time": "2023-10-14T13:32:27.743284",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Parameters\n",
    "Reference: https://www.kaggle.com/code/theoviel/get-started-quicker-dicom-png-conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55eece35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T13:32:27.766958Z",
     "iopub.status.busy": "2023-10-14T13:32:27.766098Z",
     "iopub.status.idle": "2023-10-14T13:32:27.930842Z",
     "shell.execute_reply": "2023-10-14T13:32:27.929409Z"
    },
    "papermill": {
     "duration": 0.175416,
     "end_time": "2023-10-14T13:32:27.932914",
     "exception": false,
     "start_time": "2023-10-14T13:32:27.757498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test patients : 3147\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = '/kaggle/input/rsna-2023-abdominal-trauma-detection'\n",
    "TEST_PATH = f'{BASE_PATH}/train_images/'\n",
    "\n",
    "print('Number of test patients :', len(os.listdir(TEST_PATH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99cb125e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T13:32:27.949714Z",
     "iopub.status.busy": "2023-10-14T13:32:27.949113Z",
     "iopub.status.idle": "2023-10-14T13:32:27.955658Z",
     "shell.execute_reply": "2023-10-14T13:32:27.954669Z"
    },
    "papermill": {
     "duration": 0.017155,
     "end_time": "2023-10-14T13:32:27.957735",
     "exception": false,
     "start_time": "2023-10-14T13:32:27.940580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_chunk = 4\n",
    "patients = os.listdir(TEST_PATH)\n",
    "#patients = patients[:8]\n",
    "n_patients = len(patients)\n",
    "\n",
    "#if(n_patients==3):\n",
    "#    n_chunk = 3\n",
    "#    rng_patients = np.linspace(0, n_patients, n_chunk+1, dtype = int)\n",
    "#else:\n",
    "rng_patients = np.linspace(0, n_patients, n_chunk+1, dtype = int)\n",
    "\n",
    "#rng_patients\n",
    "#patients\n",
    "#if DEBUG:\n",
    "#    rng_patients = [0, n_patients]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c037651c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T13:32:27.974962Z",
     "iopub.status.busy": "2023-10-14T13:32:27.974640Z",
     "iopub.status.idle": "2023-10-14T13:32:27.980384Z",
     "shell.execute_reply": "2023-10-14T13:32:27.979335Z"
    },
    "papermill": {
     "duration": 0.0163,
     "end_time": "2023-10-14T13:32:27.982368",
     "exception": false,
     "start_time": "2023-10-14T13:32:27.966068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compress(name, data):\n",
    "    with gzip.open(name, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def decompress(name):\n",
    "    with gzip.open(name, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "def compress_fast(name, data):\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def decompress_fast(name):\n",
    "    with open(name, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84f74158",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T13:32:27.998887Z",
     "iopub.status.busy": "2023-10-14T13:32:27.998001Z",
     "iopub.status.idle": "2023-10-14T13:32:28.003398Z",
     "shell.execute_reply": "2023-10-14T13:32:28.002474Z"
    },
    "papermill": {
     "duration": 0.015548,
     "end_time": "2023-10-14T13:32:28.005185",
     "exception": false,
     "start_time": "2023-10-14T13:32:27.989637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = TEST_PATH\n",
    "save_folder = '/tmp/3d_processed'\n",
    "\n",
    "try:\n",
    "    os.makedirs(save_folder)\n",
    "except:\n",
    "    print('save folder already exists!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44edee9",
   "metadata": {
    "papermill": {
     "duration": 0.007361,
     "end_time": "2023-10-14T13:32:28.020390",
     "exception": false,
     "start_time": "2023-10-14T13:32:28.013029",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43938e84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T13:32:28.036836Z",
     "iopub.status.busy": "2023-10-14T13:32:28.036499Z",
     "iopub.status.idle": "2023-10-14T13:32:28.044477Z",
     "shell.execute_reply": "2023-10-14T13:32:28.043358Z"
    },
    "papermill": {
     "duration": 0.018514,
     "end_time": "2023-10-14T13:32:28.046562",
     "exception": false,
     "start_time": "2023-10-14T13:32:28.028048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Returns GPU array\n",
    "def standardize_pixel_array(pixel_array, dcm_rows):\n",
    "    \"\"\"\n",
    "    Source : https://www.kaggle.com/competitions/rsna-2023-abdominal-trauma-detection/discussion/427217\n",
    "    \"\"\"\n",
    "    # Correct DICOM pixel_array if PixelRepresentation == 1.\n",
    "    for z in range(0, len(pixel_array)):\n",
    "        if int(dcm_rows[z]['PixelRepresentation']) == 1:\n",
    "            bit_shift = dcm_rows[z]['BitsAllocated'] - dcm_rows[z]['BitsStored']\n",
    "            dtype = pixel_array[z].dtype \n",
    "            pixel_array[z] = (pixel_array[z] << bit_shift).astype(dtype) >>  bit_shift\n",
    "\n",
    "    pixel_array = torch.from_numpy(pixel_array.astype(np.float16)).to(DEVICE).to(torch.float16)    \n",
    "\n",
    "    for z in range(0, len(pixel_array)):\n",
    "        intercept = float(dcm_rows[z]['RescaleIntercept'])\n",
    "        slope = float(dcm_rows[z]['RescaleSlope'])\n",
    "        center = int(dcm_rows[z]['WindowCenter'])\n",
    "        width = int(dcm_rows[z]['WindowWidth'])\n",
    "        low = center - width / 2\n",
    "        high = center + width / 2    \n",
    "        \n",
    "        pixel_array[z] = (pixel_array[z] * slope) + intercept\n",
    "        pixel_array[z] = torch.clip(pixel_array[z], low, high)\n",
    "        \n",
    "    gc.collect()    \n",
    "    return pixel_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c934c29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T13:32:28.063780Z",
     "iopub.status.busy": "2023-10-14T13:32:28.063436Z",
     "iopub.status.idle": "2023-10-14T13:32:28.076835Z",
     "shell.execute_reply": "2023-10-14T13:32:28.075692Z"
    },
    "papermill": {
     "duration": 0.024598,
     "end_time": "2023-10-14T13:32:28.079008",
     "exception": false,
     "start_time": "2023-10-14T13:32:28.054410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resize_norm_or_std(data, resize_shape, is_norm = PREPROC_NORM_OR_STD):  \n",
    "    #resize xy\n",
    "    data = transforms.Resize((int(resize_shape[1]), int(resize_shape[2])), antialias = True)(data)\n",
    "    \n",
    "    #zyx to xzy\n",
    "    data = torch.permute(data, (2, 0, 1))\n",
    "    #Resize yz\n",
    "    data = transforms.Resize((int(resize_shape[0]), int(resize_shape[1])), antialias = True)(data)\n",
    "    #xzy to zyx\n",
    "    data = torch.permute(data, (1, 2, 0))\n",
    "\n",
    "    if is_norm:\n",
    "        bottom = torch.min(data)\n",
    "        data -= bottom\n",
    "        top    = torch.max(data)\n",
    "        data/=top\n",
    "        del top, bottom\n",
    "    else:\n",
    "        avg = torch.mean(data, (0, 1, 2))\n",
    "        std = torch.std(data, (0, 1, 2))\n",
    "        data = (data-avg)/std\n",
    "        del avg, std\n",
    "\n",
    "    gc.collect()\n",
    "    #torch.cuda.empty_cache()\n",
    "    return data\n",
    "\n",
    "# Read each slice and stack them to make 3d data\n",
    "def process_3d(save_path, data_path = DATA_PATH):\n",
    "    tmp = save_path.split('/')[-1][:-4]\n",
    "    tmp = tmp.split('_')\n",
    "    patient, study = int(tmp[0]), int(tmp[1])\n",
    "    imgs = {}    \n",
    "    \n",
    "    # To load only needed slices\n",
    "    imgs = {}    \n",
    "    for f in sorted(glob.glob(data_path + f'/{patient}/{study}/*.dcm')):  \n",
    "        pos_z = -int((f.split('/')[-1])[:-4])\n",
    "        imgs[pos_z] = f\n",
    "        \n",
    "    sample_z = np.linspace(0, len(imgs)-1, RESOL, dtype=int)\n",
    "    dcm_rows = []\n",
    "    imgs_3d  = []\n",
    "    for i, k in enumerate(sorted(imgs.keys())):\n",
    "        if not np.isin([i], sample_z)[0]:\n",
    "            continue        \n",
    "        f= imgs[k]\n",
    "        opened_dicom = dicomsdl.open(f)\n",
    "        img = opened_dicom.pixelData(storedvalue=True)\n",
    "        params = opened_dicom.getPixelDataInfo()\n",
    "        \n",
    "        imgs_3d.append(img[None])\n",
    "        dcm_rows.append(params)\n",
    "\n",
    "    imgs_3d = np.vstack(imgs_3d)\n",
    "    imgs_3d = standardize_pixel_array(imgs_3d, dcm_rows)\n",
    "    \n",
    "    min_imgs = torch.min(imgs_3d)\n",
    "    max_imgs = torch.max(imgs_3d)\n",
    "        \n",
    "    imgs_3d = ((imgs_3d - min_imgs) / (max_imgs - min_imgs + 1e-6))\n",
    "\n",
    "    if str(dcm_rows[0]['PhotometricInterpretation']) == \"MONOCHROME1\":\n",
    "        imgs_3d = 1.0 - imgs_3d\n",
    "\n",
    "    imgs_3d = resize_norm_or_std(imgs_3d, [RESOL, RESOL, RESOL])\n",
    "\n",
    "    #Save the image\n",
    "    #compress(save_path, imgs_3d)         4             \n",
    "\n",
    "    del imgs, img\n",
    "    gc.collect()\n",
    "    return imgs_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f56dd91f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T13:32:28.096079Z",
     "iopub.status.busy": "2023-10-14T13:32:28.095629Z",
     "iopub.status.idle": "2023-10-14T13:32:28.101971Z",
     "shell.execute_reply": "2023-10-14T13:32:28.100841Z"
    },
    "papermill": {
     "duration": 0.017128,
     "end_time": "2023-10-14T13:32:28.103973",
     "exception": false,
     "start_time": "2023-10-14T13:32:28.086845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SEGDataset(Dataset):\n",
    "    def __init__(self, df, mode, transform):\n",
    "\n",
    "        self.df = df.reset_index()\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        \n",
    "        #try:\n",
    "        #    data_3d = decompress(row['path'])\n",
    "        #except:\n",
    "        data_3d = process_3d(row['path']).unsqueeze(0).to(torch.float16)\n",
    "            \n",
    "        #data_3d = torch.from_numpy(data_3d).to(torch.float32)\n",
    "        #file_name = row['path'].split('/')[-1]\n",
    "        #save_path = f'{seg_inference_dir}/{file_name}'\n",
    "        save_path = row['mask_path']\n",
    "\n",
    "        return data_3d, save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca6a5a4",
   "metadata": {
    "papermill": {
     "duration": 0.007495,
     "end_time": "2023-10-14T13:32:28.119207",
     "exception": false,
     "start_time": "2023-10-14T13:32:28.111712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Segmentation models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf685473",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T13:32:28.136575Z",
     "iopub.status.busy": "2023-10-14T13:32:28.135915Z",
     "iopub.status.idle": "2023-10-14T13:32:28.144835Z",
     "shell.execute_reply": "2023-10-14T13:32:28.143850Z"
    },
    "papermill": {
     "duration": 0.019928,
     "end_time": "2023-10-14T13:32:28.146845",
     "exception": false,
     "start_time": "2023-10-14T13:32:28.126917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TimmSegModel(nn.Module):\n",
    "    def __init__(self, backbone, segtype='unet', pretrained=False):\n",
    "        super(TimmSegModel, self).__init__()\n",
    "\n",
    "        self.encoder = timm_new.create_model(\n",
    "            backbone,\n",
    "            in_chans=1,\n",
    "            features_only=True,\n",
    "            drop_rate=drop_rate,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "            pretrained=False\n",
    "        )\n",
    "        g = self.encoder(torch.rand(1, 1, 64, 64))\n",
    "        encoder_channels = [1] + [_.shape[1] for _ in g]\n",
    "        decoder_channels = [256, 128, 64, 32, 16]\n",
    "        if segtype == 'unet':\n",
    "            self.decoder = smp.unet.decoder.UnetDecoder(\n",
    "                encoder_channels=encoder_channels[:n_blocks+1],\n",
    "                decoder_channels=decoder_channels[:n_blocks],\n",
    "                n_blocks=n_blocks,\n",
    "            )\n",
    "\n",
    "        self.segmentation_head = nn.Conv2d(decoder_channels[n_blocks-1], out_dim, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        global_features = [0] + self.encoder(x)[:n_blocks]\n",
    "        seg_features = self.decoder(*global_features)\n",
    "        seg_features = self.segmentation_head(seg_features)\n",
    "        return seg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58a1b3fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T13:32:28.164643Z",
     "iopub.status.busy": "2023-10-14T13:32:28.164287Z",
     "iopub.status.idle": "2023-10-14T13:32:28.182778Z",
     "shell.execute_reply": "2023-10-14T13:32:28.181761Z"
    },
    "papermill": {
     "duration": 0.03001,
     "end_time": "2023-10-14T13:32:28.185024",
     "exception": false,
     "start_time": "2023-10-14T13:32:28.155014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from timm.models.layers.conv2d_same import Conv2dSame\n",
    "from conv3d_same import Conv3dSame\n",
    "\n",
    "\n",
    "def convert_3d(module):\n",
    "\n",
    "    module_output = module\n",
    "    if isinstance(module, torch.nn.BatchNorm2d):\n",
    "        module_output = torch.nn.BatchNorm3d(\n",
    "            module.num_features,\n",
    "            module.eps,\n",
    "            module.momentum,\n",
    "            module.affine,\n",
    "            module.track_running_stats,\n",
    "        )\n",
    "        if module.affine:\n",
    "            with torch.no_grad():\n",
    "                module_output.weight = module.weight\n",
    "                module_output.bias = module.bias\n",
    "        module_output.running_mean = module.running_mean\n",
    "        module_output.running_var = module.running_var\n",
    "        module_output.num_batches_tracked = module.num_batches_tracked\n",
    "        if hasattr(module, \"qconfig\"):\n",
    "            module_output.qconfig = module.qconfig\n",
    "            \n",
    "    elif isinstance(module, Conv2dSame):\n",
    "        module_output = Conv3dSame(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.Conv2d):\n",
    "        module_output = torch.nn.Conv3d(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "            padding_mode=module.padding_mode\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.MaxPool2d):\n",
    "        module_output = torch.nn.MaxPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            dilation=module.dilation,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "    elif isinstance(module, torch.nn.AvgPool2d):\n",
    "        module_output = torch.nn.AvgPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "\n",
    "    for name, child in module.named_children():\n",
    "        module_output.add_module(\n",
    "            name, convert_3d(child)\n",
    "        )\n",
    "    del module\n",
    "\n",
    "    return module_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7e1c48",
   "metadata": {
    "papermill": {
     "duration": 0.007397,
     "end_time": "2023-10-14T13:32:28.200380",
     "exception": false,
     "start_time": "2023-10-14T13:32:28.192983",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Semgmentation inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18ed2330",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T13:32:28.217269Z",
     "iopub.status.busy": "2023-10-14T13:32:28.216317Z",
     "iopub.status.idle": "2023-10-14T13:32:28.223735Z",
     "shell.execute_reply": "2023-10-14T13:32:28.222824Z"
    },
    "papermill": {
     "duration": 0.01801,
     "end_time": "2023-10-14T13:32:28.225806",
     "exception": false,
     "start_time": "2023-10-14T13:32:28.207796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def infer_func(model, loader_valid):\n",
    "    model.eval()\n",
    "    ths = [0.1]\n",
    "    bar = tqdm(loader_valid)\n",
    "    counter = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with amp.autocast():\n",
    "            for images, save_paths in bar:\n",
    "                images = images.cuda()\n",
    "                logits = model(images)\n",
    "                for thi, th in enumerate(ths):\n",
    "                    for i in range(logits.shape[0]):                    \n",
    "                        y_pred = ((logits[i].sigmoid()> th).float().detach().cpu().numpy()+0.1).astype(np.uint8)\n",
    "                        compress(save_paths[i], y_pred)\n",
    "    del images, logits, y_pred\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80790d89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T13:32:28.245834Z",
     "iopub.status.busy": "2023-10-14T13:32:28.244863Z",
     "iopub.status.idle": "2023-10-14T13:32:28.255401Z",
     "shell.execute_reply": "2023-10-14T13:32:28.254508Z"
    },
    "papermill": {
     "duration": 0.021198,
     "end_time": "2023-10-14T13:32:28.257243",
     "exception": false,
     "start_time": "2023-10-14T13:32:28.236045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(f'{SAVE_PATH}/data_meta.csv')\n",
    "if DEBUG:\n",
    "    df_data = df_data.iloc[:N_DEBUG_SAMPLES]\n",
    "\n",
    "mask_paths = []\n",
    "cropped_paths = []\n",
    "for i in range(0, len(df_data)):\n",
    "    row = df_data.iloc[i]\n",
    "    file_name = row['path'].split('/')[-1]\n",
    "    mask_paths.append(f'{seg_inference_dir}/{file_name}')\n",
    "    cropped_paths.append(f'{cropped_img_dir}/{file_name}')\n",
    "df_data['mask_path'] = mask_paths\n",
    "df_data['cropped_path'] = cropped_paths\n",
    "df_data.tail()\n",
    "\n",
    "df_data.to_csv(f'{SAVE_PATH}/data_meta.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57c525de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T13:32:28.274501Z",
     "iopub.status.busy": "2023-10-14T13:32:28.273631Z",
     "iopub.status.idle": "2023-10-14T13:32:28.280294Z",
     "shell.execute_reply": "2023-10-14T13:32:28.279340Z"
    },
    "papermill": {
     "duration": 0.017389,
     "end_time": "2023-10-14T13:32:28.282386",
     "exception": false,
     "start_time": "2023-10-14T13:32:28.264997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run(fold):\n",
    "    model_file = seg_model_path\n",
    "\n",
    "    dataset_train = SEGDataset(df_data, 'valid', transform=transforms_valid)\n",
    "    loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    model = TimmSegModel(backbone, pretrained=True)\n",
    "    model = convert_3d(model)\n",
    "\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    print(len(dataset_train))\n",
    "\n",
    "    infer_func(model, loader_train)\n",
    "\n",
    "    del model, dataset_train, loader_train\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfc2cfd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T13:32:28.299485Z",
     "iopub.status.busy": "2023-10-14T13:32:28.298567Z",
     "iopub.status.idle": "2023-10-14T13:32:43.730313Z",
     "shell.execute_reply": "2023-10-14T13:32:43.729156Z"
    },
    "papermill": {
     "duration": 15.442932,
     "end_time": "2023-10-14T13:32:43.732849",
     "exception": false,
     "start_time": "2023-10-14T13:32:28.289917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.33s/it]\n"
     ]
    }
   ],
   "source": [
    "run(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb0fb38c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T13:32:43.751567Z",
     "iopub.status.busy": "2023-10-14T13:32:43.751183Z",
     "iopub.status.idle": "2023-10-14T13:32:43.755887Z",
     "shell.execute_reply": "2023-10-14T13:32:43.754731Z"
    },
    "papermill": {
     "duration": 0.01642,
     "end_time": "2023-10-14T13:32:43.757934",
     "exception": false,
     "start_time": "2023-10-14T13:32:43.741514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!cp /tmp/seg_infer_results/26883_57967* /kaggle/working"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6045010d",
   "metadata": {
    "papermill": {
     "duration": 0.008211,
     "end_time": "2023-10-14T13:32:43.774559",
     "exception": false,
     "start_time": "2023-10-14T13:32:43.766348",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## New Crop operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dba2190d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T13:32:43.793069Z",
     "iopub.status.busy": "2023-10-14T13:32:43.792384Z",
     "iopub.status.idle": "2023-10-14T13:32:43.800691Z",
     "shell.execute_reply": "2023-10-14T13:32:43.799502Z"
    },
    "papermill": {
     "duration": 0.019802,
     "end_time": "2023-10-14T13:32:43.802723",
     "exception": false,
     "start_time": "2023-10-14T13:32:43.782921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Returns GPU array\n",
    "def standardize_pixel_array(pixel_array, dcm_rows):\n",
    "    \"\"\"\n",
    "    Source : https://www.kaggle.com/competitions/rsna-2023-abdominal-trauma-detection/discussion/427217\n",
    "    \"\"\"\n",
    "    # Correct DICOM pixel_array if PixelRepresentation == 1.\n",
    "    #pixel_array = dcm.pixel_array\n",
    "    #pixel_array = cp.array(pixel_array)    \n",
    "    for z in range(0, len(pixel_array)):\n",
    "        if int(dcm_rows[z]['PixelRepresentation']) == 1:\n",
    "            bit_shift = dcm_rows[z]['BitsAllocated'] - dcm_rows[z]['BitsStored']\n",
    "            dtype = pixel_array[z].dtype \n",
    "            pixel_array[z] = (pixel_array[z] << bit_shift).astype(dtype) >>  bit_shift\n",
    "    #         pixel_array = pydicom.pixel_data_handlers.util.apply_modality_lut(new_array, dcm)\n",
    "\n",
    "    pixel_array = torch.from_numpy(pixel_array.astype(np.float16)).to(DEVICE).to(torch.float32)    \n",
    "\n",
    "    for z in range(0, len(pixel_array)):\n",
    "        intercept = float(dcm_rows[z]['RescaleIntercept'])\n",
    "        slope = float(dcm_rows[z]['RescaleSlope'])\n",
    "        center = int(dcm_rows[z]['WindowCenter'])\n",
    "        width = int(dcm_rows[z]['WindowWidth'])\n",
    "        low = center - width / 2\n",
    "        high = center + width / 2    \n",
    "        \n",
    "        pixel_array[z] = (pixel_array[z] * slope) + intercept\n",
    "        pixel_array[z] = torch.clip(pixel_array[z], low, high)\n",
    "        \n",
    "    gc.collect()\n",
    "    \n",
    "    return pixel_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37e847d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T13:32:43.821396Z",
     "iopub.status.busy": "2023-10-14T13:32:43.821012Z",
     "iopub.status.idle": "2023-10-14T13:32:43.846678Z",
     "shell.execute_reply": "2023-10-14T13:32:43.845590Z"
    },
    "papermill": {
     "duration": 0.037502,
     "end_time": "2023-10-14T13:32:43.848644",
     "exception": false,
     "start_time": "2023-10-14T13:32:43.811142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#The order of the crop region data format\n",
    "#Z start/end, Y start/end, X start/end for each mask channels + total region for the extravasation prediction\n",
    "def calc_crop_region(mask):\n",
    "    crop_range = np.zeros((6, 6))\n",
    "    crop_range[:,::2]=10000\n",
    "    mask_z = np.max(mask, axis = (2, 3)).astype(bool)\n",
    "    mask_y = np.max(mask, axis = (1, 3)).astype(bool)\n",
    "    mask_x = np.max(mask, axis = (1, 2)).astype(bool)\n",
    "    \n",
    "    template_range = np.arange(0, RESOL)\n",
    "\n",
    "    for mi in range(0, 5):\n",
    "        zrange = template_range[mask_z[mi]]\n",
    "        yrange = template_range[mask_y[mi]]\n",
    "        xrange = template_range[mask_x[mi]]\n",
    "        # For incomplete organ\n",
    "        if(len(zrange)==0):\n",
    "            zrange = template_range.copy()\n",
    "            yrange = template_range.copy()\n",
    "            xrange = template_range.copy()\n",
    "\n",
    "        crop_range[mi] = np.min(zrange), np.max(zrange)+1, np.min(yrange), np.max(yrange)+1, np.min(xrange), np.max(xrange)+1\n",
    "\n",
    "    crop_range[5] = np.min(crop_range[:5, 0]), np.max(crop_range[:5, 1]), np.min(crop_range[:5, 2]), \\\n",
    "                    np.max(crop_range[:5, 3]), np.min(crop_range[:5,4]), np.max(crop_range[:5, 5])\n",
    "    \n",
    "    crop_range[:,:2]/=len(mask_z[0])\n",
    "    crop_range[:,2:4]/=len(mask_y[0])\n",
    "    crop_range[:,4:6]/=len(mask_x[0])\n",
    "\n",
    "    # Then make extravasation (# 5 mask) to reference one and convert other mask's crop respective to it\n",
    "    # --> To minimize the loading size due to speed issue.\n",
    "    zmin, rel_zrange = crop_range[5,0], crop_range[5,1]-crop_range[5,0]\n",
    "    ymin, rel_yrange = crop_range[5,2], crop_range[5,3]-crop_range[5,2]\n",
    "    xmin, rel_xrange = crop_range[5,4], crop_range[5,5]-crop_range[5,4]\n",
    "\n",
    "    crop_range[:5,:2] = (crop_range[:5,:2]-zmin)/rel_zrange\n",
    "    crop_range[:5,2:4] = (crop_range[:5,2:4]-ymin)/rel_yrange\n",
    "    crop_range[:5,4:6] = (crop_range[:5,4:6]-xmin)/rel_xrange\n",
    "\n",
    "    return crop_range\n",
    "\n",
    "def crop_resize_avg_and_std_3d(data, region, resize_shape):  \n",
    "    shapes = data.shape\n",
    "    region[:2]*=shapes[0]\n",
    "    region[2:4]*=shapes[1]\n",
    "    region[4:6]*=shapes[2]\n",
    "    region = region.astype(int)\n",
    "\n",
    "    cropped = torch.clone(data[region[0]:region[1], region[2]:region[3], region[4]:region[5]])    \n",
    "\n",
    "    #resize xy\n",
    "    cropped = transforms.Resize((int(resize_shape[1]), int(resize_shape[2])), antialias = True)(cropped)\n",
    "    #slices = []\n",
    "    #for i in range(0, len(cropped)):\n",
    "    #    slices.append(cv2.resize(cropped[i], (resize_shape[2], resize_shape[1]))[None])\n",
    "    \n",
    "    #slices = np.vstack(slices)\n",
    "    #resized_cropped = np.zeros(resize_shape)\n",
    "    \n",
    "    #zyx to xzy\n",
    "    cropped = torch.permute(cropped, (2, 0, 1))\n",
    "    cropped = transforms.Resize((int(resize_shape[0]), int(resize_shape[1])), antialias = True)(cropped)\n",
    "    #xzy to zyx\n",
    "    cropped = torch.permute(cropped, (1, 2, 0))\n",
    "    #for i in range(0, len(slices[0,0])):\n",
    "    #    resized_cropped[:,:,i] = cv2.resize(slices[:,:,i], (resize_shape[1], resize_shape[0]))\n",
    "\n",
    "        \n",
    "    #std = torch.std(cropped, (0, 1, 2))\n",
    "    #avg = torch.mean(cropped, (0, 1, 2))\n",
    "    min_imgs = torch.min(cropped)\n",
    "    max_imgs = torch.max(cropped)\n",
    "    #for debugging   \n",
    "    cropped = ((cropped - min_imgs) / (max_imgs - min_imgs + 1e-6))\n",
    "    \n",
    "    del min_imgs, max_imgs, shapes, region\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return cropped\n",
    "\n",
    "# Read each slice and stack them to make 3d data\n",
    "def process_3d_crop(save_path, mask_path, resize_shapes, data_path = DATA_PATH):\n",
    "    tmp = save_path.split('/')[-1][:-4]\n",
    "    tmp = tmp.split('_')\n",
    "    patient, study = int(tmp[0]), int(tmp[1])\n",
    "    \n",
    "    mask = decompress(mask_path)\n",
    "    crop_regions = calc_crop_region(mask)\n",
    "    absolute_crop = crop_regions[5].copy() # To load minimum pixels...\n",
    "\n",
    "    del mask\n",
    "    gc.collect()\n",
    "    crop_regions[5] = 0, 1, 0, 1, 0, 1\n",
    "\n",
    "    imgs = {}    \n",
    "    \n",
    "    for f in sorted(glob.glob(data_path + f'/{patient}/{study}/*.dcm')):  \n",
    "        pos_z = -int((f.split('/')[-1])[:-4])\n",
    "        imgs[pos_z] = f\n",
    "\n",
    "    imgs_3d = []\n",
    "    n_imgs = len(imgs)    \n",
    "    z_crop_range= (absolute_crop[0:2]*n_imgs).astype(int)\n",
    "    #print(z_crop_range)\n",
    "    \n",
    "    dcm_rows = []\n",
    "    for i, k in enumerate(sorted(imgs.keys())):\n",
    "        #if i in sample_z:\n",
    "        if(i >= z_crop_range[0] and i < z_crop_range[1]):\n",
    "            IS_XY_CROP = False\n",
    "            f = imgs[k]\n",
    "            #Exception for the corrupted dicom file\n",
    "            if (f=='/kaggle/input/rsna-2023-abdominal-trauma-detection/test_images/3124/5842/514.dcm'):\n",
    "                continue\n",
    "            #try:            \n",
    "            opened_dicom = dicomsdl.open(f)\n",
    "            img = opened_dicom.pixelData(storedvalue=True)\n",
    "            params = opened_dicom.getPixelDataInfo()\n",
    "\n",
    "            if not IS_XY_CROP:\n",
    "                img_shape = np.shape(img)\n",
    "                xy_crop_range = absolute_crop[2:].copy()   \n",
    "                xy_crop_range[0:2]*=img_shape[0]\n",
    "                xy_crop_range[2:4]*=img_shape[1]            \n",
    "                xy_crop_range = xy_crop_range.astype(int)                \n",
    "                IS_XY_CROP = True\n",
    "                \n",
    "            img = img[xy_crop_range[0]:xy_crop_range[1], xy_crop_range[2]:xy_crop_range[3]]             \n",
    "\n",
    "            #dcm_row = pd.DataFrame.from_dict(params)                   \n",
    "            dcm_rows.append(params)                  \n",
    "            imgs_3d.append(img[None])\n",
    "\n",
    "    del opened_dicom\n",
    "    gc.collect()\n",
    "                \n",
    "    imgs_3d = np.vstack(imgs_3d)\n",
    "\n",
    "    imgs_3d  = standardize_pixel_array(imgs_3d, dcm_rows)\n",
    "\n",
    "    min_imgs = torch.min(imgs_3d)\n",
    "    max_imgs = torch.max(imgs_3d)\n",
    "    for i in range(0, 2):\n",
    "        min_imgs = torch.min(min_imgs)\n",
    "        max_imgs = torch.max(max_imgs)\n",
    "        \n",
    "    imgs_3d = ((imgs_3d - min_imgs) / (max_imgs - min_imgs + 1e-6))\n",
    "\n",
    "    #print(dcm_rows[0].PhotometricInterpretation)\n",
    "    if str(dcm_rows[0]['PhotometricInterpretation']) == \"MONOCHROME1\":\n",
    "        imgs_3d = 1.0 - imgs_3d\n",
    "\n",
    "    #Loaded original imgs_3d    \n",
    "    #processed_img_3d = np.zeros((6, RESOL, RESOL, RESOL))\n",
    "    \n",
    "    origin_shape = imgs_3d.shape\n",
    "    for i in range(0, 6):    \n",
    "        #To deal with almost not detected slices\n",
    "        try:   \n",
    "            # To deal with possible noises\n",
    "            if(((crop_regions[i,1]-crop_regions[i,0]) < 10/origin_shape[0]) or \n",
    "                ((crop_regions[i,3]-crop_regions[i,2]) < 10/origin_shape[1]) or \n",
    "                ((crop_regions[i,5]-crop_regions[i,4]) < 10/origin_shape[2])):\n",
    "                dummy_failure_function()\n",
    "            \n",
    "            processed_img_3d = (crop_resize_avg_and_std_3d(imgs_3d, crop_regions[i], resize_shapes[i])).to(torch.float16).to('cpu')\n",
    "            compress_fast(f'{save_path}_{i}', processed_img_3d)      \n",
    "\n",
    "            del processed_img_3d\n",
    "            gc.collect()\n",
    "        except:\n",
    "            processed_img_3d = (crop_resize_avg_and_std_3d(imgs_3d, np.array([0, 1, 0, 1, 0, 1]), resize_shapes[i])).to(torch.float16).to('cpu')\n",
    "            compress_fast(f'{save_path}_{i}', processed_img_3d)\n",
    "            del processed_img_3d\n",
    "            gc.collect()  \n",
    "\n",
    "    del imgs, img, imgs_3d\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1569d9a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T13:32:43.867389Z",
     "iopub.status.busy": "2023-10-14T13:32:43.866318Z",
     "iopub.status.idle": "2023-10-14T13:32:43.874368Z",
     "shell.execute_reply": "2023-10-14T13:32:43.873161Z"
    },
    "papermill": {
     "duration": 0.019633,
     "end_time": "2023-10-14T13:32:43.876622",
     "exception": false,
     "start_time": "2023-10-14T13:32:43.856989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "resize_shapes = np.zeros((6, 3), int)\n",
    "resize_shapes[0] = 131, 107, 148\n",
    "resize_shapes[1] = 107, 129, 150\n",
    "resize_shapes[2] = 105, 138, 144\n",
    "resize_shapes[3] = 77, 150, 180\n",
    "resize_shapes[4] =  85, 155, 158\n",
    "resize_shapes[5] = 123, 109, 155\n",
    "\n",
    "# Preprocess dataset\n",
    "rng_samples = np.linspace(0, len(df_data), N_PROCESS_CROP+1, dtype = int)\n",
    "def process_3d_wrapper(process_ind, rng_samples = rng_samples, data_meta_df = df_data, resize_shapes = resize_shapes):\n",
    "    for i in tqdm(range(rng_samples[process_ind], rng_samples[process_ind+1])):\n",
    "        process_3d_crop(data_meta_df.iloc[i]['cropped_path'], data_meta_df.iloc[i]['mask_path'], resize_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7ab067d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T13:32:43.896329Z",
     "iopub.status.busy": "2023-10-14T13:32:43.895640Z",
     "iopub.status.idle": "2023-10-14T13:33:00.171305Z",
     "shell.execute_reply": "2023-10-14T13:33:00.169472Z"
    },
    "papermill": {
     "duration": 16.28769,
     "end_time": "2023-10-14T13:33:00.173944",
     "exception": false,
     "start_time": "2023-10-14T13:32:43.886254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 208 ms, sys: 289 ms, total: 498 ms\n",
      "Wall time: 16.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.55s/it]\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.50s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "Parallel(n_jobs = N_PROCESS_CROP)(delayed(process_3d_wrapper)(i) for i in range(N_PROCESS_CROP))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c6eced",
   "metadata": {
    "papermill": {
     "duration": 0.023939,
     "end_time": "2023-10-14T13:33:00.234164",
     "exception": false,
     "start_time": "2023-10-14T13:33:00.210225",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Final inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "112e9ada",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T13:33:00.266912Z",
     "iopub.status.busy": "2023-10-14T13:33:00.266436Z",
     "iopub.status.idle": "2023-10-14T13:33:00.309729Z",
     "shell.execute_reply": "2023-10-14T13:33:00.308659Z"
    },
    "papermill": {
     "duration": 0.063552,
     "end_time": "2023-10-14T13:33:00.312053",
     "exception": false,
     "start_time": "2023-10-14T13:33:00.248501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_3d(module):\n",
    "    module_output = module\n",
    "    if isinstance(module, torch.nn.BatchNorm2d):\n",
    "        module_output = torch.nn.BatchNorm3d(\n",
    "            module.num_features,\n",
    "            module.eps,\n",
    "            module.momentum,\n",
    "            module.affine,\n",
    "            module.track_running_stats,\n",
    "        )\n",
    "        if module.affine:\n",
    "            with torch.no_grad():\n",
    "                module_output.weight = module.weight\n",
    "                module_output.bias = module.bias\n",
    "        module_output.running_mean = module.running_mean\n",
    "        module_output.running_var = module.running_var\n",
    "        module_output.num_batches_tracked = module.num_batches_tracked\n",
    "        if hasattr(module, \"qconfig\"):\n",
    "            module_output.qconfig = module.qconfig\n",
    "            \n",
    "    elif isinstance(module, Conv2dSame):\n",
    "        module_output = Conv3dSame(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.Conv2d):\n",
    "        module_output = torch.nn.Conv3d(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "            padding_mode=module.padding_mode\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.MaxPool2d):\n",
    "        module_output = torch.nn.MaxPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            dilation=module.dilation,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "    elif isinstance(module, torch.nn.AvgPool2d):\n",
    "        module_output = torch.nn.AvgPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "\n",
    "    for name, child in module.named_children():\n",
    "        module_output.add_module(\n",
    "            name, convert_3d(child)\n",
    "        )\n",
    "    del module\n",
    "\n",
    "    return module_output\n",
    "\n",
    "\n",
    "class Timm3DModel(nn.Module):\n",
    "    def __init__(self, backbone, n_channels, n_labels, segtype='unet', pretrained=False):\n",
    "        super(Timm3DModel, self).__init__()\n",
    "        self.n_labels = n_labels\n",
    "        self.encoder = timm_new.create_model(\n",
    "            backbone,\n",
    "            in_chans=n_channels,\n",
    "            features_only=True,\n",
    "            drop_rate=drop_rate,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "        g = self.encoder(torch.rand(1, n_channels, 64, 64))\n",
    "        encoder_channels = [1] + [_.shape[1] for _ in g]\n",
    "        decoder_channels = [256, 128, 64, 32, 16]\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(5, 4, 2)\n",
    "        \n",
    "        [_.shape[1] for _ in g]\n",
    "        self.convs1x1 = nn.ModuleList()    \n",
    "        self.batchnorms = nn.ModuleList()    \n",
    "        self.batchnorms13 = nn.ModuleList()\n",
    "        for i in range(0, len(g)):\n",
    "            self.convs1x1.append(nn.Conv2d(g[i].shape[1], self.n_labels, 1))\n",
    "        del g\n",
    "        gc.collect()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        batch_size = x.shape[0]\n",
    "        global_features = self.encoder(x)[:n_blocks]        \n",
    "        for i in range(0, len(global_features)):\n",
    "            global_features[i] = self.convs1x1[i](global_features[i])\n",
    "        return global_features\n",
    "    \n",
    "    \n",
    "class Timm3DModelClassifier(nn.Module):\n",
    "    def __init__(self, backbone, n_channels, n_labels, segtype='unet', pretrained=False):\n",
    "        super(Timm3DModelClassifier, self).__init__()\n",
    "        self.model_3d = Timm3DModel(backbone, n_channels, n_labels, segtype, pretrained)\n",
    "        self.model_3d = convert_3d(self.model_3d)\n",
    "        self.n_channels = n_channels\n",
    "        self.n_labels = n_labels    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.model_3d(x)\n",
    "        pooled_features = []\n",
    "        for i in range(0, len(x)):\n",
    "            pooled_features.append(torch.reshape(torch.mean(x[i], dim = (2, 3, 4)), (batch_size, self.n_labels, 1)))\n",
    "        pooled_features = torch.cat(pooled_features, dim=2)     \n",
    "        labels = torch.mean(pooled_features, dim = 2)\n",
    "        return labels\n",
    "    \n",
    "class AbdominalClassifier(nn.Module):\n",
    "    def __init__(self, device = DEVICE):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.upsample = torch.nn.Upsample(size = [UP_RESOL, UP_RESOL, UP_RESOL])\n",
    "        \n",
    "        self.model3d_bowel        = Timm3DModelClassifier(backbone_classifier, 1, 2)      \n",
    "        self.model3d_extrav       = Timm3DModelClassifier(backbone_classifier, 1, 2)\n",
    "        self.model3d_kidney_left  = Timm3DModelClassifier(backbone_classifier, 1, 3)\n",
    "        self.model3d_kidney_right = Timm3DModelClassifier(backbone_classifier, 1, 3)\n",
    "        self.model3d_liver        = Timm3DModelClassifier(backbone_classifier, 1, 3)\n",
    "        self.model3d_spleen       = Timm3DModelClassifier(backbone_classifier, 1, 3)\n",
    "        \n",
    "        self.flatten  = nn.Flatten()\n",
    "        self.dropout  = nn.Dropout(p=0.5)\n",
    "        self.softmax  = nn.Softmax(dim=1)        \n",
    "        self.maxpool  = nn.MaxPool1d(5, 1)\n",
    "        \n",
    "    def forward(self, x_bowel, x_kidney_left, x_kidney_right, x_liver, x_spleen, x_total):\n",
    "        bowel_label        = self.model3d_bowel(x_bowel)\n",
    "        extrav_label       = self.model3d_extrav(x_total)\n",
    "        kidney_label_left  = self.model3d_kidney_left(x_kidney_left)\n",
    "        kidney_label_right = self.model3d_kidney_right(x_kidney_right)\n",
    "        kidney_label       = (kidney_label_left + kidney_label_right)/2\n",
    "        liver_label        = self.model3d_liver(x_liver)\n",
    "        spleen_label       = self.model3d_spleen(x_spleen)\n",
    "        \n",
    "        bowel_soft = self.softmax(bowel_label)\n",
    "        extrav_soft = self.softmax(extrav_label)\n",
    "        kidney_soft = self.softmax(kidney_label)\n",
    "        liver_soft = self.softmax(liver_label)\n",
    "        spleen_soft = self.softmax(spleen_label)\n",
    "        \n",
    "        labels = torch.cat([bowel_soft, extrav_soft, kidney_soft, liver_soft, spleen_soft], dim = 1)\n",
    "\n",
    "        return labels\n",
    "    \n",
    "    \n",
    "class AbdominalClassifier2(nn.Module):\n",
    "    def __init__(self, device = DEVICE):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.upsample = torch.nn.Upsample(size = [UP_RESOL, UP_RESOL, UP_RESOL])\n",
    "        \n",
    "        self.model3d_bowel        = Timm3DModelClassifier(backbone_classifier2, 1, 2)      \n",
    "        self.model3d_extrav       = Timm3DModelClassifier(backbone_classifier2, 1, 2)\n",
    "        self.model3d_kidney_left  = Timm3DModelClassifier(backbone_classifier2, 1, 3)\n",
    "        self.model3d_kidney_right = Timm3DModelClassifier(backbone_classifier2, 1, 3)\n",
    "        self.model3d_liver        = Timm3DModelClassifier(backbone_classifier2, 1, 3)\n",
    "        self.model3d_spleen       = Timm3DModelClassifier(backbone_classifier2, 1, 3)\n",
    "        \n",
    "        self.flatten  = nn.Flatten()\n",
    "        self.dropout  = nn.Dropout(p=0.5)\n",
    "        self.softmax  = nn.Softmax(dim=1)        \n",
    "        self.maxpool  = nn.MaxPool1d(5, 1)\n",
    "        \n",
    "    def forward(self, x_bowel, x_kidney_left, x_kidney_right, x_liver, x_spleen, x_total):\n",
    "        bowel_label        = self.model3d_bowel(x_bowel)\n",
    "        extrav_label       = self.model3d_extrav(x_total)\n",
    "        kidney_label_left  = self.model3d_kidney_left(x_kidney_left)\n",
    "        kidney_label_right = self.model3d_kidney_right(x_kidney_right)\n",
    "        kidney_label       = (kidney_label_left + kidney_label_right)/2\n",
    "        liver_label        = self.model3d_liver(x_liver)\n",
    "        spleen_label       = self.model3d_spleen(x_spleen)\n",
    "        \n",
    "        bowel_soft = self.softmax(bowel_label)\n",
    "        extrav_soft = self.softmax(extrav_label)\n",
    "        kidney_soft = self.softmax(kidney_label)\n",
    "        liver_soft = self.softmax(liver_label)\n",
    "        spleen_soft = self.softmax(spleen_label)\n",
    "        \n",
    "        labels = torch.cat([bowel_soft, extrav_soft, kidney_soft, liver_soft, spleen_soft], dim = 1)\n",
    "\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39b4bf63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T13:33:00.332717Z",
     "iopub.status.busy": "2023-10-14T13:33:00.332384Z",
     "iopub.status.idle": "2023-10-14T13:33:00.346817Z",
     "shell.execute_reply": "2023-10-14T13:33:00.345628Z"
    },
    "papermill": {
     "duration": 0.027182,
     "end_time": "2023-10-14T13:33:00.348819",
     "exception": false,
     "start_time": "2023-10-14T13:33:00.321637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Timm3DModelClassifierEmbed(nn.Module):\n",
    "    def __init__(self, backbone, n_channels, n_labels, segtype='unet', pretrained=False):\n",
    "        super(Timm3DModelClassifierEmbed, self).__init__()\n",
    "        self.model_3d = Timm3DModel(backbone, n_channels, n_labels, segtype, pretrained)\n",
    "        self.model_3d = convert_3d(self.model_3d)\n",
    "        self.n_channels = n_channels\n",
    "        self.n_labels = n_labels                        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.model_3d(x)\n",
    "        pooled_features = []\n",
    "        for i in range(0, len(x)):\n",
    "            pooled_features.append(torch.reshape(torch.mean(x[i], dim = (2, 3, 4)), (batch_size, self.n_labels, 1)))\n",
    "        pooled_features = torch.cat(pooled_features, dim=2)     \n",
    "        labels = nn.Flatten()(pooled_features)\n",
    "        #labels = torch.mean(pooled_features, dim = 2)\n",
    "        return labels\n",
    "\n",
    "# LSTM\n",
    "class AbdominalClassifierLSTM(nn.Module):\n",
    "    def __init__(self, device = DEVICE):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.model3d_bowel        = Timm3DModelClassifierEmbed(backbone_classifiers3, 1, 32)      \n",
    "        self.model3d_extrav       = Timm3DModelClassifierEmbed(backbone_classifiers3, 1, 32)\n",
    "        self.model3d_kidney_left  = Timm3DModelClassifierEmbed(backbone_classifiers3, 1, 32)\n",
    "        self.model3d_kidney_right = Timm3DModelClassifierEmbed(backbone_classifiers3, 1, 32)\n",
    "        self.model3d_liver        = Timm3DModelClassifierEmbed(backbone_classifiers3, 1, 32)\n",
    "        self.model3d_spleen       = Timm3DModelClassifierEmbed(backbone_classifiers3, 1, 32)\n",
    "        \n",
    "        self.flatten  = nn.Flatten()\n",
    "        self.dropout  = nn.Dropout(p=0.5)\n",
    "        self.softmax  = nn.Softmax(dim=1)        \n",
    "        self.maxpool  = nn.MaxPool1d(5, 1)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size =128, hidden_size = 256, num_layers=5, batch_first=True, bidirectional=True)\n",
    "        self.head = nn.Linear(512, 13)\n",
    "        \n",
    "    def forward(self, x_bowel, x_kidney_left, x_kidney_right, x_liver, x_spleen, x_total):\n",
    "        bs = x_bowel.shape[0]\n",
    "        \n",
    "        bowel_emb        = torch.reshape(self.model3d_bowel(x_bowel), (bs, 1, 128))\n",
    "        extrav_emb       = torch.reshape(self.model3d_extrav(x_total), (bs, 1, 128))\n",
    "        kidney_left_emb  = torch.reshape(self.model3d_kidney_left(x_kidney_left), (bs, 1, 128))\n",
    "        kidney_right_emb = torch.reshape(self.model3d_kidney_right(x_kidney_right), (bs, 1, 128))\n",
    "        liver_emb        = torch.reshape(self.model3d_liver(x_liver), (bs, 1, 128))\n",
    "        spleen_emb       = torch.reshape(self.model3d_spleen(x_spleen), (bs, 1, 128))\n",
    "        \n",
    "        all_embs = torch.cat([bowel_emb, extrav_emb, kidney_left_emb, kidney_right_emb, liver_emb, spleen_emb], dim = 1)\n",
    "        \n",
    "        all_embs = self.lstm(all_embs)\n",
    "        labels   = torch.mean(all_embs[0], dim = 1) \n",
    "        labels   = self.head(labels)\n",
    "\n",
    "        bowel_soft = self.softmax(labels[:,:2])\n",
    "        extrav_soft = self.softmax(labels[:,2:4])\n",
    "        kidney_soft = self.softmax(labels[:,4:7])\n",
    "        liver_soft = self.softmax(labels[:,7:10])\n",
    "        spleen_soft = self.softmax(labels[:,10:13])\n",
    "\n",
    "        labels = torch.cat([bowel_soft, extrav_soft, kidney_soft, liver_soft, spleen_soft], dim = 1)\n",
    "\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a155491",
   "metadata": {
    "papermill": {
     "duration": 0.008792,
     "end_time": "2023-10-14T13:33:00.367464",
     "exception": false,
     "start_time": "2023-10-14T13:33:00.358672",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference with cropped regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3030f96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T13:33:00.387090Z",
     "iopub.status.busy": "2023-10-14T13:33:00.386738Z",
     "iopub.status.idle": "2023-10-14T13:33:00.394442Z",
     "shell.execute_reply": "2023-10-14T13:33:00.393217Z"
    },
    "papermill": {
     "duration": 0.01992,
     "end_time": "2023-10-14T13:33:00.396569",
     "exception": false,
     "start_time": "2023-10-14T13:33:00.376649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AbdominalCTDataset(Dataset):\n",
    "    def __init__(self, meta_df, is_train = True, transform_set = None, remain_transforms_set = None):\n",
    "        self.meta_df = meta_df\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.meta_df.iloc[idx]        \n",
    "\n",
    "        data_3ds = {}\n",
    "        base_name = self.meta_df.iloc[idx]['cropped_path']            \n",
    "        for j in range(0, 6):\n",
    "            data_3d = decompress_fast(f'{base_name}_{j}').unsqueeze(0).to(torch.float32)\n",
    "            #data_3d = torch.from_numpy(data_3d)\n",
    "            data_3ds[chan_dict[j]] = data_3d  \n",
    "\n",
    "\n",
    "        return data_3ds['bowel'], data_3ds['left_kidney'], data_3ds['right_kidney'], \\\n",
    "                data_3ds['liver'], data_3ds['spleen'], data_3ds['total']\n",
    "\n",
    "test_dataset = AbdominalCTDataset(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "056508ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T13:33:00.416735Z",
     "iopub.status.busy": "2023-10-14T13:33:00.416386Z",
     "iopub.status.idle": "2023-10-14T13:33:59.529880Z",
     "shell.execute_reply": "2023-10-14T13:33:59.528811Z"
    },
    "papermill": {
     "duration": 59.12628,
     "end_time": "2023-10-14T13:33:59.531995",
     "exception": false,
     "start_time": "2023-10-14T13:33:00.405715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.01it/s]\n",
      "/tmp/ipykernel_22/2150712000.py:89: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_data = df_data.groupby(['patient_id']).agg('mean')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>bowel_healthy</th>\n",
       "      <th>bowel_injury</th>\n",
       "      <th>extravasation_healthy</th>\n",
       "      <th>extravasation_injury</th>\n",
       "      <th>kidney_healthy</th>\n",
       "      <th>kidney_low</th>\n",
       "      <th>kidney_high</th>\n",
       "      <th>liver_healthy</th>\n",
       "      <th>liver_low</th>\n",
       "      <th>liver_high</th>\n",
       "      <th>spleen_healthy</th>\n",
       "      <th>spleen_low</th>\n",
       "      <th>spleen_high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48843</td>\n",
       "      <td>0.892306</td>\n",
       "      <td>0.107694</td>\n",
       "      <td>0.511987</td>\n",
       "      <td>0.488013</td>\n",
       "      <td>0.560934</td>\n",
       "      <td>0.296946</td>\n",
       "      <td>0.142119</td>\n",
       "      <td>0.943546</td>\n",
       "      <td>0.035143</td>\n",
       "      <td>0.021312</td>\n",
       "      <td>0.911876</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.019965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50046</td>\n",
       "      <td>0.866436</td>\n",
       "      <td>0.133564</td>\n",
       "      <td>0.481671</td>\n",
       "      <td>0.518329</td>\n",
       "      <td>0.615959</td>\n",
       "      <td>0.248909</td>\n",
       "      <td>0.135132</td>\n",
       "      <td>0.755278</td>\n",
       "      <td>0.068269</td>\n",
       "      <td>0.176453</td>\n",
       "      <td>0.832170</td>\n",
       "      <td>0.140618</td>\n",
       "      <td>0.027212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63706</td>\n",
       "      <td>0.948301</td>\n",
       "      <td>0.051699</td>\n",
       "      <td>0.631876</td>\n",
       "      <td>0.368124</td>\n",
       "      <td>0.601788</td>\n",
       "      <td>0.274645</td>\n",
       "      <td>0.123567</td>\n",
       "      <td>0.886764</td>\n",
       "      <td>0.081744</td>\n",
       "      <td>0.031492</td>\n",
       "      <td>0.754351</td>\n",
       "      <td>0.212043</td>\n",
       "      <td>0.033606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id  bowel_healthy  bowel_injury  extravasation_healthy  \\\n",
       "0       48843       0.892306      0.107694               0.511987   \n",
       "1       50046       0.866436      0.133564               0.481671   \n",
       "2       63706       0.948301      0.051699               0.631876   \n",
       "\n",
       "   extravasation_injury  kidney_healthy  kidney_low  kidney_high  \\\n",
       "0              0.488013        0.560934    0.296946     0.142119   \n",
       "1              0.518329        0.615959    0.248909     0.135132   \n",
       "2              0.368124        0.601788    0.274645     0.123567   \n",
       "\n",
       "   liver_healthy  liver_low  liver_high  spleen_healthy  spleen_low  \\\n",
       "0       0.943546   0.035143    0.021312        0.911876    0.068158   \n",
       "1       0.755278   0.068269    0.176453        0.832170    0.140618   \n",
       "2       0.886764   0.081744    0.031492        0.754351    0.212043   \n",
       "\n",
       "   spleen_high  \n",
       "0     0.019965  \n",
       "1     0.027212  \n",
       "2     0.033606  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = AbdominalClassifier()\n",
    "# model.load_state_dict(torch.load(weights_classifier))\n",
    "# model.to(DEVICE)    \n",
    "# model.eval()\n",
    "device = torch.device(\"cuda:0\")  # for the first GPU\n",
    "model = AbdominalClassifier() # resnet50\n",
    "model.load_state_dict(torch.load(weights_classifier, map_location=device))\n",
    "model.to(DEVICE)    \n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda:0\")  # for the first GPU\n",
    "model1_2 = AbdominalClassifier() # resnet50\n",
    "model1_2.load_state_dict(torch.load(weights_classifier1_2, map_location=device))\n",
    "model1_2.to(DEVICE)    \n",
    "model1_2.eval()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\")  # for the first GPU\n",
    "model2 = AbdominalClassifier2() # resnet10\n",
    "model2.load_state_dict(torch.load(weights_classifier2, map_location=device))\n",
    "model2.to(DEVICE)    \n",
    "model2.eval()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\")  # for the first GPU\n",
    "model3 = AbdominalClassifierLSTM() # resnet10\n",
    "model3.load_state_dict(torch.load(weights_classifiers3, map_location=device))\n",
    "model3.to(DEVICE)    \n",
    "model3.eval()\n",
    "\n",
    "dummy=0\n",
    "\n",
    "predss=[]\n",
    "predss2 = []\n",
    "predss1_2 = []\n",
    "predss3 = []\n",
    "#bar = tqdm(test_loader)\n",
    "\n",
    "with torch.cuda.amp.autocast(enabled=False):  \n",
    "    with torch.inference_mode():\n",
    "        for i in tqdm(range(0, len(test_dataset))):\n",
    "            X_bowel, X_lkid, X_rkid, X_liv, X_spl, X_tot = test_dataset[i]\n",
    "            X_bowel, X_lkid, X_rkid = X_bowel.unsqueeze(0).to(DEVICE), X_lkid.unsqueeze(0).to(DEVICE), X_rkid.unsqueeze(0).to(DEVICE)\n",
    "            X_liv,   X_spl,  X_tot  = X_liv.unsqueeze(0).to(DEVICE),   X_spl.unsqueeze(0).to(DEVICE),  X_tot.unsqueeze(0).to(DEVICE)            \n",
    "            preds = model(X_bowel, X_lkid, X_rkid, X_liv, X_spl, X_tot).detach().cpu().numpy()  # model (resnet50)\n",
    "            preds2 = model2(X_bowel, X_lkid, X_rkid, X_liv, X_spl, X_tot).detach().cpu().numpy()  # model2 (resnet10)                        \n",
    "            preds1_2 = model1_2(X_bowel, X_lkid, X_rkid, X_liv, X_spl, X_tot).detach().cpu().numpy()  # model2 (resnet10)\n",
    "            preds3 = model3(X_bowel, X_lkid, X_rkid, X_liv, X_spl, X_tot).detach().cpu().numpy()  # LSTM\n",
    "\n",
    "            predss.append(preds)\n",
    "            predss2.append(preds2)\n",
    "            predss1_2.append(preds1_2)\n",
    "            predss3.append(preds3)\n",
    "            del X_bowel, X_lkid, X_rkid, X_liv, X_spl, X_tot\n",
    "            gc.collect()\n",
    "            \n",
    "        \n",
    "\n",
    "predss = np.vstack(predss)\n",
    "predss2 = np.vstack(predss2)\n",
    "predss1_2 = np.vstack(predss1_2)\n",
    "predss3 = np.vstack(predss3)\n",
    "                \n",
    "#del images\n",
    "#gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "target_cols =  ['bowel_healthy','bowel_injury',\n",
    "                    'extravasation_healthy','extravasation_injury',\n",
    "                    'kidney_healthy','kidney_low','kidney_high',\n",
    "                    'liver_healthy','liver_low','liver_high',\n",
    "                    'spleen_healthy','spleen_low','spleen_high']\n",
    "\n",
    "# avg preds\n",
    "avg_preds = (predss + predss2 + predss1_2+predss3) / 4\n",
    "\n",
    "df_data[target_cols] = avg_preds\n",
    "#preds = np.vstack(preds)            \n",
    "\n",
    "try:\n",
    "    df_data = df_data.drop('series', axis = 1)\n",
    "except:\n",
    "    df_data = df_data.drop('sample', axis = 1)\n",
    "\n",
    "# More sofisticated postprocessing    \n",
    "bowel_extrav =  \\\n",
    "            df_data.groupby(['patient_id']).agg('max')[['bowel_injury', 'extravasation_injury']].to_numpy()\n",
    "\n",
    "df_data = df_data.groupby(['patient_id']).agg('mean')\n",
    "\n",
    "df_data['bowel_healthy'] = 1 - bowel_extrav[:,0]\n",
    "df_data['bowel_injury']  = bowel_extrav[:,0]\n",
    "df_data['extravasation_healthy'] = 1- bowel_extrav[:,1]\n",
    "df_data['extravasation_injury'] =  bowel_extrav[:,1]\n",
    "\n",
    "patient_id = df_data.index\n",
    "df_data = df_data.reset_index()\n",
    "\n",
    "df_data['patient_id'] = patient_id\n",
    "\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca279503",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T13:33:59.552550Z",
     "iopub.status.busy": "2023-10-14T13:33:59.552177Z",
     "iopub.status.idle": "2023-10-14T13:33:59.592215Z",
     "shell.execute_reply": "2023-10-14T13:33:59.591222Z"
    },
    "papermill": {
     "duration": 0.05224,
     "end_time": "2023-10-14T13:33:59.594200",
     "exception": false,
     "start_time": "2023-10-14T13:33:59.541960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>bowel_healthy</th>\n",
       "      <th>bowel_injury</th>\n",
       "      <th>extravasation_healthy</th>\n",
       "      <th>extravasation_injury</th>\n",
       "      <th>kidney_healthy</th>\n",
       "      <th>kidney_low</th>\n",
       "      <th>kidney_high</th>\n",
       "      <th>liver_healthy</th>\n",
       "      <th>liver_low</th>\n",
       "      <th>liver_high</th>\n",
       "      <th>spleen_healthy</th>\n",
       "      <th>spleen_low</th>\n",
       "      <th>spleen_high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48843</td>\n",
       "      <td>0.892306</td>\n",
       "      <td>0.107694</td>\n",
       "      <td>0.511987</td>\n",
       "      <td>0.488013</td>\n",
       "      <td>0.560934</td>\n",
       "      <td>0.296946</td>\n",
       "      <td>0.142119</td>\n",
       "      <td>0.943546</td>\n",
       "      <td>0.035143</td>\n",
       "      <td>0.021312</td>\n",
       "      <td>0.911876</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>0.019965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50046</td>\n",
       "      <td>0.866436</td>\n",
       "      <td>0.133564</td>\n",
       "      <td>0.481671</td>\n",
       "      <td>0.518329</td>\n",
       "      <td>0.615959</td>\n",
       "      <td>0.248909</td>\n",
       "      <td>0.135132</td>\n",
       "      <td>0.755278</td>\n",
       "      <td>0.068269</td>\n",
       "      <td>0.176453</td>\n",
       "      <td>0.832170</td>\n",
       "      <td>0.140618</td>\n",
       "      <td>0.027212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63706</td>\n",
       "      <td>0.948301</td>\n",
       "      <td>0.051699</td>\n",
       "      <td>0.631876</td>\n",
       "      <td>0.368124</td>\n",
       "      <td>0.601788</td>\n",
       "      <td>0.274645</td>\n",
       "      <td>0.123567</td>\n",
       "      <td>0.886764</td>\n",
       "      <td>0.081744</td>\n",
       "      <td>0.031492</td>\n",
       "      <td>0.754351</td>\n",
       "      <td>0.212043</td>\n",
       "      <td>0.033606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id  bowel_healthy  bowel_injury  extravasation_healthy  \\\n",
       "0       48843       0.892306      0.107694               0.511987   \n",
       "1       50046       0.866436      0.133564               0.481671   \n",
       "2       63706       0.948301      0.051699               0.631876   \n",
       "\n",
       "   extravasation_injury  kidney_healthy  kidney_low  kidney_high  \\\n",
       "0              0.488013        0.560934    0.296946     0.142119   \n",
       "1              0.518329        0.615959    0.248909     0.135132   \n",
       "2              0.368124        0.601788    0.274645     0.123567   \n",
       "\n",
       "   liver_healthy  liver_low  liver_high  spleen_healthy  spleen_low  \\\n",
       "0       0.943546   0.035143    0.021312        0.911876    0.068158   \n",
       "1       0.755278   0.068269    0.176453        0.832170    0.140618   \n",
       "2       0.886764   0.081744    0.031492        0.754351    0.212043   \n",
       "\n",
       "   spleen_high  \n",
       "0     0.019965  \n",
       "1     0.027212  \n",
       "2     0.033606  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = pd.read_csv('/kaggle/input/rsna-2023-abdominal-trauma-detection/sample_submission.csv')\n",
    "if not DEBUG:\n",
    "    for i in range(0, len(df_data)):\n",
    "        row = df_data.iloc[i]\n",
    "        patient_id = row['patient_id']\n",
    "        sample_df.loc[sample_df['patient_id']==patient_id,target_cols] = row[target_cols].to_numpy()\n",
    "#Extravasation \n",
    "sample_df.to_csv('submission.csv', index = False)\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b7a1a9",
   "metadata": {
    "papermill": {
     "duration": 0.009679,
     "end_time": "2023-10-14T13:33:59.613563",
     "exception": false,
     "start_time": "2023-10-14T13:33:59.603884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb23b87f",
   "metadata": {
    "papermill": {
     "duration": 0.009204,
     "end_time": "2023-10-14T13:33:59.632101",
     "exception": false,
     "start_time": "2023-10-14T13:33:59.622897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f564e106",
   "metadata": {
    "papermill": {
     "duration": 0.00927,
     "end_time": "2023-10-14T13:33:59.650859",
     "exception": false,
     "start_time": "2023-10-14T13:33:59.641589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 274.452429,
   "end_time": "2023-10-14T13:34:02.986095",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-14T13:29:28.533666",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
