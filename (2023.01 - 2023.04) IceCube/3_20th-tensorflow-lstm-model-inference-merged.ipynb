{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db7b2337",
   "metadata": {
    "papermill": {
     "duration": 0.011257,
     "end_time": "2023-04-20T01:41:24.964644",
     "exception": false,
     "start_time": "2023-04-20T01:41:24.953387",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 20th - Team Ice team Final Solution - Public LB 0.991, Private LB 0.992!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b961a13",
   "metadata": {
    "papermill": {
     "duration": 0.006335,
     "end_time": "2023-04-20T01:41:24.981562",
     "exception": false,
     "start_time": "2023-04-20T01:41:24.975227",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook is the 20th place final submission for Team Ice team for Kaggle's IceCube - Neutrinos in Deep Ice competition. <b>Team members are junseonglee11 (@junseonglee11), Ayaan Jang(@ayaanjang).</b> This is an ensemble of the LSTM model of 6 (2 different versions).\n",
    "\n",
    "\n",
    "* TFRecord Dataset Notebook: https://www.kaggle.com/code/junseonglee11/icecube-data-to-tfrecord-v2-1\n",
    "* Train Notebook: https://www.kaggle.com/code/junseonglee11/20th-tensorflow-tfrecord-tpu-lstm-line-fit-train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1e899e",
   "metadata": {
    "papermill": {
     "duration": 0.005875,
     "end_time": "2023-04-20T01:41:24.993555",
     "exception": false,
     "start_time": "2023-04-20T01:41:24.987680",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# References\n",
    "### Robin smith's: notebooks:\n",
    "* https://www.kaggle.com/code/rsmits/tensorflow-lstm-model-inference\n",
    "* https://www.kaggle.com/code/rsmits/tensorflow-lstm-model-training-tpu\n",
    "* https://www.kaggle.com/code/rsmits/tensorflow-lstm-model-data-preprocessor/notebook \n",
    "I modified his notebook\n",
    "* a. Converted the dataset to TFRecords\n",
    "* b. Additional inputs and some preprocessing\n",
    "* c. Changed the model (more RNN layers, GRU --> LSTM, GELU activation, rectified adam optimizer)\n",
    "\n",
    "### Robert Hatch's notebook\n",
    "* https://www.kaggle.com/code/roberthatch/lb-1-183-lightning-fast-baseline-with-polars\n",
    "* It was crucial to improve our score. Used the results of this notebook as additional inputs in our model.\n",
    "\n",
    "* |TFRecord dataset generation: https://www.kaggle.com/code/junseonglee11/icecube-data-to-tfrecord-v2-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc34b27e",
   "metadata": {
    "papermill": {
     "duration": 0.005822,
     "end_time": "2023-04-20T01:41:25.005488",
     "exception": false,
     "start_time": "2023-04-20T01:41:24.999666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this inference notebook I show that with the correct data pre-processing, model architecture and training setup an LSTM model can achieve the same performance as the current top performing Graph based models.\n",
    "\n",
    "This notebook is based on the notebook originally published with LSTM training and inference: [3 LSTMs; with Data Picking and Shifting](https://www.kaggle.com/code/seungmoklee/3-lstms-with-data-picking-and-shifting). So if you like my notebooks don't forget the work that it is based!\n",
    "\n",
    "This Inference notebook is for the largest part the same. It doesn't use the shifting as applied in the original work, it is based on only using 96 pulses and only 6 features.\n",
    "It does ensemble 3 models but they are from the same training run (just different epochs).\n",
    "\n",
    "Combining models from different model trainings with slightly different hyperparameters will very likely further increase the score. Increasing the number of bins and using more data for training is another way to increase the score of the models. Consider the Inference and Training notebooks a starting point to explore that yourself!\n",
    "\n",
    "The training notebook for these models can be found [here](https://www.kaggle.com/code/rsmits/tensorflow-lstm-model-training-tpu). Note that I performed training on my local laptop (32GB RAM / NVidia 3070). For local training I loaded the files for 70 batches into RAM. The training notebook is equal to my local setup with the change that it loads multiple rounds of training data.\n",
    "\n",
    "In the training notebook I will further explain what the differences are and how I improved the achieved score.\n",
    "\n",
    "I hope you enjoy this notebook and if you do please give it an upvote :-)\n",
    "\n",
    "!! Update in Latest Version: In the comments it was mentioned that using model files from a TPU training caused an error when trying to load the model. I've updated the load_model() with compile = False. This solves the error. Nothing else has changed in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb36e3b8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-04-20T01:41:25.020309Z",
     "iopub.status.busy": "2023-04-20T01:41:25.019198Z",
     "iopub.status.idle": "2023-04-20T01:41:29.890421Z",
     "shell.execute_reply": "2023-04-20T01:41:29.889436Z"
    },
    "papermill": {
     "duration": 4.881419,
     "end_time": "2023-04-20T01:41:29.892949",
     "exception": false,
     "start_time": "2023-04-20T01:41:25.011530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "import gc\n",
    "import os\n",
    "import multiprocessing\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c2ac1a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T01:41:29.908615Z",
     "iopub.status.busy": "2023-04-20T01:41:29.908040Z",
     "iopub.status.idle": "2023-04-20T01:41:29.915886Z",
     "shell.execute_reply": "2023-04-20T01:41:29.914962Z"
    },
    "papermill": {
     "duration": 0.017335,
     "end_time": "2023-04-20T01:41:29.917925",
     "exception": false,
     "start_time": "2023-04-20T01:41:29.900590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Directories and constants\n",
    "home_dir = \"/kaggle/input/icecube-neutrinos-in-deep-ice/\"\n",
    "test_format = home_dir + 'test/batch_{batch_id:d}.parquet'\n",
    "   \n",
    "# Model(s)\n",
    "model_names = [\n",
    "     # LB 0.993 (2) LSTM 7 layer\n",
    "    \"/kaggle/input/0419-lstm-1/epoch07-val_acc0.15238.h5\",    \n",
    "    \"/kaggle/input/0419-lstm-1/epoch01-val_acc0.15396.h5\",\n",
    "    \"/kaggle/input/0419-lstm-1/epoch02-val_acc0.15363.h5\",\n",
    "    \"/kaggle/input/0419-lstm-1/epoch03-val_acc0.15471.h5\",                        \n",
    "    \n",
    "    # LB 0.992 (2) LSTM 5 layer\n",
    "    \"/kaggle/input/0419-lstm-1/epoch03-val_acc0.15094.h5\",\n",
    "    \"/kaggle/input/0419-lstm-1/epoch07-val_acc0.15104.h5\",\n",
    "    \"/kaggle/input/0419-lstm-1/epoch08-val_acc0.15099.h5\",     \n",
    "    \"/kaggle/input/0419-lstm-1/epoch05-val_acc0.15092.h5\",                     \n",
    "    \n",
    "]\n",
    "\n",
    "# Weight earned through CV\n",
    "model_weights = np.array(\n",
    "    [0.05836763, 0.72707093, 0.71994111, 0.78264521, 0.14015217, 0.71328186, 0.47582921, 0.99825985]                                                                                                                                                                                                           \n",
    ")\n",
    "\n",
    "#0 9 features time, pos diff added\n",
    "#1 6 features pos --> pos - mean pos\n",
    "\n",
    "input_norm_method = [\n",
    "    2,    \n",
    "    2,\n",
    "    2,    \n",
    "    2,\n",
    "    \n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a13f9a9",
   "metadata": {
    "papermill": {
     "duration": 0.005915,
     "end_time": "2023-04-20T01:41:29.931272",
     "exception": false,
     "start_time": "2023-04-20T01:41:29.925357",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2bdd7a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T01:41:29.945977Z",
     "iopub.status.busy": "2023-04-20T01:41:29.945705Z",
     "iopub.status.idle": "2023-04-20T01:43:04.259735Z",
     "shell.execute_reply": "2023-04-20T01:43:04.257536Z"
    },
    "papermill": {
     "duration": 94.323333,
     "end_time": "2023-04-20T01:43:04.261942",
     "exception": false,
     "start_time": "2023-04-20T01:41:29.938609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Model File: /kaggle/input/0419-lstm-1/epoch07-val_acc0.15238.h5\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 96, 9)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, 96, 9)        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 96, 384)      310272      masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 96, 384)      886272      bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 96, 384)      886272      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 96, 384)      886272      bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 96, 384)      886272      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 96, 384)      886272      bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 384)          886272      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 384)          1536        bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          98560       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 258)          0           batch_normalization_1[0][0]      \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 258)          66822       tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 258)          66822       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 258)          66822       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1024)         265216      dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,194,706\n",
      "Trainable params: 6,193,426\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "========== Model File: /kaggle/input/0419-lstm-1/epoch01-val_acc0.15396.h5\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 96, 9)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, 96, 9)        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 96, 384)      310272      masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 96, 384)      886272      bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 96, 384)      886272      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 96, 384)      886272      bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 96, 384)      886272      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 96, 384)      886272      bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 384)          886272      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 384)          1536        bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          98560       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 258)          0           batch_normalization_1[0][0]      \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 258)          66822       tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 258)          66822       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 258)          66822       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1024)         265216      dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,194,706\n",
      "Trainable params: 6,193,426\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "========== Model File: /kaggle/input/0419-lstm-1/epoch02-val_acc0.15363.h5\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 96, 9)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, 96, 9)        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 96, 384)      310272      masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 96, 384)      886272      bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 96, 384)      886272      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 96, 384)      886272      bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 96, 384)      886272      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 96, 384)      886272      bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 384)          886272      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 384)          1536        bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          98560       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 258)          0           batch_normalization_1[0][0]      \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 258)          66822       tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 258)          66822       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 258)          66822       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1024)         265216      dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,194,706\n",
      "Trainable params: 6,193,426\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "========== Model File: /kaggle/input/0419-lstm-1/epoch03-val_acc0.15471.h5\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 96, 9)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, 96, 9)        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 96, 384)      310272      masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 96, 384)      886272      bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 96, 384)      886272      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 96, 384)      886272      bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 96, 384)      886272      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 96, 384)      886272      bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 384)          886272      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 384)          1536        bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          98560       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 258)          0           batch_normalization_1[0][0]      \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 258)          66822       tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 258)          66822       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 258)          66822       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1024)         265216      dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,194,706\n",
      "Trainable params: 6,193,426\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "========== Model File: /kaggle/input/0419-lstm-1/epoch03-val_acc0.15094.h5\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 96, 9)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, 96, 9)        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 96, 384)      310272      masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 96, 384)      886272      bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 96, 384)      886272      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 96, 384)      886272      bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 96, 384)      886272      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 96, 384)      886272      bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 384)          886272      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 384)          1536        bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          98560       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 258)          0           batch_normalization_1[0][0]      \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 258)          66822       tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 258)          66822       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 258)          66822       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1024)         265216      dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,194,706\n",
      "Trainable params: 6,193,426\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "========== Model File: /kaggle/input/0419-lstm-1/epoch07-val_acc0.15104.h5\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 96, 9)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, 96, 9)        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 96, 384)      310272      masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 96, 384)      886272      bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 96, 384)      886272      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 96, 384)      886272      bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 96, 384)      886272      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 96, 384)      886272      bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 384)          886272      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 384)          1536        bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          98560       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 258)          0           batch_normalization_1[0][0]      \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 258)          66822       tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 258)          66822       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 258)          66822       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1024)         265216      dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,194,706\n",
      "Trainable params: 6,193,426\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "========== Model File: /kaggle/input/0419-lstm-1/epoch08-val_acc0.15099.h5\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 96, 9)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, 96, 9)        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 96, 384)      310272      masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 96, 384)      886272      bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 96, 384)      886272      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 96, 384)      886272      bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 96, 384)      886272      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 96, 384)      886272      bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 384)          886272      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 384)          1536        bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          98560       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 258)          0           batch_normalization_1[0][0]      \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 258)          66822       tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 258)          66822       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 258)          66822       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1024)         265216      dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,194,706\n",
      "Trainable params: 6,193,426\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "========== Model File: /kaggle/input/0419-lstm-1/epoch05-val_acc0.15092.h5\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 96, 9)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, 96, 9)        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 96, 384)      310272      masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 96, 384)      886272      bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 96, 384)      886272      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 96, 384)      886272      bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 96, 384)      886272      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 96, 384)      886272      bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 384)          886272      bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 384)          1536        bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          98560       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 258)          0           batch_normalization_1[0][0]      \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 258)          66822       tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 258)          66822       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 258)          66822       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1024)         265216      dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,194,706\n",
      "Trainable params: 6,193,426\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "==== Model Parameters\n",
      "Bin Numbers: 32\n",
      "Maximum Pulse Count: 96\n",
      "Features Count: 9\n"
     ]
    }
   ],
   "source": [
    "# Load Models\n",
    "models = []\n",
    "feature_counts = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f'\\n========== Model File: {model_name}')\n",
    "    # Load Model\n",
    "    model_path = model_name\n",
    "    model = tf.keras.models.load_model(model_path, compile = False)\n",
    "    models.append(model)      \n",
    "    feature_count = model.inputs[0].shape[2]\n",
    "    feature_counts.append(feature_count)\n",
    "    # Model summary\n",
    "    model.summary()\n",
    "    \n",
    "# Get Model Parameters\n",
    "pulse_count = model.inputs[0].shape[1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output_bins = model.layers[-1].weights[0].shape[-1]\n",
    "bin_num = int(np.sqrt(output_bins))\n",
    "\n",
    "# Model Parameter Summary\n",
    "print(\"\\n==== Model Parameters\")\n",
    "print(f\"Bin Numbers: {bin_num}\")\n",
    "print(f\"Maximum Pulse Count: {pulse_count}\")\n",
    "print(f\"Features Count: {feature_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f22aab",
   "metadata": {
    "papermill": {
     "duration": 0.006989,
     "end_time": "2023-04-20T01:43:04.276707",
     "exception": false,
     "start_time": "2023-04-20T01:43:04.269718",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Detector Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28048fc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T01:43:04.292245Z",
     "iopub.status.busy": "2023-04-20T01:43:04.291925Z",
     "iopub.status.idle": "2023-04-20T01:43:04.318350Z",
     "shell.execute_reply": "2023-04-20T01:43:04.316903Z"
    },
    "papermill": {
     "duration": 0.036586,
     "end_time": "2023-04-20T01:43:04.320353",
     "exception": false,
     "start_time": "2023-04-20T01:43:04.283767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time valid length: 6199.700247193777 ns\n"
     ]
    }
   ],
   "source": [
    "# Load sensor_geometry\n",
    "sensor_geometry_df = pd.read_csv(home_dir + \"sensor_geometry.csv\")\n",
    "\n",
    "# Get Sensor Information\n",
    "sensor_x = sensor_geometry_df.x\n",
    "sensor_y = sensor_geometry_df.y\n",
    "sensor_z = sensor_geometry_df.z\n",
    "\n",
    "# Detector constants\n",
    "c_const = 0.299792458  # speed of light [m/ns]\n",
    "\n",
    "# Sensor Min / Max Coordinates\n",
    "x_min = sensor_x.min()\n",
    "x_max = sensor_x.max()\n",
    "y_min = sensor_y.min()\n",
    "y_max = sensor_y.max()\n",
    "z_min = sensor_z.min()\n",
    "z_max = sensor_z.max()\n",
    "\n",
    "detector_length = np.sqrt((x_max - x_min)**2 + (y_max - y_min)**2 + (z_max - z_min)**2)\n",
    "t_valid_length = detector_length / c_const\n",
    "\n",
    "print(f\"time valid length: {t_valid_length} ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3419b8cd",
   "metadata": {
    "papermill": {
     "duration": 0.006981,
     "end_time": "2023-04-20T01:43:04.334665",
     "exception": false,
     "start_time": "2023-04-20T01:43:04.327684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Angle encoding edges\n",
    "\n",
    "- It is efficient to train the model by classification task, initially.\n",
    "- azimuth and zenith are independent\n",
    "- azimuth distribution is flat and zenith distribution is sinusoidal.\n",
    "  - Flat on the spherical surface\n",
    "  - $\\phi > \\pi$ events are a little bit rarer than $\\phi < \\pi$ events, (maybe) because of the neutrino attenuation by earth.\n",
    "- So, the uniform bin is used for azimuth, and $\\left| \\cos \\right|$ bin is used for zenith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1204db7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T01:43:04.350630Z",
     "iopub.status.busy": "2023-04-20T01:43:04.350295Z",
     "iopub.status.idle": "2023-04-20T01:43:04.359593Z",
     "shell.execute_reply": "2023-04-20T01:43:04.358645Z"
    },
    "papermill": {
     "duration": 0.020227,
     "end_time": "2023-04-20T01:43:04.362028",
     "exception": false,
     "start_time": "2023-04-20T01:43:04.341801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.19634954 0.39269908 0.58904862 0.78539816 0.9817477\n",
      " 1.17809725 1.37444679 1.57079633 1.76714587 1.96349541 2.15984495\n",
      " 2.35619449 2.55254403 2.74889357 2.94524311 3.14159265 3.33794219\n",
      " 3.53429174 3.73064128 3.92699082 4.12334036 4.3196899  4.51603944\n",
      " 4.71238898 4.90873852 5.10508806 5.3014376  5.49778714 5.69413668\n",
      " 5.89048623 6.08683577 6.28318531]\n",
      "[0.         0.3554212  0.50536051 0.62236849 0.72273425 0.81275556\n",
      " 0.89566479 0.97338991 1.04719755 1.11797973 1.18639955 1.25297262\n",
      " 1.31811607 1.38217994 1.4454685  1.50825556 1.57079633 1.63333709\n",
      " 1.69612416 1.75941271 1.82347658 1.88862003 1.9551931  2.02361292\n",
      " 2.0943951  2.16820274 2.24592786 2.32883709 2.41885841 2.51922417\n",
      " 2.63623214 2.78617145 3.14159265]\n"
     ]
    }
   ],
   "source": [
    "# Create Azimuth Edges\n",
    "azimuth_edges = np.linspace(0, 2 * np.pi, bin_num + 1)\n",
    "print(azimuth_edges)\n",
    "\n",
    "# Create Zenith Edges\n",
    "zenith_edges = []\n",
    "zenith_edges.append(0)\n",
    "for bin_idx in range(1, bin_num):\n",
    "    zenith_edges.append(np.arccos(np.cos(zenith_edges[-1]) - 2 / (bin_num)))\n",
    "zenith_edges.append(np.pi)\n",
    "zenith_edges = np.array(zenith_edges)\n",
    "print(zenith_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595a1fe3",
   "metadata": {
    "papermill": {
     "duration": 0.006987,
     "end_time": "2023-04-20T01:43:04.376322",
     "exception": false,
     "start_time": "2023-04-20T01:43:04.369335",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define a function converts from prediction to angles\n",
    "\n",
    "- Calculation of the mean-vector in a bin $\\theta \\in ( \\theta_0, \\theta_1 )$ and $\\phi \\in ( \\phi_0, \\phi_1 )$\n",
    "  - $\\vec{r} \\left( \\theta, ~ \\phi \\right) = \\left< \\sin \\theta \\cos \\phi, ~ \\sin \\theta \\sin \\phi, ~ \\cos \\theta \\right>$\n",
    "  - $\\bar{\\vec{r}} = \\frac{ \\int_{\\theta_{0}}^{\\theta_{1}} \\int_{\\phi_0}^{\\phi_1} \\vec{r} \\left( \\theta, ~ \\phi \\right) \\sin \\theta \\,d\\phi \\,d\\theta }{ \\int_{\\theta_{0}}^{\\theta_{1}} \\int_{\\phi_0}^{\\phi_1} 1 \\sin \\theta \\,d\\phi \\,d\\theta }$\n",
    "  - $ \\int_{\\theta_{0}}^{\\theta_{1}} \\int_{\\phi_0}^{\\phi_1} 1 \\sin \\theta \\,d\\phi \\,d\\theta = \\left( \\phi_1 - \\phi_0 \\right) \\left( \\cos \\theta_0 - \\cos \\theta_1 \\right)$\n",
    "  - $\n",
    "\\int_{\\theta_{0}}^{\\theta_{1}} \\int_{\\phi_0}^{\\phi_1} {r}_{x} \\left( \\theta, ~ \\phi \\right) \\sin \\theta \\,d\\phi \\,d\\theta = \n",
    "\\int_{\\theta_{0}}^{\\theta_{1}} \\int_{\\phi_0}^{\\phi_1} \\sin^2 \\theta \\cos \\phi \\,d\\phi \\,d\\theta = \n",
    "\\left( \\sin \\phi_1 - \\sin \\phi_0 \\right) \\left( \\frac{\\theta_1 - \\theta_0}{2} - \\frac{\\sin 2 \\theta_1 - \\sin 2 \\theta_0}{4} \\right)\n",
    "$\n",
    "  - $\n",
    "\\int_{\\theta_{0}}^{\\theta_{1}} \\int_{\\phi_0}^{\\phi_1} {r}_{y} \\left( \\theta, ~ \\phi \\right) \\sin \\theta \\,d\\phi \\,d\\theta = \n",
    "\\int_{\\theta_{0}}^{\\theta_{1}} \\int_{\\phi_0}^{\\phi_1} \\sin^2 \\theta \\sin \\phi \\,d\\phi \\,d\\theta = \n",
    "\\left( \\cos \\phi_0 - \\cos \\phi_1 \\right) \\left( \\frac{\\theta_1 - \\theta_0}{2} - \\frac{\\sin 2 \\theta_1 - \\sin 2 \\theta_0}{4} \\right)\n",
    "$\n",
    "  - $\n",
    "\\int_{\\theta_{0}}^{\\theta_{1}} \\int_{\\phi_0}^{\\phi_1} {r}_{z} \\left( \\theta, ~ \\phi \\right) \\sin \\theta \\,d\\phi \\,d\\theta = \n",
    "\\int_{\\theta_{0}}^{\\theta_{1}} \\int_{\\phi_0}^{\\phi_1} \\sin \\theta \\cos \\theta \\,d\\phi \\,d\\theta = \n",
    "\\left( \\phi_1 - \\phi_0 \\right) \\left( \\frac{\\cos 2 \\theta_0 - \\cos 2 \\theta_1}{4} \\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae9b78e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T01:43:04.392318Z",
     "iopub.status.busy": "2023-04-20T01:43:04.391999Z",
     "iopub.status.idle": "2023-04-20T01:43:04.403006Z",
     "shell.execute_reply": "2023-04-20T01:43:04.402150Z"
    },
    "papermill": {
     "duration": 0.021494,
     "end_time": "2023-04-20T01:43:04.405111",
     "exception": false,
     "start_time": "2023-04-20T01:43:04.383617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "angle_bin_zenith0 = np.tile(zenith_edges[:-1], bin_num)\n",
    "angle_bin_zenith1 = np.tile(zenith_edges[1:], bin_num)\n",
    "angle_bin_azimuth0 = np.repeat(azimuth_edges[:-1], bin_num)\n",
    "angle_bin_azimuth1 = np.repeat(azimuth_edges[1:], bin_num)\n",
    "\n",
    "angle_bin_area = (angle_bin_azimuth1 - angle_bin_azimuth0) * (np.cos(angle_bin_zenith0) - np.cos(angle_bin_zenith1))\n",
    "angle_bin_vector_sum_x = (np.sin(angle_bin_azimuth1) - np.sin(angle_bin_azimuth0)) * ((angle_bin_zenith1 - angle_bin_zenith0) / 2 - (np.sin(2 * angle_bin_zenith1) - np.sin(2 * angle_bin_zenith0)) / 4)\n",
    "angle_bin_vector_sum_y = (np.cos(angle_bin_azimuth0) - np.cos(angle_bin_azimuth1)) * ((angle_bin_zenith1 - angle_bin_zenith0) / 2 - (np.sin(2 * angle_bin_zenith1) - np.sin(2 * angle_bin_zenith0)) / 4)\n",
    "angle_bin_vector_sum_z = (angle_bin_azimuth1 - angle_bin_azimuth0) * ((np.cos(2 * angle_bin_zenith0) - np.cos(2 * angle_bin_zenith1)) / 4)\n",
    "\n",
    "angle_bin_vector_mean_x = angle_bin_vector_sum_x / angle_bin_area\n",
    "angle_bin_vector_mean_y = angle_bin_vector_sum_y / angle_bin_area\n",
    "angle_bin_vector_mean_z = angle_bin_vector_sum_z / angle_bin_area\n",
    "\n",
    "angle_bin_vector = np.zeros((1, bin_num * bin_num, 3))\n",
    "angle_bin_vector[:, :, 0] = angle_bin_vector_mean_x\n",
    "angle_bin_vector[:, :, 1] = angle_bin_vector_mean_y\n",
    "angle_bin_vector[:, :, 2] = angle_bin_vector_mean_z\n",
    "\n",
    "angle_bin_vector_unit = angle_bin_vector[0].copy()\n",
    "angle_bin_vector_unit /= np.sqrt((angle_bin_vector_unit**2).sum(axis=1).reshape((-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf3c5ed3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T01:43:04.420575Z",
     "iopub.status.busy": "2023-04-20T01:43:04.420280Z",
     "iopub.status.idle": "2023-04-20T01:43:04.427388Z",
     "shell.execute_reply": "2023-04-20T01:43:04.426346Z"
    },
    "papermill": {
     "duration": 0.017493,
     "end_time": "2023-04-20T01:43:04.429706",
     "exception": false,
     "start_time": "2023-04-20T01:43:04.412213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pred_to_angle(_pred, power = 1.35, epsilon = 1e-8):\n",
    "    # Convert prediction\n",
    "    pred = _pred.copy()\n",
    "    pred**= power    \n",
    "    \n",
    "    \n",
    "    pred_vector = (pred.reshape((-1, bin_num**2, 1)) * angle_bin_vector).sum(axis = 1)\n",
    "    \n",
    "    # Normalize\n",
    "    pred_vector_norm = np.sqrt((pred_vector**2).sum(axis = 1))\n",
    "    mask = pred_vector_norm < epsilon\n",
    "    pred_vector_norm[mask] = 1\n",
    "    \n",
    "    # Assign <1, 0, 0> to very small vectors (badly predicted)\n",
    "    pred_vector /= pred_vector_norm.reshape((-1, 1))\n",
    "    pred_vector[mask] = np.array([1., 0., 0.])\n",
    "    \n",
    "    # Convert to angle\n",
    "    azimuth = np.arctan2(pred_vector[:, 1], pred_vector[:, 0])\n",
    "    azimuth[azimuth < 0] += 2 * np.pi\n",
    "    zenith = np.arccos(pred_vector[:, 2])\n",
    "    \n",
    "    # Mask bad norm predictions as 0, 0\n",
    "    azimuth[mask] = 0.\n",
    "    zenith[mask] = 0.\n",
    "    \n",
    "    return azimuth, zenith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70f630f",
   "metadata": {
    "papermill": {
     "duration": 0.007359,
     "end_time": "2023-04-20T01:43:04.444251",
     "exception": false,
     "start_time": "2023-04-20T01:43:04.436892",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Weighted-Vector Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c323106b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T01:43:04.460814Z",
     "iopub.status.busy": "2023-04-20T01:43:04.460497Z",
     "iopub.status.idle": "2023-04-20T01:43:04.468295Z",
     "shell.execute_reply": "2023-04-20T01:43:04.467259Z"
    },
    "papermill": {
     "duration": 0.018572,
     "end_time": "2023-04-20T01:43:04.470308",
     "exception": false,
     "start_time": "2023-04-20T01:43:04.451736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weighted_vector_ensemble(angles, weight):\n",
    "    # Convert angle to vector\n",
    "    vec_models = list()\n",
    "    for angle in angles:\n",
    "        az, zen = angle\n",
    "        sa = np.sin(az)\n",
    "        ca = np.cos(az)\n",
    "        sz = np.sin(zen)\n",
    "        cz = np.cos(zen)\n",
    "        vec = np.stack([sz * ca, sz * sa, cz], axis=1)\n",
    "        vec_models.append(vec)\n",
    "    vec_models = np.array(vec_models)\n",
    "\n",
    "    # Weighted-mean\n",
    "    vec_mean = (weight.reshape((-1, 1, 1)) * vec_models).sum(axis=0) / weight.sum()\n",
    "    vec_mean /= np.sqrt((vec_mean**2).sum(axis=1)).reshape((-1, 1))\n",
    "\n",
    "    # Convert vector to angle\n",
    "    zenith = np.arccos(vec_mean[:, 2])\n",
    "    azimuth = np.arctan2(vec_mean[:, 1], vec_mean[:, 0])\n",
    "    azimuth[azimuth < 0] += 2 * np.pi\n",
    "    \n",
    "    return azimuth, zenith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd19938",
   "metadata": {
    "papermill": {
     "duration": 0.007227,
     "end_time": "2023-04-20T01:43:04.484749",
     "exception": false,
     "start_time": "2023-04-20T01:43:04.477522",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Single event reader function\n",
    "\n",
    "- Pick-up important data points first\n",
    "    - Rank 3 (First)\n",
    "        - not aux, in valid time window\n",
    "    - Rank 2\n",
    "        - not aux, out of valid time window\n",
    "    - Rank 1\n",
    "        - aux, in valid time window\n",
    "    - Rank 0 (Last)\n",
    "        - aux, out of valid time window\n",
    "    - In each ranks, take pulses from highest charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "499ef45e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T01:43:04.508445Z",
     "iopub.status.busy": "2023-04-20T01:43:04.508011Z",
     "iopub.status.idle": "2023-04-20T01:43:04.525361Z",
     "shell.execute_reply": "2023-04-20T01:43:04.524502Z"
    },
    "papermill": {
     "duration": 0.035465,
     "end_time": "2023-04-20T01:43:04.527555",
     "exception": false,
     "start_time": "2023-04-20T01:43:04.492090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Placeholder\n",
    "open_batch_dict = dict()\n",
    "\n",
    "# Read single event from batch_meta_df\n",
    "def read_event(event_idx, batch_meta_df, pulse_count):\n",
    "    # Read metadata\n",
    "    batch_id, first_pulse_index, last_pulse_index = batch_meta_df.iloc[event_idx][[\"batch_id\", \"first_pulse_index\", \"last_pulse_index\"]].astype(\"int\")\n",
    "\n",
    "    # close past batch df\n",
    "    if batch_id - 1 in open_batch_dict.keys():\n",
    "        del open_batch_dict[batch_id - 1]\n",
    "\n",
    "    # open current batch df\n",
    "    if batch_id not in open_batch_dict.keys():\n",
    "        open_batch_dict.update({batch_id: pd.read_parquet(test_format.format(batch_id=batch_id))})\n",
    "    \n",
    "    batch_df = open_batch_dict[batch_id]\n",
    "    \n",
    "    # Read event\n",
    "    event_feature = batch_df[first_pulse_index:last_pulse_index + 1]\n",
    "    sensor_id = event_feature.sensor_id\n",
    "    \n",
    "    # Merge features into single structured array\n",
    "    dtype = [(\"time\", \"float16\"),\n",
    "             (\"charge\", \"float16\"),\n",
    "             (\"auxiliary\", \"float16\"),\n",
    "             (\"x\", \"float16\"),\n",
    "             (\"y\", \"float16\"),\n",
    "             (\"z\", \"float16\"),\n",
    "             (\"rank\", \"short\")]    \n",
    "    \n",
    "    # Create event_x\n",
    "    event_x = np.zeros(last_pulse_index - first_pulse_index + 1, dtype)\n",
    "    event_x[\"time\"] = event_feature.time.values - event_feature.time.min()\n",
    "    event_x[\"charge\"] = event_feature.charge.values\n",
    "    event_x[\"auxiliary\"] = event_feature.auxiliary.values\n",
    "    event_x[\"x\"] = sensor_geometry_df.x[sensor_id].values\n",
    "    event_x[\"y\"] = sensor_geometry_df.y[sensor_id].values\n",
    "    event_x[\"z\"] = sensor_geometry_df.z[sensor_id].values\n",
    "\n",
    "    # For long event, pick-up\n",
    "    if len(event_x) > pulse_count:\n",
    "        # Find valid time window\n",
    "        t_peak = event_x[\"time\"][event_x[\"charge\"].argmax()]\n",
    "        t_valid_min = t_peak - t_valid_length\n",
    "        t_valid_max = t_peak + t_valid_length\n",
    "        t_valid = (event_x[\"time\"] > t_valid_min) * (event_x[\"time\"] < t_valid_max)\n",
    "\n",
    "        # Rank\n",
    "        event_x[\"rank\"] = 2 * (1 - event_x[\"auxiliary\"]) + (t_valid)\n",
    "\n",
    "        # Sort by Rank and Charge (important goes to backward)\n",
    "        event_x = np.sort(event_x, order = [\"rank\", \"charge\"])\n",
    "\n",
    "        # pick-up from backward\n",
    "        event_x = event_x[-pulse_count:]\n",
    "\n",
    "        # Sort events by time \n",
    "        event_x = np.sort(event_x, order = \"time\")\n",
    "\n",
    "    return event_idx, len(event_x), event_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3487cada",
   "metadata": {
    "papermill": {
     "duration": 0.007092,
     "end_time": "2023-04-20T01:43:04.542124",
     "exception": false,
     "start_time": "2023-04-20T01:43:04.535032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e7aa0ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T01:43:04.558068Z",
     "iopub.status.busy": "2023-04-20T01:43:04.557489Z",
     "iopub.status.idle": "2023-04-20T01:43:04.645638Z",
     "shell.execute_reply": "2023-04-20T01:43:04.644707Z"
    },
    "papermill": {
     "duration": 0.09883,
     "end_time": "2023-04-20T01:43:04.648161",
     "exception": false,
     "start_time": "2023-04-20T01:43:04.549331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read Test Meta data\n",
    "test_meta_df = pq.read_table(home_dir + 'test_meta.parquet').to_pandas()\n",
    "batch_counts = test_meta_df.batch_id.value_counts().sort_index()\n",
    "\n",
    "batch_max_index = batch_counts.cumsum()\n",
    "batch_max_index[test_meta_df.batch_id.min() - 1] = 0\n",
    "batch_max_index = batch_max_index.sort_index()\n",
    "\n",
    "# Support Function\n",
    "def test_meta_df_spliter(batch_id):\n",
    "    return test_meta_df.loc[batch_max_index[batch_id - 1]:batch_max_index[batch_id] - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3c4eb2",
   "metadata": {
    "papermill": {
     "duration": 0.007144,
     "end_time": "2023-04-20T01:43:04.663478",
     "exception": false,
     "start_time": "2023-04-20T01:43:04.656334",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Read test data and predict batchwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25539e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T23:06:02.160451Z",
     "iopub.status.busy": "2023-04-14T23:06:02.159919Z",
     "iopub.status.idle": "2023-04-14T23:06:02.167573Z",
     "shell.execute_reply": "2023-04-14T23:06:02.166069Z",
     "shell.execute_reply.started": "2023-04-14T23:06:02.160416Z"
    },
    "papermill": {
     "duration": 0.006926,
     "end_time": "2023-04-20T01:43:04.677537",
     "exception": false,
     "start_time": "2023-04-20T01:43:04.670611",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# [LB 1.183] Polar Lightning\n",
    "I took this from the invaluable works of roberthatch  \n",
    "https://www.kaggle.com/code/roberthatch/lb-1-183-lightning-fast-baseline-with-polars/comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79abb633",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T01:43:04.694273Z",
     "iopub.status.busy": "2023-04-20T01:43:04.693956Z",
     "iopub.status.idle": "2023-04-20T01:43:36.133721Z",
     "shell.execute_reply": "2023-04-20T01:43:36.132652Z"
    },
    "papermill": {
     "duration": 31.4514,
     "end_time": "2023-04-20T01:43:36.136152",
     "exception": false,
     "start_time": "2023-04-20T01:43:04.684752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing polars, please wait about 35 seconds...\n",
      "Processing /kaggle/input/polars01516/polars-0.15.16-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from polars==0.15.16) (4.1.1)\r\n",
      "Installing collected packages: polars\r\n",
      "Successfully installed polars-0.15.16\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "## Configuration parameters\n",
    "#MODE = 'train'\n",
    "MODE = 'test'\n",
    "\n",
    "# USE_POLARS = False\n",
    "USE_POLARS = True\n",
    "\n",
    "# TRAIN_MAX_EVENTS = 20000\n",
    "TRAIN_MAX_EVENTS = None\n",
    "TRAIN_BATCH_START = 1\n",
    "TRAIN_N_BATCHES = 1\n",
    "\n",
    "## I pulled in one piece of older code to demonstrate \"before and after\".\n",
    "## Set to True (and USE_POLARS=False) if interested in seeing the difference.\n",
    "USE_UNOPTIMIZED = False\n",
    "\n",
    "\n",
    "#### HYPERPARAMETERS ####\n",
    "\n",
    "## For setting auxiliary = False\n",
    "## Hand-tuned and hand-validated, I mostly used batches 100-105, and probably early on also touched batch 1.\n",
    "## TODO: revisit the deep core logic now that I've learned about the deep veto layer: https://www.kaggle.com/competitions/icecube-neutrinos-in-deep-ice/discussion/381702\n",
    "FIND_BEST_POINTS = True\n",
    "MIN_PRIMARY_DATAPOINTS = 2\n",
    "MAX_Z = 3\n",
    "MAX_DEEP_Z = 1\n",
    "MAX_T = 350\n",
    "MAX_DEEP_T = 180\n",
    "if USE_POLARS:\n",
    "    ## Only implmented in polars version.\n",
    "    AUX_FALSE_WEIGHT = 0.01\n",
    "\n",
    "## Ensemble and algorithm selection parameters.\n",
    "## First weight is center of charge algorithm introduced in this notebook.\n",
    "## Second weight is the unweighted version. Which turns out to be the least-squares algorithm: https://www.kaggle.com/competitions/icecube-neutrinos-in-deep-ice/discussion/381747\n",
    "USE_ENSEMBLE = True\n",
    "WEIGHTS = [0.58, 0.42]\n",
    "if not USE_ENSEMBLE and not USE_POLARS:\n",
    "    ALGORITHM = 'center of charge'\n",
    "#     ALGORITHM = 'least squares'\n",
    "    if ALGORITHM == 'least squares':\n",
    "        USE_WEIGHTED_LEAST_SQUARES = False\n",
    "\n",
    "\n",
    "## Constants\n",
    "INPUT_DIR = '/kaggle/input/icecube-neutrinos-in-deep-ice'\n",
    "\n",
    "## Basic configuration override logic\n",
    "if MODE == 'test':\n",
    "    TRAIN_MAX_EVENTS = None\n",
    "if USE_ENSEMBLE:\n",
    "    USE_WEIGHTED_LEAST_SQUARES = False\n",
    "    \n",
    "if USE_POLARS:\n",
    "    try:\n",
    "        import polars as pl\n",
    "    except:\n",
    "        print('Installing polars, please wait about 35 seconds...')\n",
    "        !pip install /kaggle/input/polars01516/polars-0.15.16-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "        import polars as pl\n",
    "        \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import gc\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "## Condensed for space. See here for expanded original version: https://www.kaggle.com/code/sohier/mean-angular-error\n",
    "def angular_dist_score(az_true, zen_true, az_pred, zen_pred):\n",
    "    if not (np.all(np.isfinite(az_true)) and\n",
    "            np.all(np.isfinite(zen_true)) and\n",
    "            np.all(np.isfinite(az_pred)) and\n",
    "            np.all(np.isfinite(zen_pred))):\n",
    "        raise ValueError(\"All arguments must be finite\")\n",
    "    sa1 = np.sin(az_true)\n",
    "    ca1 = np.cos(az_true)\n",
    "    sz1 = np.sin(zen_true)\n",
    "    cz1 = np.cos(zen_true)\n",
    "    sa2 = np.sin(az_pred)\n",
    "    ca2 = np.cos(az_pred)\n",
    "    sz2 = np.sin(zen_pred)\n",
    "    cz2 = np.cos(zen_pred)\n",
    "    scalar_prod = sz1*sz2*(ca1*ca2 + sa1*sa2) + (cz1*cz2)\n",
    "    scalar_prod =  np.clip(scalar_prod, -1, 1)\n",
    "    return np.average(np.abs(np.arccos(scalar_prod)))\n",
    "\n",
    "## TODO: It would be good to benchmark versus other implementations, like arctan2 used here: https://www.kaggle.com/code/shlomoron/icecube-eda-pca-baseline-cv-1-28-lb-1-274 \n",
    "\n",
    "## This version has a small optimization trick, calculating azimuth without regard for z or zenith\n",
    "## This version is suboptimal if the vectors are already unit vectors, or if you need 3d unit vectors again later for some other step.\n",
    "def angles_from_vectors(vectors):\n",
    "    v_squared = np.square(vectors)\n",
    "    \n",
    "    ## Shortcut optimization for azimuth: calculate 2d unit vectors for x and y independent of z\n",
    "    xy_sq = np.sum(v_squared[:, 0:2], axis=1)\n",
    "    xy_d = np.sqrt(xy_sq)[:, None]\n",
    "    np.seterr(divide='ignore', invalid='ignore') ## Turn off the warning temporarily\n",
    "    vectors[:, 0:2] = np.where(xy_d == 0, xy_d, vectors[:, 0:2]/xy_d)\n",
    "\n",
    "    ## For z, use full 3d unit vector\n",
    "    d = np.sqrt(xy_sq + v_squared[:, 2])\n",
    "    vectors[:, 2] = np.where(d == 0, d, vectors[:, 2]/d)\n",
    "    np.seterr(divide='warn', invalid='warn') ## Turn back on\n",
    "\n",
    "    ## As mentioned by others, clip solely to avoid floating point errors, the unit vectors should already be within this range.\n",
    "    vectors =  np.clip(vectors, -1, 1)\n",
    "\n",
    "    azimuth = np.arccos(vectors[:, 0])\n",
    "    ## if y < 0, convert from quadrants 1 and 2 to quadrants 3 and 4\n",
    "    azimuth = np.where(vectors[:, 1] >= 0, azimuth, 2*math.pi - azimuth)\n",
    "    azimuth = np.where(np.isfinite(azimuth), azimuth, 0.0)\n",
    "\n",
    "    zenith = np.arccos(vectors[:, 2])\n",
    "    ## IMPORTANT: zenith angles are not evenly distributed, so set the error case to pi/2!\n",
    "    ## (even though x, y, z might be. It would be a fun exercise to check if random values\n",
    "    ##  for x, y, z converted to zenith angles would match the observed distribution of zenith angles in the train labels)\n",
    "    zenith = np.where(np.isfinite(zenith), zenith, math.pi/2)\n",
    "\n",
    "    return np.stack([azimuth, zenith], axis=1)\n",
    "\n",
    "## Takes a list of azimuth np arrays, a list of zenith np arrays, and an optional list of numerical weights,\n",
    "## and ensembles into a final direction.\n",
    "##\n",
    "## It's not really optimal in terms of lines of code nor performance,\n",
    "## since in most or all cases you are converting a unit vector to an angle,\n",
    "## converting back to a unit vector, averaging, then converting to the final angle.\n",
    "## However, it is quite convenient, because you can always use this at the end\n",
    "## to ensemble the results originating from any number of notebooks or sources.\n",
    "def average_angles(az_list, zen_list, weights=None):\n",
    "    assert(len(az_list) == len(zen_list))\n",
    "    total = az_list[0].shape[0]\n",
    "    x = np.zeros(total)\n",
    "    y = np.zeros(total)\n",
    "    z = np.zeros(total)\n",
    "    for i in range(len(az_list)):\n",
    "        w = 1\n",
    "        if weights is not None:\n",
    "            w = weights[i]\n",
    "        az = az_list[i]\n",
    "        zen = zen_list[i]\n",
    "        assert(az.shape[0] == total)\n",
    "        assert(zen.shape[0] == total)\n",
    "        if not (np.all(np.isfinite(az)) and\n",
    "                np.all(np.isfinite(zen))):\n",
    "            raise ValueError(\"All arguments must be finite\")\n",
    "        sz = np.sin(zen)\n",
    "        x += w*np.cos(az)*sz\n",
    "        y += w*np.sin(az)*sz\n",
    "        z += w*np.cos(zen)\n",
    "    tot_w = len(az_list)\n",
    "    if weights is not None:\n",
    "        tot_w = sum(weights)\n",
    "    x = x / tot_w\n",
    "    y = y / tot_w\n",
    "    z = z / tot_w\n",
    "    d = np.sqrt(np.square(x) + np.square(y) + np.square(z))\n",
    "    x = x / d\n",
    "    y = y / d\n",
    "    z = z / d\n",
    "    return angles_from_vectors(np.stack([x, y, z], axis=1))\n",
    "\n",
    "def center_of_charge(batch):\n",
    "    ## Groupby -> transform is the key trick to avoiding the dreaded 'for each event' loop,\n",
    "    ## and thus getting ~30-60x speed boost improvement!\n",
    "    ## If you need any min, max, mean or other simple value from the event group,\n",
    "    ## you can precalculate it for each group and broadcast it\n",
    "    batch['ev_t_min'] = batch.groupby('event_id')['time'].transform('min')\n",
    "    batch['ev_t_max'] = batch.groupby('event_id')['time'].transform('max')\n",
    "    \n",
    "    ## Now we can just implement our formula! w0 and w1 are the time-weighted charge cases.\n",
    "    ## Gather the values we need\n",
    "    batch['w1'] = batch.charge * (batch.time - batch.ev_t_min) / (batch.ev_t_max - batch.ev_t_min)\n",
    "    batch['w0'] = batch.charge - batch.w1\n",
    "    batch['wx0'] = batch.x * batch.w0\n",
    "    batch['wy0'] = batch.y * batch.w0\n",
    "    batch['wz0'] = batch.z * batch.w0\n",
    "    batch['wx1'] = batch.x * batch.w1\n",
    "    batch['wy1'] = batch.y * batch.w1\n",
    "    batch['wz1'] = batch.z * batch.w1\n",
    "    df = batch[['w0', 'w1', 'wx0', 'wy0', 'wz0', 'wx1', 'wy1', 'wz1']]\n",
    "\n",
    "    ## Calculate all the sums!\n",
    "    df = df.groupby('event_id').sum()\n",
    "    \n",
    "    ## Now do the final divide of the weighted center by the sum of the weights.\n",
    "    df[['wx0', 'wy0', 'wz0']] = df[['wx0', 'wy0', 'wz0']].div(df.w0, axis=0)\n",
    "    df[['wx1', 'wy1', 'wz1']] = df[['wx1', 'wy1', 'wz1']].div(df.w1, axis=0)\n",
    "    \n",
    "    ## The direction the neutrino is traveling FROM is point0 - point1, instead of point1 - point0.\n",
    "    ## Counter-intuitive to me, but fortunately, easy to notice and correct if your score is > 1.57 instead of less.\n",
    "    df[['x', 'y', 'z']] = df[['wx0', 'wy0', 'wz0']].values - df[['wx1', 'wy1', 'wz1']].values\n",
    "\n",
    "    df = df[['x', 'y', 'z']]\n",
    "    df[['azimuth', 'zenith']] = angles_from_vectors(df.values)\n",
    "\n",
    "    return(df[['azimuth', 'zenith']])\n",
    "\n",
    "def least_squares(batch, weighted=False):\n",
    "    batch['xt'] = batch.x * batch.time\n",
    "    batch['yt'] = batch.y * batch.time\n",
    "    batch['zt'] = batch.z * batch.time\n",
    "    batch['tt'] = batch.time * batch.time\n",
    "    if weighted:\n",
    "        df = batch[['x', 'y', 'z', 'time', 'xt', 'yt', 'zt', 'tt']] * batch.charge.values[:, None]\n",
    "        df['charge'] = batch.charge\n",
    "        df = df.groupby('event_id').sum()\n",
    "        df = df.div(df.charge, axis=0)\n",
    "    else:\n",
    "        df = batch[['x', 'y', 'z', 'time', 'xt', 'yt', 'zt', 'tt']]\n",
    "        df = df.groupby('event_id').mean()\n",
    "    df[['x', 'y', 'z']] = (\n",
    "                              (df[['xt', 'yt', 'zt']].values - (df[['x', 'y', 'z']].values * df['time'].values[:, None]))\n",
    "                            / (df['tt'].values - (df.time.values * df.time.values))[:, None]\n",
    "                          )\n",
    "    ## Reverse it\n",
    "    df = -df[['x', 'y', 'z']]\n",
    "    df[['azimuth', 'zenith']] = angles_from_vectors(df.values)\n",
    "    return df[['azimuth', 'zenith']]\n",
    "\n",
    "\n",
    "def process_batch(batch_id, sensor, max_events=None):\n",
    "    print('load batch...')\n",
    "    batch = pd.read_parquet(f'{INPUT_DIR}/{MODE}/batch_{batch_id}.parquet')\n",
    "\n",
    "    ## Limit to max_events\n",
    "    if max_events is not None:\n",
    "        batch_i = batch.reset_index()\n",
    "        event_ids = batch_i.event_id.drop_duplicates()\n",
    "        end_index = event_ids.index[max_events]\n",
    "        batch = batch_i[:end_index].set_index('event_id')\n",
    "    print(batch.shape)\n",
    "\n",
    "    ## Merge in sensor x,y,z data\n",
    "    batch = batch.reset_index().merge(sensor, how='left', on='sensor_id', left_index=False).set_index('event_id')\n",
    "\n",
    "    ## The logic for auxiliary = False is very basic, we can improve on it. Initial discussion here: \n",
    "    if FIND_BEST_POINTS:\n",
    "        batch = find_best_points_pandas(batch)\n",
    "\n",
    "    ## Limit to primary (aux=False) datapoints if there's enough of them.\n",
    "    ## For event_ids with too few, set all rows to aux=False. This handles these cases without any for loop logic.\n",
    "    ## MIN_PRIMARY_DATAPOINTS is a tuned value, based on optimizing the score on batches 101-110.\n",
    "    ## But the result was 'as low as possible'.\n",
    "    batch['primary_count'] = batch.groupby('event_id')['auxiliary'].transform('count') - batch.groupby('event_id')['auxiliary'].transform('sum')\n",
    "    batch.loc[batch.primary_count < MIN_PRIMARY_DATAPOINTS, 'auxiliary'] = False\n",
    "    batch = batch[batch.auxiliary == False]\n",
    "    print(batch.shape)\n",
    "\n",
    "    if USE_ENSEMBLE or ALGORITHM == 'center of charge':\n",
    "        df = center_of_charge(batch)\n",
    "        if USE_ENSEMBLE:\n",
    "            df1 = df\n",
    "    if USE_ENSEMBLE or ALGORITHM == 'least squares':\n",
    "        df = least_squares(batch, weighted=USE_WEIGHTED_LEAST_SQUARES)\n",
    "    if USE_ENSEMBLE:\n",
    "        df[['azimuth', 'zenith']] = average_angles([df1.azimuth.values, df.azimuth.values],\n",
    "                                                   [df1.zenith.values, df.zenith.values], \n",
    "                                                   weights=WEIGHTS)\n",
    "    return df\n",
    "\n",
    "def proximity(e, col):\n",
    "    ## Since we explode the data to an NxN array, if N is too high it will take a long time and anyways we'll run out of memory.\n",
    "    ## TODO: see how high we can go before we run out of memory\n",
    "    ## TODO: consider subsampling instead of simply returning the baseline auxiliary setting?\n",
    "    ##       And/or splitting on string_id to get much smaller groups\n",
    "    if e.shape[0] > 2000:\n",
    "        return e[:, col['auxiliary']]\n",
    "\n",
    "    ## The magic None in the line below is a new thing I learned while working on this notebook.\n",
    "    ## It is shorthand for np.newaxis, and broadcasts the array to another dimension.\n",
    "    ## This allows us to create an NxN array for each event, so that we can\n",
    "    ## check if *any* other row in the event meets our proximity in time and height requirements.\n",
    "    ## For any matching pairs, then we set both rows as auxiliary = False. Rows without matches are auxiliary = True.\n",
    "    deltas = np.abs(e[:, [col['string_id'], col['depth_id'], col['time']]] - e[:, None, [col['string_id'], col['depth_id'], col['time']]])\n",
    "    dz = deltas[:, :, 1]\n",
    "\n",
    "    ## if same depth or different string id, ignore by setting dz > the max threshold used later.\n",
    "    dz[(dz == 0) | (deltas[:, :, 0] != 0)] = MAX_Z + MAX_DEEP_Z + 1\n",
    "\n",
    "    ## if sensor is not a deep ice sensor, and time > MAX_T, ignore\n",
    "    mask = (e[:, col['sensor_id']] < 4680)\n",
    "    mask = np.broadcast_to(mask, (mask.shape[0], mask.shape[0])).T\n",
    "    dz[mask & (deltas[:, :, 2] > MAX_T)] = MAX_Z + MAX_DEEP_Z + 1\n",
    "    ## if sensor IS a deep ice sensor, and time > MAX_DEEP_T, ignore\n",
    "    mask = (e[:, col['sensor_id']] >= 4680)\n",
    "    mask = np.broadcast_to(mask, (mask.shape[0], mask.shape[0])).T\n",
    "    dz[mask & (deltas[:, :, 2] > MAX_DEEP_T)] = MAX_Z + MAX_DEEP_Z + 1\n",
    "\n",
    "    ## Now take the min (best) result for each row com\n",
    "    dz = dz.min(axis=1)\n",
    "    ## If no matches, the default, then everything is aux=True\n",
    "    e[:, col['auxiliary']] = True\n",
    "    ## If not deep ice and distance less than threshold, or deep ice and distance less than other threshold, then we have a match!\n",
    "    e[((e[:, col['sensor_id']] < 4680) & (dz <= MAX_Z)) | ((e[:, col['sensor_id']] >= 4680) & (dz <= MAX_DEEP_Z)), col['auxiliary']] = False\n",
    "    ## Return only the data needed to speed up the np.concatenate called next.\n",
    "    return e[:, col['auxiliary']]\n",
    "\n",
    "## You can ignore this one unless interested in a deep dive on performance optimization\n",
    "## It is provided as a way of comparing the changes versus the pure numpy version.\n",
    "## Comments removed from this copy to conserve vertical space.\n",
    "def proximity_unoptimized(df):\n",
    "    if df.shape[0] > 2000:\n",
    "        return df\n",
    "    deltas = np.abs(df[['string_id', 'depth_id', 'time']].values - df[['string_id', 'depth_id', 'time']].values[:, None, :])\n",
    "    mask = (df.sensor_id < 4680)\n",
    "    dz = deltas[:, :, 1]\n",
    "\n",
    "    dz[(dz == 0) | (deltas[:, :, 0] != 0)] = MAX_Z + MAX_DEEP_Z + 1\n",
    "\n",
    "    mask = (df.sensor_id < 4680)\n",
    "    mask = np.broadcast_to(mask, (mask.shape[0], mask.shape[0])).T\n",
    "    dz[mask & (deltas[:, :, 2] > MAX_T)] = MAX_Z + MAX_DEEP_Z + 1\n",
    "    mask = (df.sensor_id >= 4680)\n",
    "    mask = np.broadcast_to(mask, (mask.shape[0], mask.shape[0])).T\n",
    "    dz[mask & (deltas[:, :, 2] > MAX_DEEP_T)] = MAX_Z + MAX_DEEP_Z + 1\n",
    "\n",
    "    dz = dz.min(axis=1)\n",
    "    df.auxiliary = True\n",
    "    df.loc[((df.sensor_id < 4680) & (dz <= MAX_Z)) | ((df.sensor_id >= 4680) & (dz <= MAX_DEEP_Z)), 'auxiliary'] = False\n",
    "    return df\n",
    "\n",
    "\n",
    "def find_best_points_pandas(batch):\n",
    "    if USE_UNOPTIMIZED:\n",
    "        cols = ['sensor_id', 'time', 'auxiliary', 'string_id', 'depth_id']\n",
    "        batch[cols] = batch[cols].groupby('event_id').progress_apply(proximity_unoptimized)\n",
    "        return batch\n",
    "\n",
    "    ## np.split used as a pure numpy equivalent of groupby\n",
    "    ## Note this version didn't minimize the size of the inputs, but does make sure all dtypes are the same for an efficient np array.\n",
    "    column_to_index = { k:v for v,k in enumerate(batch.columns)}\n",
    "    events = np.split(batch.values.astype('float32'), np.unique(batch.index.values, return_index=True)[1][1:])\n",
    "\n",
    "    ## Run each event sequentially in a list comprehension, then join back together with np.concatenate.\n",
    "    ## So far tried and failed to find a reasonable solution to avoid this groupby > apply > join loop.\n",
    "    ## Note that this line overrides the dtype of 'auxiliary' column to float32\n",
    "    batch.auxiliary = np.concatenate([proximity(e, column_to_index) for e in tqdm(events)])\n",
    "    return batch\n",
    "\n",
    "def time_weighted_centering(batch, charge_weighted=True):\n",
    "    ## Polars equivalent to groupby->transform is called 'over'. We again use this to get min and max without a for loop.\n",
    "    batch = batch.with_columns([pl.col('time').min().over('event_id').alias('ev_t_min'),\n",
    "                                pl.col('time').max().over('event_id').alias('ev_t_max')])\n",
    "    if charge_weighted:\n",
    "        batch = batch.with_columns((pl.col('charge') * (pl.col('time') - pl.col('ev_t_min'))\n",
    "                                    / (pl.col('ev_t_max') - pl.col('ev_t_min'))).alias('w1'))\n",
    "        batch = batch.with_columns((pl.col('charge') - pl.col('w1')).alias('w0'))\n",
    "    else:\n",
    "        batch = batch.with_columns(((pl.col('time') - pl.col('ev_t_min'))\n",
    "                                    / (pl.col('ev_t_max') - pl.col('ev_t_min'))).alias('w1'))\n",
    "        batch = batch.with_columns((pl.lit(1) - pl.col('w1')).alias('w0'))\n",
    "\n",
    "    batch = batch.select(\n",
    "        [\n",
    "            pl.col('event_id'),\n",
    "            pl.col('w0'),\n",
    "            pl.col('w1'),\n",
    "            (pl.col('x') * pl.col('w0')).alias('wx0'),\n",
    "            (pl.col('y') * pl.col('w0')).alias('wy0'),\n",
    "            (pl.col('z') * pl.col('w0')).alias('wz0'),\n",
    "            (pl.col('x') * pl.col('w1')).alias('wx1'),\n",
    "            (pl.col('y') * pl.col('w1')).alias('wy1'),\n",
    "            (pl.col('z') * pl.col('w1')).alias('wz1'),\n",
    "        ]\n",
    "    ).collect().groupby('event_id', maintain_order=True).sum()\n",
    "\n",
    "    ## The direction the neutrino is traveling FROM is point0 - point1, instead of point1 - point0.\n",
    "    ## Counter-intuitive to me, but fortunately, easy to notice and correct if your score is > 1.57 instead of less.\n",
    "    batch_values = batch.select(\n",
    "        [\n",
    "            ((pl.col('wx0') / pl.col('w0')) - (pl.col('wx1') / pl.col('w1'))).alias('x'),\n",
    "            ((pl.col('wy0') / pl.col('w0')) - (pl.col('wy1') / pl.col('w1'))).alias('y'),\n",
    "            ((pl.col('wz0') / pl.col('w0')) - (pl.col('wz1') / pl.col('w1'))).alias('z'),\n",
    "        ]\n",
    "    ).to_numpy()\n",
    "    return angles_from_vectors(batch_values), batch\n",
    "\n",
    "\n",
    "## We use the same numpy proximity function, the only difference is we convert from and to a Polars df instead of a Pandas df.\n",
    "def find_best_points_polars(batch):\n",
    "    ## Minimize the size of the inputs, and make sure all data types are the same for an efficient np array\n",
    "    df = batch.select([pl.col('sensor_id'), pl.col('time'), pl.col('auxiliary'), pl.col('string_id'), pl.col('depth_id')])\n",
    "    column_to_index = { k:v for v,k in enumerate(df.columns)}\n",
    "    batch_values = df.to_numpy().astype('float32')\n",
    "\n",
    "    ## Pure numpy equivalent of groupby\n",
    "    events = np.split(batch_values, np.unique(batch.select(pl.col('event_id')), return_index=True)[1][1:])\n",
    "\n",
    "    ## Run each event sequentially in a list comprehension, then join back together with np.concatenate. So far tried and failed to find a reasonable solution to avoid this groupby > apply > join loop.\n",
    "    ## Rename 'auxiliary' -> 'best', and flip the boolean value\n",
    "    batch = batch.with_columns(pl.Series(np.concatenate([proximity(e, column_to_index) for e in tqdm(events)]).astype('bool')).alias('best')).with_columns(pl.col('best').is_not())\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "\n",
    "if USE_POLARS:\n",
    "    sub = []\n",
    "    #for batch_id in range(batch_id_start,batch_id_end):\n",
    "def get_line_fit_angles(batch_id):\n",
    "    print(MODE)\n",
    "    ## Scan parquet is part of Polars lazy evaluation, so no cost yet,\n",
    "    ## and it can figure out optimizations when we finally 'collect' it later.\n",
    "    meta = pl.scan_parquet(f'{INPUT_DIR}/{MODE}_meta.parquet')\n",
    "\n",
    "    print('load sensor data...')\n",
    "    sensor = (pl.scan_csv(f'{INPUT_DIR}/sensor_geometry.csv')\n",
    "                .with_columns([\n",
    "                    pl.col('sensor_id').cast(pl.Int16),\n",
    "                    (pl.col('sensor_id') // 60).alias('string_id'),\n",
    "                    (pl.col('sensor_id') % 60).alias('depth_id'),                \n",
    "                ])\n",
    "             )\n",
    "\n",
    "    print(sensor)\n",
    "\n",
    "    if MODE == 'train':\n",
    "        batch_id_start = TRAIN_BATCH_START\n",
    "        batch_id_end = batch_id_start + TRAIN_N_BATCHES\n",
    "    else:\n",
    "        batch_id_start = meta.select(pl.col('batch_id')).collect()[0, 0]\n",
    "        batch_id_end = meta.select(pl.col('batch_id')).collect()[-1, 0] + 1\n",
    "\n",
    "    print(batch_id_start, batch_id_end)\n",
    "    \n",
    "    print(batch_id)\n",
    "    t = time.time()\n",
    "    max_events=TRAIN_MAX_EVENTS\n",
    "    print('load batch...')\n",
    "    batch = pl.scan_parquet(f'{INPUT_DIR}/{MODE}/batch_{batch_id}.parquet')\n",
    "\n",
    "    ## Limit to max_events\n",
    "    if max_events is not None:\n",
    "        batch = batch.collect()\n",
    "        last_event_id = batch.select(pl.col('event_id')).unique()[TRAIN_MAX_EVENTS-1, 0]\n",
    "        batch = batch.lazy().filter(pl.col('event_id') <= last_event_id)\n",
    "\n",
    "    ## Merge in sensor x,y,z data\n",
    "    batch = batch.join(sensor, on='sensor_id', how='left').collect()\n",
    "\n",
    "    ## The logic for auxiliary = False is very basic, we can improve on it. Initial discussion here: \n",
    "    if FIND_BEST_POINTS:\n",
    "        batch = find_best_points_polars(batch)\n",
    "\n",
    "    ## Use data point weights instead of filtering. Still set most points to 0 weight, unless they are the only data points available.\n",
    "    ## MIN_PRIMARY_DATAPOINTS and AUX_FALSE_WEIGHT are tuned values, based on optimizing the score on batches 101-110.\n",
    "    batch = batch.lazy().with_columns((pl.col('best').count().over('event_id') - pl.col('best').sum().over('event_id')).alias('best_count'))\n",
    "    batch = batch.lazy().with_columns((pl.col('auxiliary').count().over('event_id') - pl.col('auxiliary').sum().over('event_id')).alias('non_aux_count'))\n",
    "    batch = batch.with_columns(( pl.when( pl.col('best') | ((pl.col('best_count') < MIN_PRIMARY_DATAPOINTS) & (pl.col('non_aux_count') < MIN_PRIMARY_DATAPOINTS)) )\n",
    "                                            .then(1.0)\n",
    "                                            .otherwise(pl.when(pl.col('auxiliary').is_not())\n",
    "                                                         .then(AUX_FALSE_WEIGHT)\n",
    "                                                         .otherwise(0.0)\n",
    "                                                    ) \n",
    "                                      ).alias('trust'))\n",
    "    batch = batch.lazy().with_columns((pl.col('charge') * pl.col('trust')).alias('charge'))\n",
    "\n",
    "    preds, events = time_weighted_centering(batch)\n",
    "    if USE_ENSEMBLE:\n",
    "        preds1 = preds\n",
    "#             preds, events = time_weighted_centering(batch, charge_weighted=False)\n",
    "        ## Instead of charge_weighted=False, set charge*aux to just equal aux.\n",
    "        batch = batch.lazy().with_columns(pl.col('trust').alias('charge'))\n",
    "        preds, events = time_weighted_centering(batch)\n",
    "        preds = average_angles([preds1[:, 0], preds[:, 0]], [preds1[:, 1], preds[:, 1]], weights=WEIGHTS)\n",
    "\n",
    "    '''\n",
    "    if MODE == 'test':\n",
    "        sub.append(events.select([pl.col('event_id'), pl.Series(preds[:, 0]).alias('azimuth'),\n",
    "                                                     pl.Series(preds[:, 1]).alias('zenith')]))\n",
    "    else:\n",
    "        meta = meta.filter((pl.col('batch_id') >= batch_id_start) & (pl.col('batch_id') < batch_id_end))\n",
    "        if isinstance(meta, pl.LazyFrame):\n",
    "            meta = meta.collect()\n",
    "        meta_values = meta.filter(pl.col('batch_id') == batch_id).select([pl.col('azimuth'), pl.col('zenith')]).to_numpy()\n",
    "        if TRAIN_MAX_EVENTS is not None:\n",
    "            print(angular_dist_score(meta_values[:TRAIN_MAX_EVENTS, 0], meta_values[:TRAIN_MAX_EVENTS, 1], preds[:, 0], preds[:, 1]))\n",
    "        else:\n",
    "            print(angular_dist_score(meta_values[:, 0], meta_values[:, 1], preds[:, 0], preds[:, 1]))\n",
    "    '''\n",
    "    print(f'Time: {time.time() - t:0.2f}s')\n",
    "    return preds\n",
    "\n",
    "#fitted_angle = get_line_fit_angles(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46e26147",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T01:43:36.154110Z",
     "iopub.status.busy": "2023-04-20T01:43:36.153198Z",
     "iopub.status.idle": "2023-04-20T01:43:36.166463Z",
     "shell.execute_reply": "2023-04-20T01:43:36.165512Z"
    },
    "papermill": {
     "duration": 0.024398,
     "end_time": "2023-04-20T01:43:36.168608",
     "exception": false,
     "start_time": "2023-04-20T01:43:36.144210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_data(_data, option = 0, batch_id = 0):\n",
    "    data = _data.copy()\n",
    "    if(option==0 or option==2):\n",
    "        data[:, :, 0] /= 1000   # time\n",
    "        data[:, :, 1] /= 300    # charge\n",
    "        data[:, :, 3:] /= 600   # space\n",
    "\n",
    "        #time to diff_time\n",
    "        data[:,:-1,0] = data[:,1:,0] - data[:,:-1,0]\n",
    "        data[:,-1,0] = 0    \n",
    "\n",
    "        #pseudo momentum (next time position - current position)\n",
    "        pseudo_momentum = data[:, :, 3:6].copy()\n",
    "        pseudo_momentum[:,:-1,:] = pseudo_momentum[:,1:,:] - data[:,:-1, 3:6]\n",
    "        pseudo_momentum[:,-1,:] = 0\n",
    "\n",
    "        for i in range(0, 3):\n",
    "            pseudo_momentum[:,:-1,i][data[:,:-1,0]<0]=0\n",
    "\n",
    "        data[:,:-1,0][data[:,:-1,0]<0] = 0\n",
    "\n",
    "        data[:,:,6:] = pseudo_momentum.copy()\n",
    "        del pseudo_momentum\n",
    "        gc.collect()\n",
    "        #data = np.append(data, pseudo_momentum, axis = 2)\n",
    "        if(option==2):\n",
    "            fitted_angle = get_line_fit_angles(batch_id)\n",
    "            data = {'event_pulses': data, 'fitted_targets': fitted_angle}\n",
    "    elif(option ==1):\n",
    "        # Normalize - Version 6\n",
    "        data[:, :, 0] /= 1000  # time\n",
    "        data[:, :, 1] /= 300  # charge\n",
    "        data[:, :, 3:] /= 600  # space\n",
    "\n",
    "        # distance feature\n",
    "        data[:, :, 6] = np.sqrt((data[:, :, 3] - data[:, :, 3].mean())**2 + (data[:, :, 4] - data[:, :, 4].mean())**2 + (data[:, :, 5] - data[:, :, 5].mean())**2)\n",
    "\n",
    "        data[:, :, 3] = data[:, :, 3] - data[:, :, 3].mean()\n",
    "        data[:, :, 4] = data[:, :, 4] - data[:, :, 4].mean()\n",
    "        data[:, :, 5] = data[:, :, 5] - data[:, :, 5].mean()    \n",
    "\n",
    "        #      \n",
    "        data[:, :, 6] = data[:, :, 1] * data[:, :, 6]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cd03946",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T01:43:36.185924Z",
     "iopub.status.busy": "2023-04-20T01:43:36.185632Z",
     "iopub.status.idle": "2023-04-20T01:45:38.229221Z",
     "shell.execute_reply": "2023-04-20T01:45:38.227958Z"
    },
    "papermill": {
     "duration": 122.058151,
     "end_time": "2023-04-20T01:45:38.234763",
     "exception": false,
     "start_time": "2023-04-20T01:43:36.176612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "load sensor data...\n",
      "naive plan: (run LazyFrame.describe_optimized_plan() to see the optimized plan)\n",
      "\n",
      "   WITH_COLUMNS:\n",
      "   [col(\"sensor_id\").strict_cast(Int16), [(col(\"sensor_id\")) // (60i32)].alias(\"string_id\"), [(col(\"sensor_id\")) % (60i32)].alias(\"depth_id\")]\n",
      "    CSV SCAN /kaggle/input/icecube-neutrinos-in-deep-ice/sensor_geometry.csv\n",
      "    PROJECT */4 COLUMNS\n",
      "\n",
      "661 662\n",
      "661\n",
      "load batch...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336d3299ab15499ebfb8f307b35f2283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.05s\n",
      "option 2 activated\n",
      "option 2 activated\n",
      "option 2 activated\n",
      "option 2 activated\n",
      "option 2 activated\n",
      "option 2 activated\n",
      "option 2 activated\n",
      "option 2 activated\n"
     ]
    }
   ],
   "source": [
    "# Get Batch IDs\n",
    "test_batch_ids = test_meta_df.batch_id.unique()\n",
    "\n",
    "# Submission Placeholders\n",
    "test_event_id = []\n",
    "test_azimuth = []\n",
    "test_zenith = []\n",
    "\n",
    "# Batch Loop\n",
    "for batch_id in test_batch_ids:\n",
    "    # Batch Meta DF\n",
    "    batch_meta_df = test_meta_df_spliter(batch_id)\n",
    "\n",
    "    # Set Pulses\n",
    "    test_x = np.zeros((len(batch_meta_df), pulse_count, 9), dtype = \"float16\")    \n",
    "    test_x[:, :, 2] = -1    \n",
    "\n",
    "    # Read Event Data\n",
    "    def read_event_local(event_idx):\n",
    "        return read_event(event_idx, batch_meta_df, pulse_count)\n",
    "    \n",
    "    # Multiprocess Events\n",
    "    iterator = range(len(batch_meta_df))\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        for event_idx, pulsecount, event_x in pool.map(read_event_local, iterator):\n",
    "            # Features\n",
    "            test_x[event_idx, :pulsecount, 0] = event_x[\"time\"]\n",
    "            test_x[event_idx, :pulsecount, 1] = event_x[\"charge\"]\n",
    "            test_x[event_idx, :pulsecount, 2] = event_x[\"auxiliary\"]\n",
    "            test_x[event_idx, :pulsecount, 3] = event_x[\"x\"]\n",
    "            test_x[event_idx, :pulsecount, 4] = event_x[\"y\"]\n",
    "            test_x[event_idx, :pulsecount, 5] = event_x[\"z\"]\n",
    "    \n",
    "    del batch_meta_df\n",
    "    gc.collect()\n",
    "    \n",
    "\n",
    "    '''\n",
    "    test_x[:, :, 0] /= 1000  # time\n",
    "    test_x[:, :, 1] /= 300  # charge\n",
    "    test_x[:, :, 3:] /= 600  # space\n",
    "        \n",
    "    #230328 junseonglee11 time to time diff\n",
    "    test_x[:,:-1,0] = test_x[:,1:,0] - test_x[:,:-1,0]\n",
    "    test_x[:,-1,0] = 0    \n",
    "    test_x[:,:-1,0][test_x[:,:-1,0]<0] = 0\n",
    "    '''\n",
    "    # Predict\n",
    "    counter = 0\n",
    "    pred_angles = []\n",
    "    for model in models:\n",
    "        # Normalize\n",
    "        feature_count = feature_counts[counter]\n",
    "        \n",
    "        if(counter==0):\n",
    "            test_tmp_x = normalize_data(test_x, input_norm_method[counter], batch_id)                                          \n",
    "        elif(counter>0):\n",
    "            if(input_norm_method[counter-1]!=input_norm_method[counter]):\n",
    "                del test_tmp_x\n",
    "                gc.collect()\n",
    "                test_tmp_x = normalize_data(test_x, input_norm_method[counter], batch_id)     \n",
    "                \n",
    "                \n",
    "        try:\n",
    "            test_tmp_x = test_tmp_x[:,:,:feature_count]\n",
    "        except:\n",
    "            print('option 2 activated')\n",
    "            \n",
    "        \n",
    "        pred_model = model.predict(test_tmp_x, verbose=0)\n",
    "        if(input_norm_method[counter]==0 or input_norm_method[counter]==2):\n",
    "            az_model, zen_model = pred_to_angle(pred_model['encoded_angle'])\n",
    "        else:\n",
    "            az_model, zen_model = pred_to_angle(pred_model)\n",
    "        del pred_model\n",
    "        gc.collect()\n",
    "        pred_angles.append((az_model, zen_model))\n",
    "        counter +=1\n",
    "    \n",
    "    # Get Predicted Azimuth and Zenith\n",
    "    pred_azimuth, pred_zenith = weighted_vector_ensemble(pred_angles, model_weights)\n",
    "    del pred_angles\n",
    "    gc.collect()\n",
    "    \n",
    "    # Get Event IDs\n",
    "    event_ids = test_meta_df.event_id[test_meta_df.batch_id == batch_id].values\n",
    "    \n",
    "    # Finalize \n",
    "    for event_id, azimuth, zenith in zip(event_ids, pred_azimuth, pred_zenith):\n",
    "        if np.isfinite(azimuth) and np.isfinite(zenith):\n",
    "            test_event_id.append(int(event_id))\n",
    "            test_azimuth.append(azimuth)\n",
    "            test_zenith.append(zenith)\n",
    "        else:\n",
    "            test_event_id.append(int(event_id))\n",
    "            test_azimuth.append(0.)\n",
    "            test_zenith.append(0.)\n",
    "            \n",
    "    del test_x, test_tmp_x, pred_azimuth, pred_zenith \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da0af4",
   "metadata": {
    "papermill": {
     "duration": 0.011884,
     "end_time": "2023-04-20T01:45:38.268726",
     "exception": false,
     "start_time": "2023-04-20T01:45:38.256842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd307de9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T01:45:38.295107Z",
     "iopub.status.busy": "2023-04-20T01:45:38.294237Z",
     "iopub.status.idle": "2023-04-20T01:45:38.306793Z",
     "shell.execute_reply": "2023-04-20T01:45:38.305530Z"
    },
    "papermill": {
     "duration": 0.030544,
     "end_time": "2023-04-20T01:45:38.311242",
     "exception": false,
     "start_time": "2023-04-20T01:45:38.280698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create and Save Submission.csv\n",
    "submission_df = pd.DataFrame({\"event_id\": test_event_id,\n",
    "                              \"azimuth\": test_azimuth,\n",
    "                              \"zenith\": test_zenith})\n",
    "submission_df = submission_df.sort_values(by = ['event_id'])\n",
    "submission_df.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0023a041",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T01:45:38.337199Z",
     "iopub.status.busy": "2023-04-20T01:45:38.336507Z",
     "iopub.status.idle": "2023-04-20T01:45:38.356242Z",
     "shell.execute_reply": "2023-04-20T01:45:38.355412Z"
    },
    "papermill": {
     "duration": 0.035364,
     "end_time": "2023-04-20T01:45:38.358721",
     "exception": false,
     "start_time": "2023-04-20T01:45:38.323357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>azimuth</th>\n",
       "      <th>zenith</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2092</td>\n",
       "      <td>1.862817</td>\n",
       "      <td>1.432417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7344</td>\n",
       "      <td>3.329414</td>\n",
       "      <td>2.441870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9482</td>\n",
       "      <td>4.646359</td>\n",
       "      <td>1.568192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_id   azimuth    zenith\n",
       "0      2092  1.862817  1.432417\n",
       "1      7344  3.329414  2.441870\n",
       "2      9482  4.646359  1.568192"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ceed2",
   "metadata": {
    "papermill": {
     "duration": 0.011732,
     "end_time": "2023-04-20T01:45:38.384347",
     "exception": false,
     "start_time": "2023-04-20T01:45:38.372615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 264.219647,
   "end_time": "2023-04-20T01:45:41.328068",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-20T01:41:17.108421",
   "version": "2.3.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1475b98fd12f4e89b02036687a6a9749": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6ea422cdb7b947cebd8eef52befa568c",
       "placeholder": "",
       "style": "IPY_MODEL_e0c5d7d8d1114042939563e1a5578b03",
       "value": " 3/3 [00:00&lt;00:00, 91.65it/s]"
      }
     },
     "3064d3173cc04679b4949098b976663f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_73834d7e70f247e0964d8a300f21b255",
       "placeholder": "",
       "style": "IPY_MODEL_d14d6a2bb6b04069a019a13a0e7d59b4",
       "value": "100%"
      }
     },
     "336d3299ab15499ebfb8f307b35f2283": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3064d3173cc04679b4949098b976663f",
        "IPY_MODEL_427fc2722ccc4d60a0dfd3a925ee33ba",
        "IPY_MODEL_1475b98fd12f4e89b02036687a6a9749"
       ],
       "layout": "IPY_MODEL_93e68fa8672b4f5dbcf66e80a85b463f"
      }
     },
     "427fc2722ccc4d60a0dfd3a925ee33ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_978800c889f848ed866fafdea14c1342",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7d08e09ff0b44b738679a1fee0c88d0d",
       "value": 3.0
      }
     },
     "6ea422cdb7b947cebd8eef52befa568c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "73834d7e70f247e0964d8a300f21b255": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7d08e09ff0b44b738679a1fee0c88d0d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "93e68fa8672b4f5dbcf66e80a85b463f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "978800c889f848ed866fafdea14c1342": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d14d6a2bb6b04069a019a13a0e7d59b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e0c5d7d8d1114042939563e1a5578b03": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
